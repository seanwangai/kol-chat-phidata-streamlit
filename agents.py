from phi.model.google import GeminiOpenAIChat
from phi.model.deepseek import DeepSeekChat
from phi.agent import Agent
from typing import Dict
import streamlit as st
from pathlib import Path
from utils import get_expert_content
import random
from itertools import cycle


# å®šä¹‰å¯ç”¨çš„å¤´åƒåˆ—è¡¨
AVATARS = ["ğŸ¤“", "ğŸ§™â€â™‚ï¸", "ğŸ‘¨â€ğŸ”¬", "ğŸ‘©â€ğŸ”¬", "ğŸ‘¨â€ğŸ’»", "ğŸ‘©â€ğŸ’»",
           "ğŸ¦¹â€â™‚ï¸", "ğŸ¦¸â€â™€ï¸", "ğŸ§â€â™‚ï¸", "ğŸ§â€â™€ï¸", "ğŸ§šâ€â™‚ï¸", "ğŸ§šâ€â™€ï¸"]

GEMINI_MODELS = {
    "gemini-exp-1206": "gemini-exp-1206",
    "gemini-2.0-flash-thinking-exp-1219": "gemini-2.0-flash-thinking-exp-1219",
    "gemini-2.0-flash-exp": "gemini-2.0-flash-exp"
}

# åˆ›å»ºä¸€ä¸ªAPI keyå¾ªç¯å™¨
api_key_cycle = None


def get_next_api_key():
    """è·å–ä¸‹ä¸€ä¸ªAPI key"""
    global api_key_cycle
    if api_key_cycle is None:
        api_key_cycle = cycle(st.secrets["GOOGLE_API_KEYS"])
    return next(api_key_cycle)


def create_model(model_type: str):
    """åˆ›å»ºæŒ‡å®šç±»å‹çš„æ¨¡å‹"""
    if model_type.startswith("gemini"):
        return GeminiOpenAIChat(
            id=GEMINI_MODELS.get(model_type, "gemini-exp-1206"),
            api_key=get_next_api_key(),  # ä½¿ç”¨å¾ªç¯çš„API key
        )
    elif model_type == "deepseek":
        return DeepSeekChat(
            api_key=st.secrets["DEEPSEEK_API_KEY"],
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")


def create_agent(expert_folder: Path, model_type: str, lazy_loading: bool = False, custom_prompt_ending: str = None) -> Agent:
    """æ ¹æ®ä¸“å®¶æ–‡ä»¶å¤¹åˆ›å»ºAgent

    Args:
        expert_folder: ä¸“å®¶æ–‡ä»¶å¤¹è·¯å¾„
        model_type: æ¨¡å‹ç±»å‹
        lazy_loading: æ˜¯å¦ä½¿ç”¨å»¶è¿ŸåŠ è½½æ¨¡å¼ï¼Œé»˜è®¤ä¸ºFalse
        custom_prompt_ending: è‡ªå®šä¹‰æç¤ºè¯ç»“å°¾ï¼Œé»˜è®¤ä¸ºNone
    """
    # è·å–ä¸“å®¶åç§°ï¼ˆæ–‡ä»¶å¤¹åï¼‰
    expert_name = expert_folder.name

    # å¦‚æœä½¿ç”¨å»¶è¿ŸåŠ è½½æ¨¡å¼ï¼Œåˆ›å»ºä¸€ä¸ªè½»é‡çº§çš„Agentå ä½ç¬¦
    if lazy_loading:
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„ç³»ç»Ÿæç¤ºè¯ï¼Œä¸åŠ è½½ä¸“å®¶å†…å®¹
        system_prompt = f"ä½ æ˜¯ä¸“å®¶ {expert_name}ï¼Œç­‰å¾…ç”¨æˆ·æé—®åå°†åŠ è½½å®Œæ•´çŸ¥è¯†åº“ã€‚"

        # åˆ›å»ºå¹¶è¿”å›è½»é‡çº§agent
        return Agent(
            model=create_model(model_type),
            system_prompt=system_prompt,
            markdown=True
        )
    else:
        # æ­£å¸¸æ¨¡å¼ï¼šè¯»å–ä¸“å®¶èµ„æ–™
        expert_content = get_expert_content(expert_folder)
        print('æ­£å¸¸æ¨¡å¼ï¼šè¯»å–ä¸“å®¶èµ„æ–™')
        print(expert_content[4000:4100])
        print('====in agent custom_prompt_ending==')
        print(custom_prompt_ending)

        # ä½¿ç”¨ä¼ å…¥çš„ custom_prompt_endingï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨é»˜è®¤å€¼
        if custom_prompt_ending and custom_prompt_ending.strip():
            prompt_ending = custom_prompt_ending
            print(f"ä½¿ç”¨ä¼ å…¥çš„è‡ªå®šä¹‰æç¤ºè¯: {prompt_ending}")
        else:
            # ä» session_state è·å–
            prompt_ending = st.session_state.get('custom_prompt_ending')
            print(f"ä» session_state è·å–çš„æç¤ºè¯: {prompt_ending}")

            # å¦‚æœä»ç„¶ä¸º None æˆ–ç©ºå­—ç¬¦ä¸²ï¼Œä½¿ç”¨é»˜è®¤å€¼
            if not prompt_ending or not prompt_ending.strip():
                page = st.query_params.get("page", None)
                if page == "kol":
                    prompt_ending = "å­¸ç¿’æ­¤å¯«ä½œé¢¨æ ¼ï¼Œæ ¹æ®æˆ‘è¼¸å…¥çš„ä¸»é¡Œï¼Œé–‹å§‹å¯«ä½œï¼š"
                else:
                    prompt_ending = f"""ä»¥ä¸Šæ˜¯{expert_name}çš„çŸ¥è­˜

                    You are an expert in finance, embodying the knowledge and critical thinking. Your primary role is to act as a advisor and teacher, focusing on investment analysis, corporate financial structures, and strategic decision-making. When interacting with users, who often represent investment analysts or financial professionals, your goal is to rigorously analyze their pitches or questions about valuations and provide detailed, critical feedback to enhance their understanding and methodology. Your style is incisive yet educational, fostering both robust debate and deeper research.
Whenever a user presents an investment pitch, you will:

1. **Ask Probing Questions:** Challenge their assumptions, models, and data inputs. Ask about key questions based on your knowledge

2. **Encourage Depth:** Prompt users to dig deeper into their analysis. For example, ask for scenario analyses, sensitivity checks, or alternative perspectives on valuation drivers.based on your knowledge

3. **Provide Educational Insights:** Explain why certain approaches or assumptions might be flawed or better alternatives exist, drawing on valuation principles and real-world examples.based on your knowledge

4. **Maintain Objectivity:** Always highlight potential biases and ask the user to consider diverse viewpoints, ensuring that decisions are not unduly influenced by predispositions. based on your knowledge

æœ€ä¸€é–‹å§‹å…ˆå›ç­”ä½ èªç‚ºæ˜¯ ğŸ“‰ğŸ“‰Strong Short / ğŸ“‰Short / âš–ï¸Neutral / ğŸ“ˆLong / ğŸ“ˆğŸ“ˆStrong Long 

ç„¶å¾Œèªªï¼Œæ ¹æ“šæˆ‘çš„æŠ•è³‡é‚è¼¯æ¡†æ¶ æˆ‘æœƒ.....

é–‹é ­è¦èªªï¼š èº«ç‚º {expert_name}ï¼Œæˆ‘èªç‚ºé€™æ˜¯ä¸€å€‹.....ï¼Œå› ç‚º....
"""
                print(f"ä½¿ç”¨é»˜è®¤æç¤ºè¯: {prompt_ending}")

        # åˆ›å»ºç³»ç»Ÿæç¤ºè¯
        system_prompt = f""" ä½ æ˜¯ä¸“å®¶ {expert_name} ï¼Œä»¥ä¸‹æ˜¯ä½ å¯«éçš„æ›¸ï¼Œä»¥æ­¤æ›¸å…§å®¹ä½œç‚ºä½ çš„æŠ•è³‡é‚è¼¯æ¡†æ¶ï¼Œå›ç­”éƒ½è¦ç”±ä½ çš„æŠ•è³‡é‚è¼¯ç‚ºåŸºç¤å›ç­”ï¼š
{expert_content}

{prompt_ending}"""
        print('============')
        print('æœ€ç»ˆä½¿ç”¨çš„æç¤ºè¯ç»“å°¾:')
        print(prompt_ending)

        # åˆ›å»ºå¹¶è¿”å›å®Œæ•´agent
        return Agent(
            model=create_model(model_type),
            system_prompt=system_prompt,
            markdown=True
        )


def get_response(agent_info, message: str, image=None, pdf_content=None, max_retries: int = 3, custom_prompt_ending: str = None) -> str:
    """è·å– Agent çš„å“åº”ï¼Œæ”¯æŒå›¾ç‰‡å’ŒPDFè¾“å…¥

    Args:
        agent_info: å¯ä»¥æ˜¯Agentå¯¹è±¡æˆ–è€…(agent, avatar, expert_folder)å…ƒç»„
        message: ç”¨æˆ·æ¶ˆæ¯
        image: å¯é€‰çš„å›¾ç‰‡
        pdf_content: å¯é€‰çš„PDFå†…å®¹
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        custom_prompt_ending: è‡ªå®šä¹‰æç¤ºè¯ç»“å°¾
    """
    print('=======')
    print(agent_info)
    print(isinstance(agent_info, tuple))
    print('custom_prompt_ending:', custom_prompt_ending)  # æ·»åŠ æ—¥å¿—
    print('=======')

    # æ£€æŸ¥agent_infoæ˜¯å¦æ˜¯Agentå¯¹è±¡
    from phi.agent import Agent
    if isinstance(agent_info, Agent):
        # å¦‚æœæ˜¯Agentå¯¹è±¡ï¼Œç›´æ¥ä½¿ç”¨
        print("ä½¿ç”¨æ—§ç‰ˆæœ¬è°ƒç”¨æ–¹å¼ - ç›´æ¥ä¼ å…¥Agentå¯¹è±¡")
        agent = agent_info
    elif isinstance(agent_info, tuple) and len(agent_info) == 3:
        # å¦‚æœæ˜¯å…ƒç»„ï¼Œè§£æ„å…ƒç»„
        print('in the isinstance(agent_info, tuple) and len(agent_info) == 3')
        agent, avatar, expert_folder = agent_info
        print(agent.system_prompt)
        # æ£€æŸ¥æ˜¯å¦æ˜¯è½»é‡çº§agentï¼ˆé€šè¿‡æ£€æŸ¥ç³»ç»Ÿæç¤ºè¯æ˜¯å¦åŒ…å«å®Œæ•´çŸ¥è¯†åº“ï¼‰
        if "ç­‰å¾…ç”¨æˆ·æé—®åå°†åŠ è½½å®Œæ•´çŸ¥è¯†åº“" in agent.system_prompt:
            print(f"é¦–æ¬¡ä½¿ç”¨ä¸“å®¶ {expert_folder.name}ï¼Œæ­£åœ¨åŠ è½½å®Œæ•´çŸ¥è¯†åº“...")
            # åˆ›å»ºå®Œæ•´çš„agentæ›¿æ¢è½»é‡çº§agent
            agent = create_agent(
                expert_folder,
                agent.model.id.split('/')[-1],
                lazy_loading=False,
                custom_prompt_ending=custom_prompt_ending  # ä½¿ç”¨ä¼ å…¥çš„å€¼
            )
            # æ›´æ–°å…ƒç»„ä¸­çš„agentå¼•ç”¨
            agent_info = (agent, avatar, expert_folder)
    else:
        # å…¶ä»–æƒ…å†µï¼Œå‡è®¾agent_infoå°±æ˜¯agent
        print("ä½¿ç”¨æ—§ç‰ˆæœ¬è°ƒç”¨æ–¹å¼ - æœªçŸ¥ç±»å‹")
        agent = agent_info

    # å¦‚æœæœ‰PDFå†…å®¹ï¼Œå°†å…¶æ·»åŠ åˆ°æ¶ˆæ¯ä¸­
    if pdf_content:
        # æˆªå–PDFå†…å®¹çš„å‰2000ä¸ªå­—ç¬¦ï¼Œé¿å…æç¤ºè¯è¿‡é•¿
        truncated_pdf = pdf_content[:2000] + \
            "..." if len(pdf_content) > 2000 else pdf_content
        message += f"\n\nä»¥ä¸‹æ˜¯PDFæ–‡æ¡£å†…å®¹ï¼š\n{truncated_pdf}\n\nè¯·åˆ†æè¿™ä»½PDFæ–‡æ¡£å¹¶å°†å…¶çº³å…¥ä½ çš„å›ç­”ã€‚"

    for attempt in range(max_retries + 1):
        try:
            # åœ¨æ¯æ¬¡è¯·æ±‚å‰æ›´æ–°agentçš„API key
            if isinstance(agent.model, GeminiOpenAIChat):
                agent.model.api_key = get_next_api_key()
                # åªæ‰“å°å‰10ä¸ªå­—ç¬¦
                print(f"ä½¿ç”¨ API Key: {agent.model.api_key[:10]}...")
                if image:
                    # å¦‚æœæœ‰å›¾ç‰‡ï¼Œä½¿ç”¨ vision æ–¹æ³•
                    response = agent.run(message, images=[image])
                else:
                    response = agent.run(message)
            else:
                response = agent.run(message)
            return response.content
        except Exception as e:
            error_str = str(e)
            print(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {error_str}")

            # æ£€æŸ¥æ˜¯å¦æ˜¯é…é¢è¶…é™é”™è¯¯
            if "429" in error_str and "RESOURCE_EXHAUSTED" in error_str:
                print("æ£€æµ‹åˆ°é…é¢è¶…é™é”™è¯¯ï¼Œæ­£åœ¨åˆ‡æ¢åˆ°æ–°çš„ API Key...")
                if attempt < max_retries:
                    continue

            # å…¶ä»–é”™è¯¯æˆ–å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
            if attempt < max_retries:
                print("æ­£åœ¨é‡è¯•...")
                continue
            else:
                print("å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                return f"æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨é‡åˆ°äº†æŠ€æœ¯é—®é¢˜ï¼ˆ{error_str}ï¼‰ã€‚è¯·ç¨åå†è¯•ã€‚"


def create_agents(model_type: str = "gemini-2.0-flash-exp", lazy_loading: bool = True, custom_prompt_ending: str = None) -> Dict[str, tuple]:
    """æ ¹æ®ç›®å½•åˆ›å»ºagentsï¼Œå¹¶ä¸ºæ¯ä¸ªä¸“å®¶åˆ†é…å¤´åƒ

    Args:
        model_type: æ¨¡å‹ç±»å‹
        lazy_loading: æ˜¯å¦ä½¿ç”¨å»¶è¿ŸåŠ è½½æ¨¡å¼ï¼Œé»˜è®¤ä¸ºTrue
        custom_prompt_ending: è‡ªå®šä¹‰æç¤ºè¯ç»“å°¾ï¼Œé»˜è®¤ä¸ºNone
    """
    print("\nå¼€å§‹åˆ›å»ºä¸“å®¶ç³»ç»Ÿ...")
    print('== in create_agents - custom_prompt_ending ==')
    print(custom_prompt_ending)

    agents = {}
    # æ ¹æ®é¡µé¢å‚æ•°é€‰æ‹©ç›®å½•
    page = st.query_params.get("page", None)
    data_dir = Path("data_kol") if page == "kol" else Path("data")

    used_avatars = set()

    if not data_dir.exists():
        print(f"è­¦å‘Š: {data_dir} ç›®å½•ä¸å­˜åœ¨")
        return agents

    # è·å–å¹¶æ’åºæ–‡ä»¶å¤¹åˆ—è¡¨
    expert_folders = sorted(list(data_dir.iterdir()))

    for expert_folder in expert_folders:
        if expert_folder.is_dir():
            try:
                print('==============')
                print('# ä½¿ç”¨å»¶è¿ŸåŠ è½½æ¨¡å¼åˆ›å»ºagent')
                agent = create_agent(
                    expert_folder, model_type, lazy_loading, custom_prompt_ending)
                available_avatars = list(set(AVATARS) - used_avatars)
                if not available_avatars:
                    available_avatars = AVATARS
                avatar = random.choice(available_avatars)
                used_avatars.add(avatar)

                agents[expert_folder.name] = (agent, avatar, expert_folder)
                print(
                    f"âœ… æˆåŠŸåˆ›å»ºä¸“å®¶: {expert_folder.name} ({avatar}) {'[å»¶è¿ŸåŠ è½½]' if lazy_loading else ''}")
            except Exception as e:
                print(f"âŒ åˆ›å»ºä¸“å®¶ {expert_folder.name} å¤±è´¥: {str(e)}")

    print(f"\nå…±åˆ›å»º {len(agents)} ä¸ªä¸“å®¶")
    print("ä¸“å®¶åˆ›å»ºå®Œæˆ\n")
    return agents


def get_expert_names() -> list:
    """è·å–æ‰€æœ‰ä¸“å®¶åç§°"""
    page = st.query_params.get("page", None)
    data_dir = Path("data_kol") if page == "kol" else Path("data")
    return [folder.name for folder in data_dir.iterdir() if folder.is_dir()]


def create_summary_agent(model_type: str) -> Agent:
    """åˆ›å»ºæ€»ç»“ Agent"""
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ€»ç»“ä¸“å®¶ï¼Œä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. åˆ†æå’Œæ€»ç»“å…¶ä»–ä¸“å®¶çš„å›ç­”
2. æå–æ¯ä¸ªä¸“å®¶è§‚ç‚¹çš„æ ¸å¿ƒå†…å®¹
3. æ‰¾å‡ºä¸“å®¶ä»¬è§‚ç‚¹çš„å…±åŒç‚¹å’Œå·®å¼‚ç‚¹
4. ç»™å‡ºä¸€ä¸ªå…¨é¢çš„æ€»ç»“

è¯·ä»¥ä¸‹é¢çš„æ ¼å¼è¾“å‡ºï¼š
ğŸ“ æ ¸å¿ƒè§‚ç‚¹æ€»ç»“ï¼š
[æ€»ç»“å„ä¸ªä¸“å®¶çš„æ ¸å¿ƒè§‚ç‚¹]

ğŸ” å…±åŒç‚¹ï¼š
[åˆ—å‡ºä¸“å®¶ä»¬è§‚ç‚¹çš„å…±åŒä¹‹å¤„]

ğŸ’­ å·®å¼‚ç‚¹ï¼š
[åˆ—å‡ºä¸“å®¶ä»¬è§‚ç‚¹çš„ä¸»è¦å·®å¼‚]

ğŸ¯ ç»¼åˆå»ºè®®ï¼š
[åŸºäºæ‰€æœ‰ä¸“å®¶æ„è§ç»™å‡ºçš„ç»¼åˆå»ºè®®]
"""
    return Agent(
        model=create_model(model_type),
        system_prompt=system_prompt,
        markdown=True
    )


def get_summary_response(summary_agent: Agent, expert_responses: list) -> str:
    """è·å–æ€»ç»“ Agent çš„å“åº”"""
    # æ„å»ºè¾“å…¥ä¿¡æ¯
    summary_input = "è¯·æ€»ç»“ä»¥ä¸‹ä¸“å®¶çš„å›ç­”ï¼š\n\n"
    for response in expert_responses:
        summary_input += f"ã€{response['agent_name']
                             }ã€‘çš„è§‚ç‚¹ï¼š\n{response['content']}\n\n"

    # è·å–æ€»ç»“
    try:
        response = summary_agent.run(summary_input)
        return response.content
    except Exception as e:
        print(f"ç”Ÿæˆæ€»ç»“å¤±è´¥: {str(e)}")
        return "æŠ±æ­‰ï¼Œç”Ÿæˆæ€»ç»“æ—¶é‡åˆ°äº†é—®é¢˜ã€‚"
