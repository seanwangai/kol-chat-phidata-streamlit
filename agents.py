from phi.model.google import GeminiOpenAIChat
from phi.model.deepseek import DeepSeekChat
from phi.agent import Agent
from typing import Dict
import streamlit as st
from pathlib import Path
from utils import get_expert_content
import random
from itertools import cycle


# å®šä¹‰å¯ç”¨çš„å¤´åƒåˆ—è¡¨
AVATARS = ["ğŸ¤“", "ğŸ§™â€â™‚ï¸", "ğŸ‘¨â€ğŸ”¬", "ğŸ‘©â€ğŸ”¬", "ğŸ‘¨â€ğŸ’»", "ğŸ‘©â€ğŸ’»",
           "ğŸ¦¹â€â™‚ï¸", "ğŸ¦¸â€â™€ï¸", "ğŸ§â€â™‚ï¸", "ğŸ§â€â™€ï¸", "ğŸ§šâ€â™‚ï¸", "ğŸ§šâ€â™€ï¸"]

GEMINI_MODELS = {
    "gemini-exp-1206": "gemini-exp-1206",
    "gemini-2.0-flash-thinking-exp-1219": "gemini-2.0-flash-thinking-exp-1219",
    "gemini-2.0-flash-exp": "gemini-2.0-flash-exp"
}

# åˆ›å»ºä¸€ä¸ªAPI keyå¾ªç¯å™¨
api_key_cycle = None


def get_next_api_key():
    """è·å–ä¸‹ä¸€ä¸ªAPI key"""
    global api_key_cycle
    if api_key_cycle is None:
        api_key_cycle = cycle(st.secrets["GOOGLE_API_KEYS"])
    return next(api_key_cycle)


def create_model(model_type: str):
    """åˆ›å»ºæŒ‡å®šç±»å‹çš„æ¨¡å‹"""
    if model_type.startswith("gemini"):
        return GeminiOpenAIChat(
            id=GEMINI_MODELS.get(model_type, "gemini-exp-1206"),
            api_key=get_next_api_key(),  # ä½¿ç”¨å¾ªç¯çš„API key
        )
    elif model_type == "deepseek":
        return DeepSeekChat(
            api_key=st.secrets["DEEPSEEK_API_KEY"],
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")


def create_agent(expert_folder: Path, model_type: str, lazy_loading: bool = False, custom_prompt_ending: str = None) -> Agent:
    """æ ¹æ®ä¸“å®¶æ–‡ä»¶å¤¹åˆ›å»ºAgent

    Args:
        expert_folder: ä¸“å®¶æ–‡ä»¶å¤¹è·¯å¾„
        model_type: æ¨¡å‹ç±»å‹
        lazy_loading: æ˜¯å¦ä½¿ç”¨å»¶è¿ŸåŠ è½½æ¨¡å¼ï¼Œé»˜è®¤ä¸ºFalse
        custom_prompt_ending: è‡ªå®šä¹‰æç¤ºè¯ç»“å°¾ï¼Œé»˜è®¤ä¸ºNone
    """
    # è·å–ä¸“å®¶åç§°ï¼ˆæ–‡ä»¶å¤¹åï¼‰
    expert_name = expert_folder.name

    # å¦‚æœä½¿ç”¨å»¶è¿ŸåŠ è½½æ¨¡å¼ï¼Œåˆ›å»ºä¸€ä¸ªè½»é‡çº§çš„Agentå ä½ç¬¦
    if lazy_loading:
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„ç³»ç»Ÿæç¤ºè¯ï¼Œä¸åŠ è½½ä¸“å®¶å†…å®¹
        system_prompt = f"ä½ æ˜¯ä¸“å®¶ {expert_name}ï¼Œç­‰å¾…ç”¨æˆ·æé—®åå°†åŠ è½½å®Œæ•´çŸ¥è¯†åº“ã€‚"

        # åˆ›å»ºå¹¶è¿”å›è½»é‡çº§agent
        return Agent(
            model=create_model(model_type),
            system_prompt=system_prompt,
            markdown=True
        )
    else:
        # æ­£å¸¸æ¨¡å¼ï¼šè¯»å–ä¸“å®¶èµ„æ–™
        expert_content = get_expert_content(expert_folder)
        print('æ­£å¸¸æ¨¡å¼ï¼šè¯»å–ä¸“å®¶èµ„æ–™')
        print(expert_content[4000:4100])
        print('====in agent custom_prompt_ending==')
        print(custom_prompt_ending)

        # ä½¿ç”¨ä¼ å…¥çš„ custom_prompt_endingï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨é»˜è®¤å€¼
        if custom_prompt_ending and custom_prompt_ending.strip():
            prompt_ending = custom_prompt_ending
            print(f"ä½¿ç”¨ä¼ å…¥çš„è‡ªå®šä¹‰æç¤ºè¯: {prompt_ending}")
        else:
            # ä» session_state è·å–
            prompt_ending = st.session_state.get('custom_prompt_ending')
            print(f"ä» session_state è·å–çš„æç¤ºè¯: {prompt_ending}")

            # å¦‚æœä»ç„¶ä¸º None æˆ–ç©ºå­—ç¬¦ä¸²ï¼Œä½¿ç”¨é»˜è®¤å€¼
            if not prompt_ending or not prompt_ending.strip():
                page = st.query_params.get("page", None)
                if page == "kol":
                    prompt_ending = "å­¸ç¿’æ­¤å¯«ä½œé¢¨æ ¼ï¼Œæ ¹æ®æˆ‘è¼¸å…¥çš„ä¸»é¡Œï¼Œé–‹å§‹å¯«ä½œï¼š"
                else:
                    prompt_ending = f"""ä»¥ä¸Šæ˜¯ {expert_name} çš„çŸ¥è­˜ã€‚

You are now embodying {expert_name}, a legendary investor and finance expert. You are known for your rigorous critical thinking, deep knowledge in finance, valuation and strategic decision-making.

Your primary mission is to act as an investment mentor and analyst, guiding professional investors and analysts in sharpening their thinking and investment theses.

When a user presents an investment pitch, your structured response should always follow this format:

---

### Step 0: Initial Rating  
Start your answer by choosing one of the following and explain **why**:  
ğŸ“‰ğŸ“‰ Strong Short / ğŸ“‰ Short / âš–ï¸ Neutral / ğŸ“ˆ Long / ğŸ“ˆğŸ“ˆ Strong Long  
ç›¡é‡ä¸é¸âš–ï¸ Neutralï¼Œé™¤éçœŸçš„æ˜¯ç„¶è¦é¸âš–ï¸ Neutral

**Begin your response with this sentence:**  
#### {{ğŸ“‰ğŸ“‰ Strong Short / ğŸ“ˆğŸ“ˆ Strong Long  ...}} 
èº«ç‚º {expert_name}ï¼Œæˆ‘èªç‚ºé€™æ˜¯... ï¼Œå› ç‚º..

---
### ğŸ§­ Step 1: Investment Philosophy
åš´æ ¼ä½¿ç”¨ {expert_name} çš„çŸ¥è­˜ï¼Œ{expert_name} çŸ¥è­˜ä¸­æåˆ°çš„æ‰€æœ‰æŠ•è³‡é‚è¼¯ï¼Œæ·±åº¦å»è©•ä¼°æåˆ°çš„é€™é–“å…¬å¸
åˆ—å‡ºæˆ‘æ‰€æœ‰æåˆ°æŠ•è³‡é‚è¼¯ï¼Œé€ä¸€å»è§£ææ˜¯å¦æœ‰é”æ¨™


### ğŸ§  Step 2: Core Investment Logic  
Explain your logic based on your investing framework:
- Is the thesis internally consistent?
- Are key drivers realistic?
- Are there critical blind spots?

Use bullet points and back your views with examples or financial reasoning **based on your knowledge**.

---

### ğŸ” Step 3: Challenge & Deepen  
Ask **probing questions** to test the pitch:
- What assumptions need more clarity?
- Are valuation inputs reasonable?
- What sensitivity or scenario analyses are missing?

Challenge them like a top-tier investment committee would.

---

### ğŸ“š Step 4: Educational Insight  
Provide **1-2 educational insights** that help the user level up:
- Point out flaws in logic or modeling
- Suggest better frameworks or comparables
- Reference valuation theory or real-world cases

---

### âš–ï¸ Step 5: Bias & Objectivity Check  
Ask the user to examine potential **biases**:
- Confirmation bias?
- Overconfidence in management?
- Narrative vs. numbers?

---

## Language & Tone Guidelines:
- ä½¿ç”¨ä¸­æ–‡å›ç­”ï¼Œé™¤éæœ‰ç‰¹åˆ¥æŒ‡å®š
- Tone: incisive, Socratic, yet educational
- Do not fabricate factsâ€”use only the embedded knowledge of {expert_name}

"""

                print(f"ä½¿ç”¨é»˜è®¤æç¤ºè¯: {prompt_ending}")

        # åˆ›å»ºç³»ç»Ÿæç¤ºè¯
        system_prompt = f""" ä½ æ˜¯ä¸“å®¶ {expert_name} ï¼Œä»¥ä¸‹æ˜¯ä½ å¯«éçš„æ›¸ï¼Œä»¥æ­¤æ›¸å…§å®¹ä½œç‚ºä½ çš„æŠ•è³‡é‚è¼¯æ¡†æ¶ï¼Œå›ç­”éƒ½è¦ç”±ä½ çš„æŠ•è³‡é‚è¼¯ç‚ºåŸºç¤å›ç­”ï¼š
{expert_content}

{prompt_ending}"""
        print('============')
        print('æœ€ç»ˆä½¿ç”¨çš„æç¤ºè¯ç»“å°¾:')
        print(prompt_ending)

        # åˆ›å»ºå¹¶è¿”å›å®Œæ•´agent
        return Agent(
            model=create_model(model_type),
            system_prompt=system_prompt,
            markdown=True
        )


def get_response(agent_info, message: str, image=None, pdf_content=None, max_retries: int = 3, custom_prompt_ending: str = None) -> str:
    """è·å– Agent çš„å“åº”ï¼Œæ”¯æŒå›¾ç‰‡å’ŒPDFè¾“å…¥

    Args:
        agent_info: å¯ä»¥æ˜¯Agentå¯¹è±¡æˆ–è€…(agent, avatar, expert_folder)å…ƒç»„
        message: ç”¨æˆ·æ¶ˆæ¯
        image: å¯é€‰çš„å›¾ç‰‡
        pdf_content: å¯é€‰çš„PDFå†…å®¹
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        custom_prompt_ending: è‡ªå®šä¹‰æç¤ºè¯ç»“å°¾
    """
    print('=======')
    print(agent_info)
    print(isinstance(agent_info, tuple))
    print('custom_prompt_ending:', custom_prompt_ending)  # æ·»åŠ æ—¥å¿—
    print('=======')

    # æ£€æŸ¥agent_infoæ˜¯å¦æ˜¯Agentå¯¹è±¡
    from phi.agent import Agent
    if isinstance(agent_info, Agent):
        # å¦‚æœæ˜¯Agentå¯¹è±¡ï¼Œç›´æ¥ä½¿ç”¨
        print("ä½¿ç”¨æ—§ç‰ˆæœ¬è°ƒç”¨æ–¹å¼ - ç›´æ¥ä¼ å…¥Agentå¯¹è±¡")
        agent = agent_info
    elif isinstance(agent_info, tuple) and len(agent_info) == 3:
        # å¦‚æœæ˜¯å…ƒç»„ï¼Œè§£æ„å…ƒç»„
        print('in the isinstance(agent_info, tuple) and len(agent_info) == 3')
        agent, avatar, expert_folder = agent_info
        print(agent.system_prompt)
        # æ£€æŸ¥æ˜¯å¦æ˜¯è½»é‡çº§agentï¼ˆé€šè¿‡æ£€æŸ¥ç³»ç»Ÿæç¤ºè¯æ˜¯å¦åŒ…å«å®Œæ•´çŸ¥è¯†åº“ï¼‰
        if "ç­‰å¾…ç”¨æˆ·æé—®åå°†åŠ è½½å®Œæ•´çŸ¥è¯†åº“" in agent.system_prompt:
            print(f"é¦–æ¬¡ä½¿ç”¨ä¸“å®¶ {expert_folder.name}ï¼Œæ­£åœ¨åŠ è½½å®Œæ•´çŸ¥è¯†åº“...")
            # åˆ›å»ºå®Œæ•´çš„agentæ›¿æ¢è½»é‡çº§agent
            agent = create_agent(
                expert_folder,
                agent.model.id.split('/')[-1],
                lazy_loading=False,
                custom_prompt_ending=custom_prompt_ending  # ä½¿ç”¨ä¼ å…¥çš„å€¼
            )
            # æ›´æ–°å…ƒç»„ä¸­çš„agentå¼•ç”¨
            agent_info = (agent, avatar, expert_folder)
    else:
        # å…¶ä»–æƒ…å†µï¼Œå‡è®¾agent_infoå°±æ˜¯agent
        print("ä½¿ç”¨æ—§ç‰ˆæœ¬è°ƒç”¨æ–¹å¼ - æœªçŸ¥ç±»å‹")
        agent = agent_info

    # å¦‚æœæœ‰PDFå†…å®¹ï¼Œå°†å…¶æ·»åŠ åˆ°æ¶ˆæ¯ä¸­
    if pdf_content:
        # æˆªå–PDFå†…å®¹çš„å‰2000ä¸ªå­—ç¬¦ï¼Œé¿å…æç¤ºè¯è¿‡é•¿
        truncated_pdf = pdf_content[:2000] + \
            "..." if len(pdf_content) > 2000 else pdf_content
        message += f"\n\nä»¥ä¸‹æ˜¯PDFæ–‡æ¡£å†…å®¹ï¼š\n{truncated_pdf}\n\nè¯·åˆ†æè¿™ä»½PDFæ–‡æ¡£å¹¶å°†å…¶çº³å…¥ä½ çš„å›ç­”ã€‚"

    for attempt in range(max_retries + 1):
        try:
            # åœ¨æ¯æ¬¡è¯·æ±‚å‰æ›´æ–°agentçš„API key
            if isinstance(agent.model, GeminiOpenAIChat):
                agent.model.api_key = get_next_api_key()
                # åªæ‰“å°å‰10ä¸ªå­—ç¬¦
                print(f"ä½¿ç”¨ API Key: {agent.model.api_key[:10]}...")
                if image:
                    # å¦‚æœæœ‰å›¾ç‰‡ï¼Œä½¿ç”¨ vision æ–¹æ³•
                    response = agent.run(message, images=[image])
                else:
                    response = agent.run(message)
            else:
                response = agent.run(message)
            return response.content
        except Exception as e:
            error_str = str(e)
            print(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {error_str}")

            # æ£€æŸ¥æ˜¯å¦æ˜¯é…é¢è¶…é™é”™è¯¯
            if "429" in error_str and "RESOURCE_EXHAUSTED" in error_str:
                print("æ£€æµ‹åˆ°é…é¢è¶…é™é”™è¯¯ï¼Œæ­£åœ¨åˆ‡æ¢åˆ°æ–°çš„ API Key...")
                if attempt < max_retries:
                    continue

            # å…¶ä»–é”™è¯¯æˆ–å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
            if attempt < max_retries:
                print("æ­£åœ¨é‡è¯•...")
                continue
            else:
                print("å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                return f"æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨é‡åˆ°äº†æŠ€æœ¯é—®é¢˜ï¼ˆ{error_str}ï¼‰ã€‚è¯·ç¨åå†è¯•ã€‚"


def create_agents(model_type: str = "gemini-2.0-flash-exp", lazy_loading: bool = True, custom_prompt_ending: str = None) -> Dict[str, tuple]:
    """æ ¹æ®ç›®å½•åˆ›å»ºagentsï¼Œå¹¶ä¸ºæ¯ä¸ªä¸“å®¶åˆ†é…å¤´åƒ

    Args:
        model_type: æ¨¡å‹ç±»å‹
        lazy_loading: æ˜¯å¦ä½¿ç”¨å»¶è¿ŸåŠ è½½æ¨¡å¼ï¼Œé»˜è®¤ä¸ºTrue
        custom_prompt_ending: è‡ªå®šä¹‰æç¤ºè¯ç»“å°¾ï¼Œé»˜è®¤ä¸ºNone
    """
    print("\nå¼€å§‹åˆ›å»ºä¸“å®¶ç³»ç»Ÿ...")
    print('== in create_agents - custom_prompt_ending ==')
    print(custom_prompt_ending)

    agents = {}
    # æ ¹æ®é¡µé¢å‚æ•°é€‰æ‹©ç›®å½•
    page = st.query_params.get("page", None)
    data_dir = Path("data_kol") if page == "kol" else Path("data")

    used_avatars = set()

    if not data_dir.exists():
        print(f"è­¦å‘Š: {data_dir} ç›®å½•ä¸å­˜åœ¨")
        return agents

    # è·å–å¹¶æ’åºæ–‡ä»¶å¤¹åˆ—è¡¨
    expert_folders = sorted(list(data_dir.iterdir()))

    for expert_folder in expert_folders:
        if expert_folder.is_dir():
            try:
                print('==============')
                print('# ä½¿ç”¨å»¶è¿ŸåŠ è½½æ¨¡å¼åˆ›å»ºagent')
                agent = create_agent(
                    expert_folder, model_type, lazy_loading, custom_prompt_ending)
                available_avatars = list(set(AVATARS) - used_avatars)
                if not available_avatars:
                    available_avatars = AVATARS
                avatar = random.choice(available_avatars)
                used_avatars.add(avatar)

                agents[expert_folder.name] = (agent, avatar, expert_folder)
                print(
                    f"âœ… æˆåŠŸåˆ›å»ºä¸“å®¶: {expert_folder.name} ({avatar}) {'[å»¶è¿ŸåŠ è½½]' if lazy_loading else ''}")
            except Exception as e:
                print(f"âŒ åˆ›å»ºä¸“å®¶ {expert_folder.name} å¤±è´¥: {str(e)}")

    print(f"\nå…±åˆ›å»º {len(agents)} ä¸ªä¸“å®¶")
    print("ä¸“å®¶åˆ›å»ºå®Œæˆ\n")
    return agents


def get_expert_names() -> list:
    """è·å–æ‰€æœ‰ä¸“å®¶åç§°"""
    page = st.query_params.get("page", None)
    data_dir = Path("data_kol") if page == "kol" else Path("data")
    return [folder.name for folder in data_dir.iterdir() if folder.is_dir()]


def create_summary_agent(model_type: str) -> Agent:
    """åˆ›å»ºæ€»ç»“ Agent"""
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ€»ç»“ä¸“å®¶ï¼Œä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. åˆ†æå’Œæ€»ç»“å…¶ä»–ä¸“å®¶çš„å›ç­”
2. æå–æ¯ä¸ªä¸“å®¶è§‚ç‚¹çš„æ ¸å¿ƒå†…å®¹
3. æ‰¾å‡ºä¸“å®¶ä»¬è§‚ç‚¹çš„å…±åŒç‚¹å’Œå·®å¼‚ç‚¹
4. ç»™å‡ºä¸€ä¸ªå…¨é¢çš„æ€»ç»“

è¯·ä»¥ä¸‹é¢çš„æ ¼å¼è¾“å‡ºï¼š
ğŸ“ æ ¸å¿ƒè§‚ç‚¹æ€»ç»“ï¼š
[æ€»ç»“å„ä¸ªä¸“å®¶çš„æ ¸å¿ƒè§‚ç‚¹]

ğŸ” å…±åŒç‚¹ï¼š
[åˆ—å‡ºä¸“å®¶ä»¬è§‚ç‚¹çš„å…±åŒä¹‹å¤„]

ğŸ’­ å·®å¼‚ç‚¹ï¼š
[åˆ—å‡ºä¸“å®¶ä»¬è§‚ç‚¹çš„ä¸»è¦å·®å¼‚]

ğŸ¯ ç»¼åˆå»ºè®®ï¼š
[åŸºäºæ‰€æœ‰ä¸“å®¶æ„è§ç»™å‡ºçš„ç»¼åˆå»ºè®®]
"""
    return Agent(
        model=create_model(model_type),
        system_prompt=system_prompt,
        markdown=True
    )


def get_summary_response(summary_agent: Agent, expert_responses: list) -> str:
    """è·å–æ€»ç»“ Agent çš„å“åº”"""
    # æ„å»ºè¾“å…¥ä¿¡æ¯
    summary_input = "è¯·æ€»ç»“ä»¥ä¸‹ä¸“å®¶çš„å›ç­”ï¼š\n\n"
    for response in expert_responses:
        summary_input += f"ã€{response['agent_name']
                             }ã€‘çš„è§‚ç‚¹ï¼š\n{response['content']}\n\n"

    # è·å–æ€»ç»“
    try:
        response = summary_agent.run(summary_input)
        return response.content
    except Exception as e:
        print(f"ç”Ÿæˆæ€»ç»“å¤±è´¥: {str(e)}")
        return "æŠ±æ­‰ï¼Œç”Ÿæˆæ€»ç»“æ—¶é‡åˆ°äº†é—®é¢˜ã€‚"
