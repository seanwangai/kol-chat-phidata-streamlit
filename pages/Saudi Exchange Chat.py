"""
SEC & è´¢æŠ¥ä¼šè®®è®°å½•åˆ†æå¸ˆ - Productionçº§åˆ«
ä¸“ä¸šçš„è´¢åŠ¡æ•°æ®ç»¼åˆåˆ†æå¹³å°

Features:
- SECæ–‡ä»¶è·å–å’Œåˆ†æ
- è´¢æŠ¥ä¼šè®®è®°å½•è·å–å’Œåˆ†æ
- AIé©±åŠ¨çš„æ–‡æ¡£å¤„ç†
- å®æ—¶è¿›åº¦è·Ÿè¸ª
- é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
- ç¼“å­˜ä¼˜åŒ–
"""

import streamlit as st
import os
import json
import requests
import warnings
import logging
import time
import hashlib
import tempfile
import uuid
import fitz  # PyMuPDF for PDF processing
import shutil
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass, field, fields
from functools import lru_cache
from contextlib import contextmanager
import re
import html
from urllib.parse import urljoin
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# é…ç½®é€‰é¡¹ï¼šæ˜¯å¦ä¿å­˜transcriptæ–‡ä»¶åˆ°ç£ç›˜
SAVE_TRANSCRIPT_FILES = os.getenv("SAVE_TRANSCRIPT_FILES", "false").lower() == "true"

# ç¬¬ä¸‰æ–¹åº“
from sec_edgar_api import EdgarClient
from bs4 import BeautifulSoup, XMLParsedAsHTMLWarning
import httpx
from google import genai
from google.genai import types
from itertools import cycle
from newspaper import Article

# é¡µé¢é…ç½®
try:
    st.set_page_config(
        page_title="SEC & è´¢æŠ¥ä¼šè®®è®°å½•åˆ†æå¸ˆ",
        page_icon="ğŸ“Š",
        layout="wide"
    )
except Exception:
    # é™é»˜å¤„ç†é¡µé¢é…ç½®é”™è¯¯
    pass

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# è¯­è¨€é…ç½®
LANGUAGE_CONFIG = {
    "English": {
        "title": "ğŸ“Š Saudi Exchange Chat",
        "sidebar_header": "ğŸ“‹ Configuration",
        "ticker_label": "Ticker",
        "ticker_placeholder": "e.g., AAPL, 1024 HK, 2222 SA",
        "years_label": "Years of Data",
        "data_type_header": "ğŸ“„ Data Type",
        "sec_reports_us": "Quarterly & Annual (10-K, 10-Q, 20-F, 6-K, 424B4)",
        "sec_others_us": "Others (8-K, S-8, DEF 14A, F-3)",
        "sec_reports_hk": "Quarterly & Annual Results",
        "sec_others_hk": "Other Announcements",
        "earnings_label": "Earnings Call Transcripts",
        "earnings_caption": "Earnings call transcripts",
        "model_header": "ğŸ¤– AI Model",
        "model_label": "Select Model",
        "analysis_mode_header": "âš¡ Analysis Mode",
        "analysis_mode_label": "Select Mode",
        "fast_mode": "Fast Mode",
        "detailed_mode": "Detailed Mode",
        "fast_mode_caption": "Analyze all documents together in one pass",
        "detailed_mode_caption": "Analyze each document separately, then integrate results",
        "api_header": "ğŸ’³ API Configuration",
        "access_code_label": "Enter Access Code",
        "access_code_placeholder": "Enter access code to enable premium API",
        "premium_enabled": "âœ… Premium API Service Enabled",
        "free_api": "â„¹ï¸ Using Free API Service",
        "access_code_error": "âŒ Invalid Access Code",
        "premium_success": "ğŸ‰ Premium API Service Enabled!",
        "language_header": "ğŸŒ Language",
        "language_label": "Select Language",
        "hk_stock_info": "ğŸ¢ Hong Kong Stock - Standardized to: {}",
        "us_stock_info": "ğŸ‡ºğŸ‡¸ US Stock",
        "saudi_stock_info": "ğŸ‡¸ğŸ‡¦ Saudi Stock",
        "chat_placeholder": "Please enter your question...",
        "status_header": "ğŸ“‹ STATUS",
        "stop_button": "â¹ï¸ Stop Processing",
        "progress_text": "Progress: {}/{} documents",
        "stop_success": "â¹ï¸ Processing stopped by user",
        "processing_stopped": "Processing has been stopped by user request."
    },
    "ä¸­æ–‡": {
        "title": "ğŸ“Š æ²™ç‰¹äº¤æ˜“æ‰€èŠå¤©",
        "sidebar_header": "ğŸ“‹ Configuration",
        "ticker_label": "Ticker",
        "ticker_placeholder": "e.g., AAPL, 1024 HK, 2222 SA",
        "years_label": "æ•°æ®å¹´ä»½",
        "data_type_header": "ğŸ“„ Data Type",
        "sec_reports_us": "Quarterly & Annual (10-K, 10-Q, 20-F, 6-K, 424B4)",
        "sec_others_us": "Others (8-K, S-8, DEF 14A, F-3)",
        "sec_reports_hk": "Quarterly & Annual Results",
        "sec_others_hk": "Other Announcements",
        "earnings_label": "Earnings Call Transcripts",
        "earnings_caption": "Earnings call transcripts",
        "model_header": "ğŸ¤– AI Model",
        "model_label": "Select Model",
        "analysis_mode_header": "âš¡ åˆ†ææ¨¡å¼",
        "analysis_mode_label": "é€‰æ‹©æ¨¡å¼",
        "fast_mode": "å¿«é€Ÿæ¨¡å¼",
        "detailed_mode": "è¯¦ç»†æ¨¡å¼",
        "fast_mode_caption": "ä¸€æ¬¡æ€§åˆ†ææ‰€æœ‰æ–‡æ¡£",
        "detailed_mode_caption": "é€ä¸ªåˆ†ææ¯ä»½æ–‡æ¡£ï¼Œç„¶åæ•´åˆç»“æœ",
        "api_header": "ğŸ’³ API Configuration",
        "access_code_label": "Enter Access Code",
        "access_code_placeholder": "Enter access code to enable premium API",
        "premium_enabled": "âœ… Premium API Service Enabled",
        "free_api": "â„¹ï¸ Using Free API Service",
        "access_code_error": "âŒ Invalid Access Code",
        "premium_success": "ğŸ‰ Premium API Service Enabled!",
        "language_header": "ğŸŒ Language",
        "language_label": "Select Language",
        "hk_stock_info": "ğŸ¢ æ¸¯è‚¡ - å·²æ ‡å‡†åŒ–ä¸º: {}",
        "us_stock_info": "ğŸ‡ºğŸ‡¸ ç¾è‚¡",
        "saudi_stock_info": "ğŸ‡¸ğŸ‡¦ æ²™ç‰¹è‚¡ç¥¨",
        "chat_placeholder": "è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...",
        "status_header": "ğŸ“‹ STATUS",
        "stop_button": "â¹ï¸ åœæ­¢å¤„ç†",
        "progress_text": "Progress: {}/{} documents",
        "stop_success": "â¹ï¸ ç”¨æˆ·å·²åœæ­¢å¤„ç†",
        "processing_stopped": "å¤„ç†å·²è¢«ç”¨æˆ·åœæ­¢ã€‚"
    },
    "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©": {
        "title": "ğŸ“Š Ø¯Ø±Ø¯Ø´Ø© Ø§Ù„Ø³ÙˆÙ‚ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©",
        "sidebar_header": "ğŸ“‹ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª",
        "ticker_label": "Ø±Ù…Ø² Ø§Ù„Ø³Ù‡Ù…",
        "ticker_placeholder": "Ù…Ø«Ø§Ù„: AAPL, 1024 HK, 2222 SA",
        "years_label": "Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª",
        "data_type_header": "ğŸ“„ Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª",
        "sec_reports_us": "Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„ÙØµÙ„ÙŠØ© ÙˆØ§Ù„Ø³Ù†ÙˆÙŠØ© (10-K, 10-Q, 20-F, 6-K, 424B4)",
        "sec_others_us": "Ø£Ø®Ø±Ù‰ (8-K, S-8, DEF 14A, F-3)",
        "sec_reports_hk": "Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ÙØµÙ„ÙŠØ© ÙˆØ§Ù„Ø³Ù†ÙˆÙŠØ©",
        "sec_others_hk": "Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ø£Ø®Ø±Ù‰",
        "earnings_label": "Ù†ØµÙˆØµ Ù…ÙƒØ§Ù„Ù…Ø§Øª Ø§Ù„Ø£Ø±Ø¨Ø§Ø­",
        "earnings_caption": "Ù†ØµÙˆØµ Ù…ÙƒØ§Ù„Ù…Ø§Øª Ø§Ù„Ø£Ø±Ø¨Ø§Ø­",
        "model_header": "ğŸ¤– Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ",
        "model_label": "Ø§Ø®ØªØ± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬",
        "analysis_mode_header": "âš¡ ÙˆØ¶Ø¹ Ø§Ù„ØªØ­Ù„ÙŠÙ„",
        "analysis_mode_label": "Ø§Ø®ØªØ± Ø§Ù„ÙˆØ¶Ø¹",
        "fast_mode": "Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø³Ø±ÙŠØ¹",
        "detailed_mode": "Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ù…ÙØµÙ„",
        "fast_mode_caption": "ØªØ­Ù„ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…Ø¹Ø§Ù‹ ÙÙŠ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø©",
        "detailed_mode_caption": "ØªØ­Ù„ÙŠÙ„ ÙƒÙ„ Ù…Ø³ØªÙ†Ø¯ Ø¹Ù„Ù‰ Ø­Ø¯Ø©ØŒ Ø«Ù… Ø¯Ù…Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬",
        "api_header": "ğŸ’³ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª",
        "access_code_label": "Ø£Ø¯Ø®Ù„ Ø±Ù…Ø² Ø§Ù„ÙˆØµÙˆÙ„",
        "access_code_placeholder": "Ø£Ø¯Ø®Ù„ Ø±Ù…Ø² Ø§Ù„ÙˆØµÙˆÙ„ Ù„ØªÙØ¹ÙŠÙ„ Ø®Ø¯Ù…Ø© API Ø§Ù„Ù…Ù…ÙŠØ²Ø©",
        "premium_enabled": "âœ… ØªÙ… ØªÙØ¹ÙŠÙ„ Ø®Ø¯Ù…Ø© API Ø§Ù„Ù…Ù…ÙŠØ²Ø©",
        "free_api": "â„¹ï¸ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®Ø¯Ù…Ø© API Ø§Ù„Ù…Ø¬Ø§Ù†ÙŠØ©",
        "access_code_error": "âŒ Ø±Ù…Ø² ÙˆØµÙˆÙ„ ØºÙŠØ± ØµØ­ÙŠØ­",
        "premium_success": "ğŸ‰ ØªÙ… ØªÙØ¹ÙŠÙ„ Ø®Ø¯Ù…Ø© API Ø§Ù„Ù…Ù…ÙŠØ²Ø©!",
        "language_header": "ğŸŒ Ø§Ù„Ù„ØºØ©",
        "language_label": "Ø§Ø®ØªØ± Ø§Ù„Ù„ØºØ©",
        "hk_stock_info": "ğŸ¢ Ø³Ù‡Ù… Ù‡ÙˆÙ†Øº ÙƒÙˆÙ†Øº - ØªÙ… Ø§Ù„ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠ Ø¥Ù„Ù‰: {}",
        "us_stock_info": "ğŸ‡ºğŸ‡¸ Ø³Ù‡Ù… Ø£Ù…Ø±ÙŠÙƒÙŠ",
        "saudi_stock_info": "ğŸ‡¸ğŸ‡¦ Ø³Ù‡Ù… Ø³Ø¹ÙˆØ¯ÙŠ",
        "chat_placeholder": "ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ø³Ø¤Ø§Ù„Ùƒ...",
        "status_header": "ğŸ“‹ Ø§Ù„Ø­Ø§Ù„Ø©",
        "stop_button": "â¹ï¸ Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©",
        "progress_text": "Ø§Ù„ØªÙ‚Ø¯Ù…: {}/{} Ù…Ø³ØªÙ†Ø¯",
        "stop_success": "â¹ï¸ ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…",
        "processing_stopped": "ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…."
    }
}

# å¸¸é‡é…ç½®
@dataclass
class Config:
    """åº”ç”¨é…ç½®"""
    # æ¨¡å‹é…ç½®
    MODELS: Dict[str, str] = field(default_factory=lambda: {
        "gemini-2.5-flash": "Gemini 2.5 Flash",
        "gemini-2.5-pro": "Gemini 2.5 Pro"
    })
    
    # SECé…ç½®
    SEC_USER_AGENT: str = "SEC Earnings Analyzer <analysis@example.com>"
    SEC_FORMS: List[str] = field(default_factory=lambda: [
        '10-K', '10-Q', '8-K', '20-F', '6-K', '424B4', 'DEF 14A', 'S-8'
    ])
    
    # è¯·æ±‚é…ç½®
    REQUEST_TIMEOUT: int = 30
    MAX_RETRIES: int = 3
    RETRY_DELAY: float = 1.0
    
    # å†…å®¹é™åˆ¶
    MAX_CONTENT_LENGTH: int = 900000
    
    # ç¼“å­˜é…ç½®
    CACHE_TTL: int = 3600  # 1å°æ—¶
    
    # æ—¥æœŸè§£ææ ¼å¼
    DATE_FORMATS: List[str] = field(default_factory=lambda: [
        '%B %d, %Y',    # January 1, 2023
        '%b %d, %Y',    # Jan 1, 2023
        '%Y-%m-%d',     # 2023-01-01
        '%m/%d/%Y',     # 01/01/2023
    ])
    
    # æ—¥æœŸæ¨¡å¼
    DATE_PATTERNS: List[str] = field(default_factory=lambda: [
        r'\b\w+ \d{1,2}, \d{4}\b',  # January 1, 2023
        r'\b\d{4}-\d{1,2}-\d{1,2}\b',  # 2023-01-01
        r'\b\d{1,2}/\d{1,2}/\d{4}\b',  # 01/01/2023
    ])

config = Config()

# æ•°æ®ç±»å®šä¹‰
@dataclass
class Document:
    """æ–‡æ¡£æ•°æ®ç±»"""
    type: str
    title: str
    date: datetime.date
    url: str
    content: Optional[str] = None
    form_type: Optional[str] = None
    year: Optional[int] = None
    quarter: Optional[int] = None
    temp_file_path: Optional[str] = None  # æ·»åŠ ä¸´æ—¶æ–‡ä»¶è·¯å¾„å­—æ®µ

@dataclass
class ProcessingStatus:
    """å¤„ç†çŠ¶æ€æ•°æ®ç±»"""
    is_processing: bool = False
    current_step: Optional[str] = None
    completed_documents: int = 0
    total_documents: int = 0
    document_results: List[Dict] = field(default_factory=list)
    processing_step: int = 0
    processing_prompt: str = ""
    integration_prompt: str = ""
    documents: List[Document] = field(default_factory=list)
    user_question: str = ""
    error_message: Optional[str] = None
    status_messages: List[str] = field(default_factory=list)
    current_status_label: str = ""
    progress_percentage: float = 0.0
    stop_requested: bool = False

    def __post_init__(self):
        # Ensure fields are initialized if not provided
        if self.processing_step is None:
            self.processing_step = 0
        if self.completed_documents is None:
            self.completed_documents = 0
        if self.total_documents is None:
            self.total_documents = 0
        if self.document_results is None:
            self.document_results = []
        if self.processing_prompt is None:
            self.processing_prompt = ""
        if self.integration_prompt is None:
            self.integration_prompt = ""
        if self.documents is None:
            self.documents = []
        if self.user_question is None:
            self.user_question = ""
        if self.error_message is None:
            self.error_message = None
        if self.status_messages is None:
            self.status_messages = []
        if self.current_status_label is None:
            self.current_status_label = ""
        if self.progress_percentage is None:
            self.progress_percentage = 0.0
        if self.stop_requested is None:
            self.stop_requested = False
    
    def add_status_message(self, message: str):
        """æ·»åŠ çŠ¶æ€æ¶ˆæ¯"""
        self.status_messages.append(f"â±ï¸ {datetime.now().strftime('%H:%M:%S')} - {message}")
        # åªä¿ç•™æœ€è¿‘çš„20æ¡æ¶ˆæ¯
        if len(self.status_messages) > 20:
            self.status_messages = self.status_messages[-20:]
    
    def update_progress(self, completed: int, total: int, label: str = ""):
        """æ›´æ–°è¿›åº¦"""
        self.completed_documents = completed
        self.total_documents = total
        self.progress_percentage = (completed / total) * 100 if total > 0 else 0
        self.current_status_label = label

# å¼‚å¸¸ç±»å®šä¹‰
class SECAnalyzerError(Exception):
    """SECåˆ†æå™¨åŸºç¡€å¼‚å¸¸"""
    pass

class APIError(SECAnalyzerError):
    """APIè°ƒç”¨å¼‚å¸¸"""
    pass

class DataRetrievalError(SECAnalyzerError):
    """æ•°æ®è·å–å¼‚å¸¸"""
    pass

class DateParsingError(SECAnalyzerError):
    """æ—¥æœŸè§£æå¼‚å¸¸"""
    pass

# å·¥å…·ç±»
def is_hk_stock(ticker: str) -> bool:
    """æ£€æµ‹æ˜¯å¦ä¸ºæ¸¯è‚¡ä»£ç """
    if not ticker:
        return False
    
    ticker_upper = ticker.upper().strip()
    
    # æ£€æŸ¥æ˜¯å¦ä»¥.HKç»“å°¾
    if ticker_upper.endswith('.HK'):
        return True
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯ "æ•°å­— HK" æ ¼å¼
    if ' HK' in ticker_upper:
        return True
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯çº¯æ•°å­—ï¼Œä½†æ’é™¤4ä½æ•°å­—ï¼ˆä¼˜å…ˆç»™æ²™ç‰¹äº¤æ˜“æ‰€ï¼‰
    ticker_clean = ticker.strip()
    if ticker_clean.isdigit() and len(ticker_clean) != 4:
        return True
    
    return False

def is_saudi_stock(ticker: str) -> bool:
    """æ£€æµ‹æ˜¯å¦ä¸ºæ²™ç‰¹äº¤æ˜“æ‰€ä»£ç """
    if not ticker:
        return False
    
    ticker_upper = ticker.upper().strip()
    
    # æ£€æŸ¥æ˜¯å¦ä»¥.SAç»“å°¾
    if ticker_upper.endswith('.SA'):
        return True
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯ "æ•°å­— SA" æ ¼å¼
    if ' SA' in ticker_upper:
        return True
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯æ•°å­—ï¼ˆæ²™ç‰¹äº¤æ˜“æ‰€ä»£ç é€šå¸¸æ˜¯æ•°å­—ï¼Œä¼˜å…ˆè¯†åˆ«ï¼‰
    ticker_clean = ticker.strip()
    if ticker_clean.isdigit():
        return True
    
    return False

def normalize_saudi_ticker(ticker: str) -> str:
    """æ ‡å‡†åŒ–æ²™ç‰¹äº¤æ˜“æ‰€ä»£ç ä¸º XXXX æ ¼å¼ï¼ˆ4ä½æ•°å­—ï¼‰"""
    if not ticker:
        return ""
    
    ticker_clean = ticker.upper().strip()
    
    # ç§»é™¤.SAåç¼€
    if ticker_clean.endswith('.SA'):
        ticker_clean = ticker_clean[:-3]
    
    # ç§»é™¤ç©ºæ ¼å’ŒSA
    if ' SA' in ticker_clean:
        ticker_clean = ticker_clean.replace(' SA', '')
    
    # ç¡®ä¿æ˜¯4ä½æ•°å­—
    if ticker_clean.isdigit():
        return ticker_clean.zfill(4)  # è¡¥é›¶åˆ°4ä½
    
    return ticker_clean

def normalize_hk_ticker(ticker: str) -> str:
    """æ ‡å‡†åŒ–æ¸¯è‚¡ä»£ç ä¸º XXXX.HK æ ¼å¼ï¼Œè‡ªå‹•è£œ0æˆå››ä½æ•¸"""
    if not ticker:
        return ticker
    
    ticker_clean = ticker.strip()
    
    # å¦‚æœå·²ç»æ˜¯æ ‡å‡†æ ¼å¼ï¼Œæå–æ•¸å­—éƒ¨åˆ†è™•ç†
    if ticker_clean.upper().endswith('.HK'):
        number_part = ticker_clean.upper().replace('.HK', '').strip()
        if number_part.isdigit():
            # è£œ0æˆå››ä½æ•¸
            padded_number = number_part.zfill(4)
            return f"{padded_number}.HK"
        return ticker_clean.upper()
    
    # å¤„ç† "æ•°å­— HK" æ ¼å¼
    if ' HK' in ticker_clean.upper():
        number_part = ticker_clean.upper().replace(' HK', '').strip()
        if number_part.isdigit():
            # è£œ0æˆå››ä½æ•¸
            padded_number = number_part.zfill(4)
            return f"{padded_number}.HK"
    
    # å¤„ç†çº¯æ•°å­—
    if ticker_clean.isdigit():
        # è£œ0æˆå››ä½æ•¸
        padded_number = ticker_clean.zfill(4)
        return f"{padded_number}.HK"
    
    # å…¶ä»–æƒ…å†µè¿”å›åŸå€¼
    return ticker_clean.upper()

def clean_hk_ticker(ticker: str) -> str:
    """æ¸…ç†æ¸¯è‚¡ä»£ç ï¼Œç§»é™¤.HKåç¼€ï¼Œè¿”å›çº¯æ•°å­—"""
    normalized = normalize_hk_ticker(ticker)
    return normalized.replace('.HK', '').replace('.hk', '')

class RateLimiter:
    """APIè¯·æ±‚é™æµå™¨"""
    def __init__(self, max_calls: int = 30, window: int = 60):
        self.max_calls = max_calls
        self.window = window
        self.calls = []
    
    def wait_if_needed(self):
        """å¦‚æœéœ€è¦ï¼Œç­‰å¾…ç›´åˆ°å¯ä»¥å‘å‡ºè¯·æ±‚"""
        now = time.time()
        self.calls = [call_time for call_time in self.calls if now - call_time < self.window]
        
        if len(self.calls) >= self.max_calls:
            wait_time = self.window - (now - self.calls[0])
            if wait_time > 0:
                time.sleep(wait_time)
                self.calls = []
        
        self.calls.append(now)

class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨"""
    def __init__(self):
        # Session state is now initialized by SessionManager
        pass
    
    def get_cache_key(self, *args) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_string = "|".join(str(arg) for arg in args)
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def get(self, key: str, default=None):
        """è·å–ç¼“å­˜å€¼"""
        cache_data = st.session_state.cache.get(key)
        if cache_data:
            timestamp, value = cache_data
            if time.time() - timestamp < config.CACHE_TTL:
                return value
            else:
                # ç¼“å­˜è¿‡æœŸï¼Œåˆ é™¤
                del st.session_state.cache[key]
        return default
    
    def set(self, key: str, value: Any):
        """è®¾ç½®ç¼“å­˜å€¼"""
        st.session_state.cache[key] = (time.time(), value)
    
    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        st.session_state.cache.clear()

class DocumentManager:
    """æ–‡æ¡£ç®¡ç†å™¨ï¼Œè´Ÿè´£ä¿å­˜å’Œç®¡ç†ä¸´æ—¶æ–‡ä»¶"""
    
    def __init__(self):
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        self.temp_dir = tempfile.mkdtemp(prefix="sec_analyzer_")
        logger.info(f"åˆ›å»ºä¸´æ—¶ç›®å½•: {self.temp_dir}")
    
    def save_document_content(self, document: Document) -> str:
        """ä¿å­˜æ–‡æ¡£å†…å®¹åˆ°ä¸´æ—¶æ–‡ä»¶å¹¶è¿”å›æ–‡ä»¶è·¯å¾„"""
        try:
            # ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶å
            file_id = str(uuid.uuid4())[:8]
            safe_title = re.sub(r'[^\w\-_\.]', '_', document.title)
            filename = f"{file_id}_{safe_title}.txt"
            file_path = os.path.join(self.temp_dir, filename)
            
            # ä¿å­˜å†…å®¹
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(f"æ–‡æ¡£æ ‡é¢˜: {document.title}\n")
                f.write(f"æ–‡æ¡£ç±»å‹: {document.type}\n")
                f.write(f"æ–‡æ¡£æ—¥æœŸ: {document.date}\n")
                f.write(f"åŸå§‹URL: {document.url}\n")
                f.write("="*80 + "\n\n")
                f.write(document.content or "å†…å®¹ä¸ºç©º")
            
            document.temp_file_path = file_path
            logger.info(f"æ–‡æ¡£å·²ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶: {filename}")
            return file_path
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ–‡æ¡£å†…å®¹å¤±è´¥: {e}")
            return None
    
    def get_download_content(self, file_path: str) -> Optional[bytes]:
        """è·å–æ–‡ä»¶å†…å®¹ç”¨äºä¸‹è½½"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read().encode('utf-8')
        except Exception as e:
            logger.error(f"è¯»å–æ–‡ä»¶å†…å®¹å¤±è´¥: {e}")
            return None
    
    def cleanup(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        try:
            import shutil
            shutil.rmtree(self.temp_dir)
            logger.info(f"å·²æ¸…ç†ä¸´æ—¶ç›®å½•: {self.temp_dir}")
        except Exception as e:
            logger.error(f"æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")

class SixKProcessor:
    """6-Kæ–‡ä»¶å¤„ç†å™¨"""
    
    def __init__(self, temp_dir: str):
        self.temp_dir = temp_dir
        self.headers = {"User-Agent": config.SEC_USER_AGENT}
    
    def process_6k_filing(self, ticker: str, cik: str, filing_url: str, document: Document) -> List[Document]:
        """å¤„ç†6-Kæ–‡ä»¶ï¼Œä¸‹è½½é™„ä»¶å¹¶æå–ex99æ–‡ä»¶"""
        try:
            logger.info(f"ğŸ” [6K-DEBUG] å¼€å§‹å¤„ç†6-Kæ–‡ä»¶: {ticker}")
            logger.info(f"ğŸ” [6K-DEBUG] æ–‡ä»¶URL: {filing_url}")
            logger.info(f"ğŸ” [6K-DEBUG] CIK: {cik}")
            
            # ä»URLä¸­æå–accession numberï¼ˆæ— ç ´æŠ˜å·æ ¼å¼ï¼‰
            # URLæ ¼å¼ï¼šhttps://www.sec.gov/Archives/edgar/data/{cik}/{accession_no_no_dashes}/{primary_doc}
            accession_match = re.search(r'/(\d{18})/', filing_url)
            if not accession_match:
                logger.error(f"âŒ [6K-DEBUG] æ— æ³•ä»URLä¸­æå–accession number: {filing_url}")
                return []  # è¿”å›ç©ºåˆ—è¡¨
            
            accession_no_no_dashes = accession_match.group(1)
            # é‡æ–°æ„é€ å¸¦ç ´æŠ˜å·çš„æ ¼å¼ç”¨äºæ˜¾ç¤º
            accession_no = f"{accession_no_no_dashes[:10]}-{accession_no_no_dashes[10:12]}-{accession_no_no_dashes[12:]}"
            
            logger.info(f"ğŸ” [6K-DEBUG] æå–åˆ°çš„accession number: {accession_no}")
            
            # åˆ›å»º6-Kæ–‡ä»¶ä¸“ç”¨ç›®å½•
            filing_dir = os.path.join(self.temp_dir, f"6K_{ticker}_{accession_no}")
            os.makedirs(filing_dir, exist_ok=True)
            logger.info(f"ğŸ” [6K-DEBUG] åˆ›å»ºç›®å½•: {filing_dir}")
            
            logger.info(f"ğŸ” [6K-DEBUG] å¼€å§‹ä¸‹è½½é™„ä»¶...")
            
            # ä¸‹è½½æ‰€æœ‰é™„ä»¶
            attachments = self._download_6k_attachments(cik, accession_no_no_dashes, filing_dir)
            
            logger.info(f"ğŸ” [6K-DEBUG] ä¸‹è½½é™„ä»¶ç»“æœ: {len(attachments) if attachments else 0} ä¸ªæ–‡ä»¶")
            if attachments:
                logger.info(f"ğŸ” [6K-DEBUG] é™„ä»¶åˆ—è¡¨: {[os.path.basename(f) for f in attachments]}")
            
            if not attachments:
                logger.warning(f"âš ï¸ [6K-DEBUG] æœªæ‰¾åˆ°6-Ké™„ä»¶: {ticker} - {accession_no}")
                return []  # è¿”å›ç©ºåˆ—è¡¨è€Œä¸æ˜¯åŸæ–‡æ¡£
            
            logger.info(f"ğŸ” [6K-DEBUG] å¼€å§‹å¤„ç†ex99æ–‡ä»¶...")
            
            # å¤„ç†ex99æ–‡ä»¶
            ex99_documents = self._process_ex99_files(attachments, filing_dir, document, ticker)
            
            logger.info(f"ğŸ” [6K-DEBUG] ex99å¤„ç†ç»“æœ: {len(ex99_documents) if ex99_documents else 0} ä¸ªæ–‡æ¡£")
            
            if not ex99_documents:
                logger.info(f"â„¹ï¸ [6K-DEBUG] æœªæ‰¾åˆ°ex99æ–‡ä»¶: {ticker} - {accession_no} (è¿™å¯èƒ½æ˜¯æ­£å¸¸æƒ…å†µ)")
                return []  # è¿”å›ç©ºåˆ—è¡¨è€Œä¸æ˜¯åŸæ–‡æ¡£
            
            logger.info(f"âœ… [6K-DEBUG] æˆåŠŸå¤„ç†6-Kæ–‡ä»¶ï¼Œç”Ÿæˆ {len(ex99_documents)} ä¸ªæ–‡æ¡£")
            for i, doc in enumerate(ex99_documents):
                logger.info(f"ğŸ” [6K-DEBUG] æ–‡æ¡£{i+1}: {doc.title}, å†…å®¹é•¿åº¦: {len(doc.content) if doc.content else 0}")
            
            return ex99_documents
            
        except Exception as e:
            logger.error(f"âŒ [6K-DEBUG] å¤„ç†6-Kæ–‡ä»¶å¤±è´¥: {str(e)}")
            import traceback
            logger.error(f"âŒ [6K-DEBUG] è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return []  # è¿”å›ç©ºåˆ—è¡¨è€Œä¸æ˜¯åŸæ–‡æ¡£
    
    def _download_6k_attachments(self, cik: str, accession_no_no_dashes: str, filing_dir: str) -> List[str]:
        """ä¸‹è½½6-Kæ–‡ä»¶çš„æ‰€æœ‰é™„ä»¶ï¼Œåªä¸‹è½½pdf/htm/htmlæ–‡ä»¶"""
        base_url = f"https://www.sec.gov/Archives/edgar/data/{cik}/{accession_no_no_dashes}/"
        index_url = base_url + "index.json"
        
        logger.info(f"ğŸ” [6K-ATTACH] å¼€å§‹ä¸‹è½½é™„ä»¶ï¼Œindex URL: {index_url}")
        
        try:
            response = httpx.get(index_url, headers=self.headers, timeout=config.REQUEST_TIMEOUT)
            response.raise_for_status()
            
            logger.info(f"ğŸ” [6K-ATTACH] index.json ä¸‹è½½æˆåŠŸï¼ŒçŠ¶æ€ç : {response.status_code}")
            
            index_data = response.json()
            directory = index_data.get('directory', {})
            items = directory.get('item', [])
            
            logger.info(f"ğŸ” [6K-ATTACH] index.json è§£ææˆåŠŸï¼Œæ‰¾åˆ° {len(items)} ä¸ªé¡¹ç›®")
            
            if not items:
                logger.warning(f"âš ï¸ [6K-ATTACH] 6-Kæ–‡ä»¶æ— é™„ä»¶åˆ—è¡¨: {index_url}")
                return []
            
            # ç­›é€‰åªéœ€è¦çš„æ–‡ä»¶ç±»å‹
            target_files = []
            logger.info(f"ğŸ” [6K-ATTACH] å¼€å§‹ç­›é€‰ç›®æ ‡æ–‡ä»¶ç±»å‹...")
            
            for i, item in enumerate(items):
                file_name = item.get('name', '')
                if not file_name:
                    continue
                
                logger.info(f"ğŸ” [6K-ATTACH] æ–‡ä»¶{i+1}: {file_name}")
                
                # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
                ext = os.path.splitext(file_name)[1].lower()
                if ext in ['.pdf', '.htm', '.html']:
                    target_files.append(item)
                    logger.info(f"âœ… [6K-ATTACH] æ–‡ä»¶{i+1} {file_name} ç¬¦åˆæ¡ä»¶ (æ‰©å±•å: {ext})")
                else:
                    logger.info(f"â­ï¸ [6K-ATTACH] è·³è¿‡éç›®æ ‡æ–‡ä»¶ç±»å‹: {file_name} (æ‰©å±•å: {ext})")
            
            if not target_files:
                logger.warning(f"âš ï¸ [6K-ATTACH] æœªæ‰¾åˆ°pdf/htm/htmlæ–‡ä»¶")
                return []
            
            downloaded_files = []
            logger.info(f"ğŸ” [6K-ATTACH] æ‰¾åˆ° {len(target_files)} ä¸ªç›®æ ‡æ–‡ä»¶ï¼Œå¼€å§‹ä¸‹è½½...")
            
            for i, item in enumerate(target_files):
                file_name = item.get('name', '')
                file_url = base_url + file_name
                file_path = os.path.join(filing_dir, file_name)
                
                logger.info(f"ğŸ” [6K-ATTACH] ä¸‹è½½æ–‡ä»¶{i+1}: {file_name}")
                logger.info(f"ğŸ” [6K-ATTACH] æ–‡ä»¶URL: {file_url}")
                
                try:
                    file_response = httpx.get(file_url, headers=self.headers, timeout=config.REQUEST_TIMEOUT)
                    file_response.raise_for_status()
                    
                    logger.info(f"ğŸ” [6K-ATTACH] æ–‡ä»¶{i+1}ä¸‹è½½æˆåŠŸï¼ŒçŠ¶æ€ç : {file_response.status_code}")
                    logger.info(f"ğŸ” [6K-ATTACH] æ–‡ä»¶å¤§å°: {len(file_response.content)} bytes")
                    
                    # åˆ¤æ–­æ–‡ä»¶ç±»å‹å¹¶ä¿å­˜
                    content_type = file_response.headers.get('content-type', '').lower()
                    logger.info(f"ğŸ” [6K-ATTACH] æ–‡ä»¶{i+1}å†…å®¹ç±»å‹: {content_type}")
                    
                    if 'text' in content_type or 'html' in content_type or 'xml' in content_type:
                        with open(file_path, 'w', encoding='utf-8') as f:
                            f.write(file_response.text)
                        logger.info(f"ğŸ” [6K-ATTACH] æ–‡ä»¶{i+1}ä¿å­˜ä¸ºæ–‡æœ¬æ–‡ä»¶")
                    else:
                        with open(file_path, 'wb') as f:
                            f.write(file_response.content)
                        logger.info(f"ğŸ” [6K-ATTACH] æ–‡ä»¶{i+1}ä¿å­˜ä¸ºäºŒè¿›åˆ¶æ–‡ä»¶")
                    
                    downloaded_files.append(file_path)
                    logger.info(f"âœ… [6K-ATTACH] å·²ä¸‹è½½6-Ké™„ä»¶: {file_name}")
                    
                except Exception as e:
                    logger.warning(f"âŒ [6K-ATTACH] ä¸‹è½½6-Ké™„ä»¶å¤±è´¥ {file_name}: {str(e)}")
                    import traceback
                    logger.warning(f"âŒ [6K-ATTACH] è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            
            logger.info(f"âœ… [6K-ATTACH] æˆåŠŸä¸‹è½½ {len(downloaded_files)} ä¸ª6-Kç›®æ ‡é™„ä»¶")
            return downloaded_files
            
        except Exception as e:
            logger.error(f"âŒ [6K-ATTACH] è·å–6-Ké™„ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}")
            import traceback
            logger.error(f"âŒ [6K-ATTACH] è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return []
    
    def _process_ex99_files(self, attachments: List[str], filing_dir: str, original_doc: Document, ticker: str) -> List[Document]:
        """å¤„ç†ex99æ–‡ä»¶ï¼ŒæŒ‰ç…§è¦æ±‚åˆ†ç±»å¤„ç†HTMLå’ŒPDF"""
        logger.info(f"ğŸ” [6K-EX99] å¼€å§‹å¤„ç†ex99æ–‡ä»¶ï¼Œè¾“å…¥é™„ä»¶æ•°é‡: {len(attachments)}")
        logger.info(f"ğŸ” [6K-EX99] é™„ä»¶æ–‡ä»¶åˆ—è¡¨: {[os.path.basename(f) for f in attachments]}")
        
        ex99_files = []
        
        # æ‰¾åˆ°æ‰€æœ‰åŒ…å«_ex99çš„æ–‡ä»¶
        logger.info(f"ğŸ” [6K-EX99] æœç´¢åŒ…å«'ex99'çš„æ–‡ä»¶...")
        
        for i, file_path in enumerate(attachments):
            file_name = os.path.basename(file_path).lower()
            logger.info(f"ğŸ” [6K-EX99] æ£€æŸ¥æ–‡ä»¶{i+1}: {file_name}")
            
            if 'ex99' in file_name:
                ex99_files.append(file_path)
                logger.info(f"âœ… [6K-EX99] æ‰¾åˆ°ex99æ–‡ä»¶: {file_name}")
            else:
                logger.info(f"â­ï¸ [6K-EX99] éex99æ–‡ä»¶: {file_name}")
        
        if not ex99_files:
            logger.info(f"â„¹ï¸ [6K-EX99] æœªæ‰¾åˆ°åŒ…å«'ex99'çš„æ–‡ä»¶")
            logger.info(f"ğŸ” [6K-EX99] å®Œæ•´æ–‡ä»¶ååˆ—è¡¨ç”¨äºè°ƒè¯•: {[os.path.basename(f) for f in attachments]}")
            return []
        
        logger.info(f"âœ… [6K-EX99] æ‰¾åˆ° {len(ex99_files)} ä¸ªex99æ–‡ä»¶: {[os.path.basename(f) for f in ex99_files]}")
        
        # åˆ†ç±»å¤„ç†ä¸åŒç±»å‹çš„æ–‡ä»¶
        html_files = []
        pdf_files = []
        
        logger.info(f"ğŸ” [6K-EX99] å¼€å§‹æŒ‰æ–‡ä»¶ç±»å‹åˆ†ç±»...")
        
        for file_path in ex99_files:
            ext = os.path.splitext(file_path)[1].lower()
            file_name = os.path.basename(file_path)
            
            if ext in ['.html', '.htm']:
                html_files.append(file_path)
                logger.info(f"ğŸ“„ [6K-EX99] HTMLæ–‡ä»¶: {file_name}")
            elif ext == '.pdf':
                pdf_files.append(file_path)
                logger.info(f"ğŸ“„ [6K-EX99] PDFæ–‡ä»¶: {file_name}")
            else:
                logger.info(f"âš ï¸ [6K-EX99] è·³è¿‡ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_path} (æ‰©å±•å: {ext})")
        
        documents = []
        
        # å¤„ç†HTML/HTMæ–‡ä»¶ - è½¬æ¢ä¸ºmarkdownåæ£€æŸ¥æ˜¯å¦å¯ä»¥åˆå¹¶
        if html_files:
            logger.info(f"å¤„ç† {len(html_files)} ä¸ªHTMLæ–‡ä»¶")
            
            # å…ˆè½¬æ¢æ‰€æœ‰HTMLæ–‡ä»¶ä¸ºmarkdown
            html_contents = []
            for file_path in html_files:
                content = self._convert_html_to_markdown(file_path)
                if content:
                    file_name = os.path.basename(file_path)
                    html_contents.append({
                        'file_name': file_name,
                        'content': content
                    })
            
            if html_contents:
                # è®¡ç®—æ€»å­—ç¬¦æ•°
                total_chars = sum(len(item['content']) for item in html_contents)
                
                if total_chars <= config.MAX_CONTENT_LENGTH:
                    # åˆå¹¶æ‰€æœ‰HTMLå†…å®¹
                    combined_content = ""
                    for item in html_contents:
                        combined_content += f"\n\n=== {item['file_name']} ===\n\n{item['content']}"
                    
                    html_doc = Document(
                        type=original_doc.type,
                        title=f"{ticker} 6-K ex99 files",
                        date=original_doc.date,
                        url=original_doc.url,
                        content=combined_content.strip(),
                        form_type="6-K-Ex99-HTML-Combined"
                    )
                    documents.append(html_doc)
                    logger.info(f"HTMLæ–‡ä»¶å·²åˆå¹¶å¤„ç†ï¼Œæ€»å­—ç¬¦æ•°: {total_chars}")
                else:
                    # åˆ†åˆ«å¤„ç†æ¯ä¸ªHTMLæ–‡ä»¶
                    for i, item in enumerate(html_contents):
                        html_doc = Document(
                            type=original_doc.type,
                            title=f"{ticker} 6-K ex99-{i+1}",
                            date=original_doc.date,
                            url=original_doc.url,
                            content=item['content'],
                            form_type="6-K-Ex99-HTML"
                        )
                        documents.append(html_doc)
                    logger.info(f"HTMLæ–‡ä»¶åˆ†åˆ«å¤„ç†ï¼Œæ€»å­—ç¬¦æ•°è¶…è¿‡é™åˆ¶: {total_chars}")
        
        # å¤„ç†PDFæ–‡ä»¶ - å¿…é¡»åˆ†å¼€å¤„ç†
        if pdf_files:
            logger.info(f"å¤„ç† {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")
            
            for i, file_path in enumerate(pdf_files):
                content = self._convert_pdf_to_text(file_path)
                if content:
                    file_name = os.path.basename(file_path)
                    pdf_doc = Document(
                        type=original_doc.type,
                        title=f"{ticker} 6-K ex99 PDF-{i+1}",
                        date=original_doc.date,
                        url=original_doc.url,
                        content=content,
                        form_type="6-K-Ex99-PDF"
                    )
                    documents.append(pdf_doc)
                    logger.info(f"PDFæ–‡ä»¶å·²å¤„ç†: {file_name}")
                else:
                    logger.warning(f"PDFæ–‡ä»¶è½¬æ¢å¤±è´¥: {file_path}")
        
        logger.info(f"Ex99æ–‡ä»¶å¤„ç†å®Œæˆï¼Œç”Ÿæˆ {len(documents)} ä¸ªæ–‡æ¡£")
        return documents
    
    def _convert_html_to_markdown(self, file_path: str) -> str:
        """å°†HTMLè½¬æ¢ä¸ºmarkdownæ ¼å¼çš„æ–‡æœ¬"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
            
            # ä½¿ç”¨BeautifulSoupè§£æHTML
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾
            for script in soup(["script", "style"]):
                script.decompose()
            
            # å¤„ç†è¡¨æ ¼ - ä¿æŒç»“æ„
            for table in soup.find_all('table'):
                # ç®€å•çš„è¡¨æ ¼è½¬æ¢
                rows = table.find_all('tr')
                table_text = "\n"
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    if cells:
                        cell_texts = [cell.get_text(strip=True) for cell in cells]
                        table_text += "| " + " | ".join(cell_texts) + " |\n"
                table_text += "\n"
                table.replace_with(table_text)
            
            # å¤„ç†æ ‡é¢˜
            for i in range(1, 7):
                for heading in soup.find_all(f'h{i}'):
                    heading_text = heading.get_text(strip=True)
                    heading.replace_with(f"\n{'#' * i} {heading_text}\n")
            
            # å¤„ç†æ®µè½
            for p in soup.find_all('p'):
                p_text = p.get_text(strip=True)
                if p_text:
                    p.replace_with(f"\n{p_text}\n")
            
            # å¤„ç†åˆ—è¡¨
            for ul in soup.find_all('ul'):
                items = ul.find_all('li')
                list_text = "\n"
                for item in items:
                    item_text = item.get_text(strip=True)
                    if item_text:
                        list_text += f"- {item_text}\n"
                list_text += "\n"
                ul.replace_with(list_text)
            
            for ol in soup.find_all('ol'):
                items = ol.find_all('li')
                list_text = "\n"
                for i, item in enumerate(items, 1):
                    item_text = item.get_text(strip=True)
                    if item_text:
                        list_text += f"{i}. {item_text}\n"
                list_text += "\n"
                ol.replace_with(list_text)
            
            # è·å–æœ€ç»ˆæ–‡æœ¬
            text = soup.get_text()
            
            # æ¸…ç†æ–‡æœ¬
            lines = text.split('\n')
            cleaned_lines = []
            for line in lines:
                line = line.strip()
                if line:
                    cleaned_lines.append(line)
            
            # åˆå¹¶è¿ç»­çš„ç©ºè¡Œ
            final_text = '\n'.join(cleaned_lines)
            
            # å»é™¤å¤šä½™çš„ç©ºè¡Œ
            final_text = re.sub(r'\n{3,}', '\n\n', final_text)
            
            return final_text.strip()
            
        except Exception as e:
            logger.error(f"è½¬æ¢HTMLæ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return ""
    
    def _convert_pdf_to_text(self, file_path: str) -> str:
        """å°†PDFè½¬æ¢ä¸ºæ–‡æœ¬"""
        try:
            doc = fitz.open(file_path)
            text = ""
            
            for page_num in range(doc.page_count):
                page = doc[page_num]
                page_text = page.get_text()
                if page_text.strip():
                    text += f"\n--- ç¬¬ {page_num + 1} é¡µ ---\n"
                    text += page_text
                    text += "\n"
            
            doc.close()
            
            # æ¸…ç†æ–‡æœ¬
            text = re.sub(r'\n{3,}', '\n\n', text)
            return text.strip()
            
        except Exception as e:
            logger.error(f"è½¬æ¢PDFæ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return ""

# æ¸¯è‚¡æ–‡ä»¶ä¸‹è½½å™¨
class HKStockFilingsDownloader:
    """æ¸¯è‚¡å…¬å‘Šä¸‹è½½å™¨"""
    def __init__(self):
        self.base_url = "https://www1.hkexnews.hk"
        self.prefix_url = f"{self.base_url}/search/prefix.do"
        self.search_url = f"{self.base_url}/search/titlesearch.xhtml"
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,zh-TW;q=0.8,zh;q=0.7,zh-CN;q=0.6,ja;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'same-origin',
            'Sec-Fetch-User': '?1',
        }
        
        # æ–‡ä»¶ç±»å‹æ˜ å°„
        self.filing_types = {
            'Annual Report': 'å¹´æŠ¥',
            'Quarterly Results': 'å­£æŠ¥',
            'Interim Results': 'ä¸­æœŸæŠ¥å‘Š',
            'Final Results': 'å…¨å¹´ä¸šç»©',
            'Financial Statements': 'è´¢åŠ¡æŠ¥è¡¨'
        }
        
    def get_stock_id(self, ticker):
        """æ ¹æ®è‚¡ç¥¨ä»£ç è·å–è‚¡ç¥¨ID"""
        # æ¸…ç†è‚¡ç¥¨ä»£ç ï¼Œç§»é™¤.HKåç¼€
        clean_ticker = clean_hk_ticker(ticker)
        
        # æ„å»ºè¯·æ±‚URL
        timestamp = int(time.time() * 1000)
        params = {
            'callback': 'callback',
            'lang': 'EN',
            'type': 'A',
            'name': clean_ticker,
            'market': 'SEHK',
            '_': timestamp
        }
        
        # è®¾ç½®AJAXè¯·æ±‚å¤´
        ajax_headers = self.headers.copy()
        ajax_headers.update({
            'Accept': 'text/javascript, application/javascript, application/ecmascript, application/x-ecmascript, */*; q=0.01',
            'X-Requested-With': 'XMLHttpRequest',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Referer': f'{self.base_url}/search/titlesearch.xhtml?lang=en'
        })
        
        try:
            with httpx.Client(headers=ajax_headers, timeout=30) as client:
                response = client.get(self.prefix_url, params=params)
                response.raise_for_status()
                
                # è§£æJSONPå“åº”
                content = response.text
                logger.debug(f"æ¸¯è‚¡APIåŸå§‹å“åº”: {content}")
                
                # ç§»é™¤JSONPåŒ…è£…
                data = None
                if content.startswith('callback(') and content.endswith(');'):
                    json_str = content[9:-2]
                    data = json.loads(json_str)
                elif content.startswith('callback(') and content.endswith('});'):
                    json_str = content[9:-3]
                    data = json.loads(json_str)
                else:
                    # å°è¯•ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–JSONéƒ¨åˆ†
                    match = re.search(r'callback\((.*)\);?\s*$', content)
                    if match:
                        json_str = match.group(1)
                        data = json.loads(json_str)
                    else:
                        logger.error(f"æ— æ³•è§£æJSONPæ ¼å¼: {content}")
                        return None, None, None
                
                if 'stockInfo' in data and data['stockInfo']:
                    stock_info = data['stockInfo'][0]
                    stock_id = stock_info['stockId']
                    stock_code = stock_info['code']
                    stock_name = stock_info['name']
                    
                    logger.info(f"æ‰¾åˆ°æ¸¯è‚¡: {stock_code} - {stock_name} (ID: {stock_id})")
                    return stock_id, stock_code, stock_name
                else:
                    logger.warning(f"æœªæ‰¾åˆ°æ¸¯è‚¡ä»£ç  {ticker} çš„ä¿¡æ¯")
                    return None, None, None
                    
        except Exception as e:
            logger.error(f"è·å–æ¸¯è‚¡IDæ—¶å‡ºé”™: {str(e)}")
            return None, None, None
    
    def get_filings_list(self, stock_id, from_date=None, to_date=None, cutoff_date=None, status_callback=None):
        """è·å–æŒ‡å®šè‚¡ç¥¨çš„æ‰€æœ‰å…¬å‘Šåˆ—è¡¨ï¼Œæ”¯æŒè‡ªåŠ¨ç¿»é¡µ"""
        if not from_date:
            from_date = "19990401"  # é»˜è®¤ä»1999å¹´å¼€å§‹
        if not to_date:
            to_date = datetime.now().strftime("%Y%m%d")
        
        # è§£ææˆªæ­¢æ—¥æœŸ
        cutoff_datetime = None
        if cutoff_date:
            cutoff_datetime = datetime.strptime(cutoff_date, "%Y%m%d")
        
        all_filings = []
        row_range = 0  # ä»ç¬¬ä¸€é¡µå¼€å§‹
        page_size = 100  # æ¯é¡µ100æ¡è®°å½•
        total_record_count = None  # æ€»è®°å½•æ•°
        
        while True:
            # æ„å»ºGETè¯·æ±‚URLï¼ˆç¬¬ä¸€é¡µç”¨POSTï¼Œåç»­é¡µé¢ç”¨GETï¼‰
            if row_range == 0:
                # ç¬¬ä¸€é¡µä½¿ç”¨POSTè¯·æ±‚
                post_data = {
                    'lang': 'EN',
                    'category': '0',
                    'market': 'SEHK',
                    'searchType': '0',
                    'documentType': '-1',
                    't1code': '-2',
                    't2Gcode': '-2',
                    't2code': '-2',
                    'stockId': str(stock_id),
                    'from': from_date,
                    'to': to_date,
                    'MB-Daterange': '0',
                    'title': ''
                }
                
                # è®¾ç½®POSTè¯·æ±‚å¤´
                post_headers = self.headers.copy()
                post_headers.update({
                    'Content-Type': 'application/x-www-form-urlencoded',
                    'Cache-Control': 'max-age=0',
                    'Referer': f'{self.base_url}/search/titlesearch.xhtml?lang=en'
                })
                
                try:
                    with httpx.Client(headers=post_headers, timeout=30) as client:
                        response = client.post(self.search_url, data=post_data)
                        response.raise_for_status()
                        
                        page_filings = self.parse_filings_html(response.text)
                        
                except Exception as e:
                    logger.error(f"è·å–æ¸¯è‚¡å…¬å‘Šåˆ—è¡¨ç¬¬1é¡µæ—¶å‡ºé”™: {str(e)}")
                    break
            else:
                # åç»­é¡µé¢ä½¿ç”¨GETè¯·æ±‚
                get_url = f"{self.base_url}/search/titleSearchServlet.do"
                params = {
                    'sortDir': '0',
                    'sortByOptions': 'DateTime',
                    'category': '0',
                    'market': 'SEHK',
                    'stockId': str(stock_id),
                    'documentType': '-1',
                    'fromDate': from_date,
                    'toDate': to_date,
                    'title': '',
                    'searchType': '0',
                    't1code': '-2',
                    't2Gcode': '-2',
                    't2code': '-2',
                    'rowRange': str(row_range),
                    'lang': 'E'
                }
                
                # è®¾ç½®GETè¯·æ±‚å¤´
                get_headers = self.headers.copy()
                get_headers.update({
                    'accept': '*/*',
                    'x-requested-with': 'XMLHttpRequest',
                    'Referer': f'{self.base_url}/search/titlesearch.xhtml?lang=en'
                })
                
                try:
                    with httpx.Client(headers=get_headers, timeout=30) as client:
                        response = client.get(get_url, params=params)
                        response.raise_for_status()
                        
                        # è§£æJSONå“åº”
                        json_data = response.json()
                        page_filings = self.parse_filings_json(json_data.get('result', '[]'))
                        
                        # ä»JSONå“åº”ä¸­è·å–æ€»è®°å½•æ•°
                        if total_record_count is None:
                            try:
                                import json
                                result_data = json.loads(json_data.get('result', '[]'))
                                if result_data and len(result_data) > 0:
                                    total_record_count = int(result_data[0].get('TOTAL_COUNT', 0))
                                    logger.info(f"æ¸¯è‚¡å…¬å‘Šæ€»è®°å½•æ•°: {total_record_count}")
                            except (json.JSONDecodeError, ValueError, KeyError) as e:
                                logger.warning(f"æ— æ³•è§£ææ€»è®°å½•æ•°: {e}")
                        
                except Exception as e:
                    logger.error(f"è·å–æ¸¯è‚¡å…¬å‘Šåˆ—è¡¨ç¬¬{row_range//page_size + 1}é¡µæ—¶å‡ºé”™: {str(e)}")
                    break
            
            if not page_filings:
                logger.info(f"æ¸¯è‚¡å…¬å‘Šåˆ—è¡¨ç¬¬{row_range//page_size + 1}é¡µæ— æ•°æ®ï¼Œåœæ­¢ç¿»é¡µ")
                break
            
            # å…ˆå°†æœ¬é¡µæ‰€æœ‰å…¬å‘Šæ·»åŠ åˆ°ç»“æœä¸­
            all_filings.extend(page_filings)
            
            # æ£€æŸ¥æœ¬é¡µæœ€åä¸€ä¸ªå…¬å‘Šçš„æ—¥æœŸæ˜¯å¦æ—©äºæˆªæ­¢æ—¥æœŸ
            if cutoff_datetime and page_filings:
                last_filing = page_filings[-1]  # è·å–æœ€åä¸€ä¸ªå…¬å‘Š
                last_filing_date = self.parse_filing_date(last_filing.get('release_time', ''))
                if last_filing_date and last_filing_date < cutoff_datetime:
                    logger.info(f"æ¸¯è‚¡å…¬å‘Šæœ€åä¸€ä¸ªæ—¥æœŸ {last_filing_date} æ—©äºæˆªæ­¢æ—¥æœŸ {cutoff_datetime}ï¼Œåœæ­¢ç¿»é¡µ")
                    break
            
            # å¦‚æœè¿™ä¸€é¡µçš„è®°å½•æ•°å°‘äºé¡µé¢å¤§å°ï¼Œè¯´æ˜å·²ç»æ˜¯æœ€åä¸€é¡µ
            if len(page_filings) < page_size:
                logger.info(f"æ¸¯è‚¡å…¬å‘Šåˆ—è¡¨ç¬¬{row_range//page_size + 1}é¡µè®°å½•æ•° {len(page_filings)} å°‘äºé¡µé¢å¤§å°ï¼Œåœæ­¢ç¿»é¡µ")
                break
            
            # å‡†å¤‡ä¸‹ä¸€é¡µ
            row_range += page_size
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»è¶…è¿‡æ€»è®°å½•æ•°
            if total_record_count is not None and row_range >= total_record_count:
                logger.info(f"æ¸¯è‚¡å…¬å‘Šåˆ—è¡¨rowRange({row_range}) å·²è¾¾åˆ°æ€»è®°å½•æ•°({total_record_count})ï¼Œåœæ­¢ç¿»é¡µ")
                break
            
            logger.info(f"æ¸¯è‚¡å…¬å‘Šåˆ—è¡¨å‡†å¤‡è·å–ç¬¬{row_range//page_size + 1}é¡µ (rowRange={row_range})")
            if status_callback:
                status_callback(f"æ­£åœ¨è·å–ç¬¬ {row_range//page_size + 1} é¡µæ¸¯è‚¡å…¬å‘Š...")
        
        logger.info(f"æ¸¯è‚¡å…¬å‘Šåˆ—è¡¨å…±è·å– {len(all_filings)} æ¡è®°å½•")
        return all_filings
    
    def parse_filings_html(self, html_content):
        """è§£æHTMLå“åº”ï¼Œæå–å…¬å‘Šé“¾æ¥"""
        soup = BeautifulSoup(html_content, 'html.parser')
        filings = []
        
        # æŸ¥æ‰¾æ‰€æœ‰è¡¨æ ¼è¡Œ
        table_rows = soup.find_all('tr')
        
        for row in table_rows:
            # æŸ¥æ‰¾åŒ…å«PDFé“¾æ¥çš„å•å…ƒæ ¼
            doc_link_cell = row.find('div', class_='doc-link')
            if doc_link_cell:
                # è·å–PDFé“¾æ¥
                pdf_link = doc_link_cell.find('a')
                if pdf_link and pdf_link.get('href'):
                    href = pdf_link.get('href')
                    
                    # è·å–å‘å¸ƒæ—¶é—´
                    time_cell = row.find('td', class_='release-time')
                    release_time = time_cell.get_text(strip=True).replace('Release Time: ', '') if time_cell else 'N/A'
                    
                    # è·å–è‚¡ç¥¨ä»£ç 
                    code_cell = row.find('td', class_='stock-short-code')
                    stock_code = code_cell.get_text(strip=True).replace('Stock Code: ', '').split('\n')[0] if code_cell else 'N/A'
                    
                    # è·å–è‚¡ç¥¨åç§°
                    name_cell = row.find('td', class_='stock-short-name')
                    stock_name = name_cell.get_text(strip=True).replace('Stock Short Name: ', '').split('\n')[0] if name_cell else 'N/A'
                    
                    # è·å–æ–‡æ¡£æ ‡é¢˜
                    doc_title = pdf_link.get_text(strip=True)
                    
                    # è·å–æ–‡æ¡£ç±»å‹ï¼ˆä»headline divè·å–ï¼‰
                    headline_div = row.find('div', class_='headline')
                    doc_type = headline_div.get_text(strip=True) if headline_div else 'Unknown'
                    
                    # è·å–æ–‡ä»¶å¤§å°
                    filesize_span = doc_link_cell.find('span', class_='attachment_filesize')
                    filesize = filesize_span.get_text(strip=True) if filesize_span else 'N/A'
                    
                    # æ„å»ºå®Œæ•´URL
                    full_url = urljoin(self.base_url, href)
                    
                    filing_info = {
                        'release_time': release_time,
                        'stock_code': stock_code,
                        'stock_name': stock_name,
                        'doc_type': doc_type,
                        'doc_title': doc_title,
                        'file_size': filesize,
                        'url': full_url,
                        'href': href
                    }
                    
                    filings.append(filing_info)
        
        return filings
    
    def parse_filings_json(self, json_string):
        """è§£æJSONæ ¼å¼çš„å…¬å‘Šå“åº”"""
        try:
            import json
            filings_data = json.loads(json_string)
            filings = []
            
            for item in filings_data:
                filing_info = {
                    'release_time': item.get('DATE_TIME', ''),
                    'stock_code': item.get('STOCK_CODE', '').replace('<br/>', '/'),
                    'stock_name': item.get('STOCK_NAME', '').replace('<br/>', '/'),
                    'doc_type': item.get('LONG_TEXT', ''),
                    'doc_title': item.get('TITLE', ''),
                    'file_size': item.get('FILE_INFO', ''),
                    'url': f"{self.base_url}{item.get('FILE_LINK', '')}",
                    'href': item.get('FILE_LINK', '')
                }
                filings.append(filing_info)
            
            return filings
            
        except Exception as e:
            logger.error(f"è§£æJSONå…¬å‘Šæ•°æ®å¤±è´¥: {e}")
            return []
    
    def parse_filing_date(self, date_str):
        """è§£æå…¬å‘Šæ—¥æœŸå­—ç¬¦ä¸²ä¸ºdatetimeå¯¹è±¡"""
        try:
            # æ¸¯è‚¡æ—¥æœŸæ ¼å¼ï¼šDD/MM/YYYY HH:MM
            if '/' in date_str and ' ' in date_str:
                date_part = date_str.split(' ')[0]  # å–æ—¥æœŸéƒ¨åˆ†
                parts = date_part.split('/')
                if len(parts) == 3:
                    day, month, year = parts
                    return datetime(int(year), int(month), int(day))
            return None
        except Exception as e:
            logger.warning(f"è§£ææ¸¯è‚¡æ—¥æœŸå¤±è´¥: {date_str} - {e}")
            return None
    
    def categorize_filings(self, filings):
        """å°†å…¬å‘Šåˆ†ä¸ºä¸¤ç»„ï¼šå­£æŠ¥å¹´æŠ¥ç»„å’Œå…¶ä»–ç»„"""
        # å­£æŠ¥å¹´æŠ¥å…³é”®è¯ï¼ˆåŸºäºheadline divå†…å®¹ï¼‰
        quarterly_annual_keywords = [
            'Final Results',
            'Quarterly Results', 
            'Interim Results',
            'Offer for Subscription'
        ]
        
        quarterly_annual_filings = []
        other_filings = []
        
        for filing in filings:
            doc_type = filing['doc_type']
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å­£æŠ¥å¹´æŠ¥å…³é”®è¯
            is_quarterly_annual = any(keyword in doc_type for keyword in quarterly_annual_keywords)
            
            if is_quarterly_annual:
                quarterly_annual_filings.append(filing)
            else:
                other_filings.append(filing)
        
        return quarterly_annual_filings, other_filings
    
    def download_filing_content(self, filing_info):
        """ä¸‹è½½å•ä¸ªå…¬å‘Šæ–‡ä»¶å†…å®¹"""
        try:
            with httpx.Client(headers=self.headers, timeout=60) as client:
                response = client.get(filing_info['url'])
                response.raise_for_status()
                
                return response.content
                
        except Exception as e:
            logger.error(f"ä¸‹è½½æ¸¯è‚¡æ–‡ä»¶æ—¶å‡ºé”™ {filing_info['url']}: {str(e)}")
            return None

# è£…é¥°å™¨
def retry_on_failure(max_retries: int = config.MAX_RETRIES, delay: float = config.RETRY_DELAY):
    """é‡è¯•è£…é¥°å™¨"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            last_exception = None
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    logger.warning(f"å‡½æ•° {func.__name__} ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {e}")
                    if attempt < max_retries - 1:
                        time.sleep(delay * (2 ** attempt))  # æŒ‡æ•°é€€é¿
            
            logger.error(f"å‡½æ•° {func.__name__} åœ¨ {max_retries} æ¬¡å°è¯•åä»ç„¶å¤±è´¥")
            raise last_exception
        return wrapper
    return decorator

@contextmanager
def error_handler(operation_name: str):
    """é”™è¯¯å¤„ç†ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    try:
        yield
    except Exception as e:
        logger.error(f"{operation_name} å¤±è´¥: {e}")
        st.error(f"{operation_name} å¤±è´¥: {e}")
        raise

# Session State ç®¡ç†
class SessionManager:
    """SessionçŠ¶æ€ç®¡ç†å™¨"""
    
    @staticmethod
    def init_session_state():
        """åˆå§‹åŒ–æ‰€æœ‰å¿…è¦çš„session stateå˜é‡"""
        
        # æ·»åŠ å›é€€æœºåˆ¶ï¼šå…ˆæ£€æŸ¥ secrets.toml æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨å°±ä½¿ç”¨ st.secretsï¼Œå¦åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
        def get_secret_value(key: str, default=None):
            """ä» st.secrets æˆ–ç¯å¢ƒå˜é‡ä¸­è·å–å¯†é’¥å€¼"""
            import os
            import json
            from pathlib import Path
            
            # æ£€æŸ¥ secrets.toml æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            secrets_paths = [
                Path(".streamlit/secrets.toml"),
                Path("/root/.streamlit/secrets.toml"),
                Path("/app/.streamlit/secrets.toml")
            ]
            
            secrets_file_exists = any(path.exists() for path in secrets_paths)
            
            if secrets_file_exists:
                try:
                    return st.secrets[key]
                except KeyError:
                    # å¦‚æœ secrets.toml å­˜åœ¨ä½†æ²¡æœ‰è¯¥é”®ï¼Œå›é€€åˆ°ç¯å¢ƒå˜é‡
                    pass
            
            # ç›´æ¥ä»ç¯å¢ƒå˜é‡è¯»å–
            env_value = os.environ.get(key, default)
            if env_value is None:
                return default
                
            # å°è¯•è§£æ JSON æ ¼å¼çš„ç¯å¢ƒå˜é‡ï¼ˆç”¨äºåˆ—è¡¨ç±»å‹çš„å¯†é’¥ï¼‰
            if isinstance(env_value, str) and env_value.startswith('[') and env_value.endswith(']'):
                try:
                    return json.loads(env_value)
                except json.JSONDecodeError:
                    return env_value
            
            return env_value
        
        defaults = {
            "analyzer_messages": [],
            "analyzer_ticker": "",
            "analyzer_years": 1,
            "analyzer_use_sec_reports": True,
            "analyzer_use_sec_others": False,
            "analyzer_use_earnings": True,
            "analyzer_model": "gemini-2.5-flash",
            "api_key_cycle": cycle(get_secret_value("GOOGLE_API_KEYS", [])),
            "processing_status": ProcessingStatus().__dict__,
            "cache": {},
            "use_premium_api": False,
            "premium_access_code": "",
            "selected_language": "English",
            "analyzer_analysis_mode": "fast_mode"
        }
        
        for key, default_value in defaults.items():
            if key not in st.session_state:
                st.session_state[key] = default_value
    
    @staticmethod
    def get_processing_status() -> ProcessingStatus:
        """è·å–å¤„ç†çŠ¶æ€å¯¹è±¡"""
        # Get the field names from the dataclass
        known_fields = {f.name for f in fields(ProcessingStatus)}
        # Filter the session state dict to only include known fields
        current_status_dict = st.session_state.get("processing_status", {})
        filtered_args = {k: v for k, v in current_status_dict.items() if k in known_fields}
        return ProcessingStatus(**filtered_args)
    
    @staticmethod
    def update_processing_status(status: ProcessingStatus):
        """æ›´æ–°å¤„ç†çŠ¶æ€"""
        st.session_state.processing_status = status.__dict__

# AI æœåŠ¡
class GeminiService:
    """Gemini AIæœåŠ¡"""
    
    def __init__(self):
        self.rate_limiter = RateLimiter(max_calls=30, window=60)
    
    def get_next_api_key(self) -> str:
        """è·å–ä¸‹ä¸€ä¸ªAPIå¯†é’¥"""
        # æª¢æŸ¥æ˜¯å¦ä½¿ç”¨ä»˜è²»API
        if st.session_state.get("use_premium_api", False):
            return self._get_secret_value("PREMIUM_API_KEY")
        
        # ä½¿ç”¨ä¸€èˆ¬çš„è¼ªæ›API
        if hasattr(st.session_state, 'api_key_cycle'):
            return next(st.session_state.api_key_cycle)
        else:
            # å¦‚æœsession stateæœªåˆå§‹åŒ–ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
            if not hasattr(self, '_api_key_cycle'):
                api_keys = self._get_secret_value("GOOGLE_API_KEYS", [])
                self._api_key_cycle = cycle(api_keys)
            return next(self._api_key_cycle)
    
    def _get_secret_value(self, key: str, default=None):
        """ä» st.secrets æˆ–ç¯å¢ƒå˜é‡ä¸­è·å–å¯†é’¥å€¼"""
        import os
        import json
        from pathlib import Path
        
        # æ£€æŸ¥ secrets.toml æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        secrets_paths = [
            Path(".streamlit/secrets.toml"),
            Path("/root/.streamlit/secrets.toml"),
            Path("/app/.streamlit/secrets.toml")
        ]
        
        secrets_file_exists = any(path.exists() for path in secrets_paths)
        
        if secrets_file_exists:
            try:
                return st.secrets[key]
            except KeyError:
                # å¦‚æœ secrets.toml å­˜åœ¨ä½†æ²¡æœ‰è¯¥é”®ï¼Œå›é€€åˆ°ç¯å¢ƒå˜é‡
                pass
        
        # ç›´æ¥ä»ç¯å¢ƒå˜é‡è¯»å–
        env_value = os.environ.get(key, default)
        if env_value is None:
            return default
            
        # å°è¯•è§£æ JSON æ ¼å¼çš„ç¯å¢ƒå˜é‡ï¼ˆç”¨äºåˆ—è¡¨ç±»å‹çš„å¯†é’¥ï¼‰
        if isinstance(env_value, str) and env_value.startswith('[') and env_value.endswith(']'):
            try:
                return json.loads(env_value)
            except json.JSONDecodeError:
                return env_value
        
        return env_value
    
    def init_client(self) -> genai.Client:
        """åˆå§‹åŒ–Geminiå®¢æˆ·ç«¯"""
        return genai.Client(api_key=self.get_next_api_key())
    
    @retry_on_failure(max_retries=3)
    def call_api(self, prompt: str, model_type: str = "gemini-2.5-flash") -> str:
        """è°ƒç”¨Gemini API"""
        self.rate_limiter.wait_if_needed()
        
        try:
            client = self.init_client()
            
            contents = [
                types.Content(
                    role="user",
                    parts=[types.Part.from_text(text=prompt)],
                ),
            ]

            response = client.models.generate_content(
                model=model_type,
                contents=contents,
            )

            return response.candidates[0].content.parts[0].text
            
        except Exception as e:
            logger.error(f"Gemini APIè°ƒç”¨å¤±è´¥: {e}")
            raise APIError(f"Gemini APIè°ƒç”¨å¤±è´¥: {e}")
    
    @retry_on_failure(max_retries=3)
    def call_api_stream(self, prompt: str, model_type: str = "gemini-2.5-flash"):
        """è°ƒç”¨Gemini API æµå¼å“åº”"""
        self.rate_limiter.wait_if_needed()
        
        try:
            client = self.init_client()
            
            contents = [
                types.Content(
                    role="user",
                    parts=[types.Part.from_text(text=prompt)],
                ),
            ]

            # ä½¿ç”¨æµå¼å“åº”
            response_stream = client.models.generate_content_stream(
                model=model_type,
                contents=contents,
            )
            
            # ç”Ÿæˆå™¨å‡½æ•°ï¼Œé€æ­¥è¿”å›æ–‡æœ¬ç‰‡æ®µ
            for chunk in response_stream:
                if chunk.candidates and chunk.candidates[0].content.parts:
                    yield chunk.candidates[0].content.parts[0].text
            
        except Exception as e:
            logger.error(f"Gemini APIæµå¼è°ƒç”¨å¤±è´¥: {e}")
            raise APIError(f"Gemini APIæµå¼è°ƒç”¨å¤±è´¥: {e}")
    
    @retry_on_failure(max_retries=3)
    def classify_6k_document(self, document_content: str) -> bool:
        """ä½¿ç”¨ä¾¿å®œæ¨¡å‹åˆ¤æ–­6-Kæ–‡ä»¶æ˜¯å¦ä¸ºå­£æŠ¥/å¹´æŠ¥/IPOæŠ¥å‘Š"""
        try:
            # è·å–å½“å‰è¯­è¨€è®¾ç½®
            language = st.session_state.get("selected_language", "English")
            
            if language == "English":
                prompt = f"""
                You are a financial document classifier. Please analyze the following 6-K filing content and determine if it is a Quarterly Report, Annual Report, or IPO Report.

                Classification criteria:
                - Quarterly Report: Contains quarterly financial results, earnings data, quarterly business updates
                - Annual Report: Contains annual financial results, yearly business summary, annual shareholder information
                - IPO Report: Contains initial public offering information, prospectus data, listing announcements

                Please respond with ONLY a JSON object in this exact format:
                {{
                    "is_quarterly_annual_ipo": true/false,
                    "document_type": "quarterly/annual/ipo/other",
                    "confidence": "high/medium/low"
                }}

                Document content (first 5000 characters):
                {document_content[:5000]}
                """
            else:
                prompt = f"""
                ä½ æ˜¯ä¸€ä¸ªé‡‘èæ–‡æ¡£åˆ†ç±»å™¨ã€‚è¯·åˆ†æä»¥ä¸‹6-Kæ–‡ä»¶å†…å®¹ï¼Œåˆ¤æ–­å®ƒæ˜¯å¦æ˜¯å­£æŠ¥ã€å¹´æŠ¥æˆ–IPOæŠ¥å‘Šã€‚

                åˆ†ç±»æ ‡å‡†ï¼š
                - å­£æŠ¥ï¼šåŒ…å«å­£åº¦è´¢åŠ¡ç»“æœã€ç›ˆåˆ©æ•°æ®ã€å­£åº¦ä¸šåŠ¡æ›´æ–°
                - å¹´æŠ¥ï¼šåŒ…å«å¹´åº¦è´¢åŠ¡ç»“æœã€å¹´åº¦ä¸šåŠ¡æ€»ç»“ã€å¹´åº¦è‚¡ä¸œä¿¡æ¯
                - IPOæŠ¥å‘Šï¼šåŒ…å«é¦–æ¬¡å…¬å¼€å‘è¡Œä¿¡æ¯ã€æ‹›è‚¡è¯´æ˜ä¹¦æ•°æ®ã€ä¸Šå¸‚å…¬å‘Š

                è¯·åªå›ç­”JSONæ ¼å¼ï¼š
                {{
                    "is_quarterly_annual_ipo": true/false,
                    "document_type": "quarterly/annual/ipo/other",
                    "confidence": "high/medium/low"
                }}

                æ–‡æ¡£å†…å®¹ï¼ˆå‰5000å­—ç¬¦ï¼‰ï¼š
                {document_content[:5000]}
                """
            
            result = self.call_api(prompt, "gemini-2.5-flash-lite-preview-06-17")
            
            # å°è¯•è§£æJSON
            try:
                # å°è¯•ä»Markdownä»£ç å—ä¸­æå–JSON
                import re
                match = re.search(r"```json\s*(\{.*?\})\s*```", result, re.DOTALL)
                if match:
                    json_str = match.group(1)
                else:
                    json_str = result
                
                import json
                classification = json.loads(json_str)
                return classification.get("is_quarterly_annual_ipo", False)
                
            except (json.JSONDecodeError, ValueError) as e:
                logger.warning(f"è§£æ6-Kåˆ†ç±»JSONå¤±è´¥: {e}. æ¨¡å‹è¿”å›: {result}")
                # å¦‚æœè§£æå¤±è´¥ï¼Œä¿å®ˆå¤„ç†ï¼Œè¿”å›Trueç»§ç»­åˆ†æ
                return True
                
        except Exception as e:
            logger.error(f"6-Kæ–‡æ¡£åˆ†ç±»å¤±è´¥: {e}")
            # å¦‚æœåˆ†ç±»å¤±è´¥ï¼Œä¿å®ˆå¤„ç†ï¼Œè¿”å›Trueç»§ç»­åˆ†æ
            return True

# SEC æœåŠ¡
class SECService:
    """SECæ–‡ä»¶æœåŠ¡"""
    
    def __init__(self, cache_manager: CacheManager):
        self.rate_limiter = RateLimiter(max_calls=30, window=60)
        warnings.filterwarnings("ignore", category=XMLParsedAsHTMLWarning)
        self.cache_manager = cache_manager
        self.sixk_processor = None  # å°†åœ¨éœ€è¦æ—¶åˆå§‹åŒ–
    
    def _init_sixk_processor(self, temp_dir: str):
        """åˆå§‹åŒ–6-Kå¤„ç†å™¨"""
        if self.sixk_processor is None:
            self.sixk_processor = SixKProcessor(temp_dir)

    @lru_cache(maxsize=100)
    def get_cik_map(self) -> Dict[str, str]:
        """è·å–tickeråˆ°CIKçš„æ˜ å°„ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        cache_key = "cik_map"
        cached_result = self.cache_manager.get(cache_key)
        if cached_result:
            return cached_result
        
        try:
            headers = {'User-Agent': config.SEC_USER_AGENT}
            response = httpx.get(
                "https://www.sec.gov/files/company_tickers.json", 
                headers=headers,
                timeout=config.REQUEST_TIMEOUT
            )
            response.raise_for_status()
            
            all_companies = response.json()
            cik_map = {company['ticker']: company['cik_str'] for company in all_companies.values()}
            
            self.cache_manager.set(cache_key, cik_map)
            return cik_map
            
        except Exception as e:
            logger.error(f"è·å–CIKæ˜ å°„å¤±è´¥: {e}")
            raise DataRetrievalError(f"è·å–CIKæ˜ å°„å¤±è´¥: {e}")
    
    @retry_on_failure(max_retries=3)
    def get_filings(self, ticker: str, years: int = 3, forms_to_include: Optional[List[str]] = None, status_callback=None) -> List[Document]:
        """è·å–SECæ–‡ä»¶åˆ—è¡¨"""
        self.rate_limiter.wait_if_needed()
        
        # å¦‚æœæœªæä¾›è¦åŒ…å«çš„è¡¨å•ï¼Œåˆ™ä½¿ç”¨é…ç½®ä¸­çš„æ‰€æœ‰è¡¨å•
        if forms_to_include is None:
            forms_to_include = config.SEC_FORMS
        
        cache_key = self.cache_manager.get_cache_key("sec_filings", ticker, years, tuple(sorted(forms_to_include)))
        cached_result = self.cache_manager.get(cache_key)
        if cached_result:
            return cached_result
        
        try:
            edgar = EdgarClient(user_agent=config.SEC_USER_AGENT)
            
            # ä¿®æ­£å¹´ä»½è®¡ç®—é€»è¾‘ï¼šå¦‚æœæ˜¯2å¹´ï¼Œå°±æ˜¯2024/1/1åˆ°2025/1/1
            current_year = datetime.now().year
            end_date = datetime(current_year + 1, 1, 1)  # å»¶ä¼¸åˆ°ä¸‹ä¸€å¹´çš„1æœˆ1å·
            start_date = datetime(current_year - years + 1, 1, 1)  # å¾€å‰æ¨yearså¹´
            
            logger.info(f"SECæ–‡ä»¶æ—¥æœŸèŒƒå›´: {start_date.date()} åˆ° {end_date.date()}")
            
            ticker_map = self.get_cik_map()
            cik = ticker_map.get(ticker.upper())
            
            if not cik:
                logger.warning(f"æœªæ‰¾åˆ°ticker {ticker} çš„CIK")
                return []
            
            submissions = edgar.get_submissions(cik=str(cik).zfill(10))
            
            # è·å–æ‰€æœ‰æ–‡ä»¶æ•°æ®ï¼ˆåŒ…æ‹¬recentå’Œfilesï¼‰
            all_forms = []
            all_accession_numbers = []
            all_filing_dates = []
            all_primary_documents = []
            
            # å¤„ç†recentæ•°æ®
            recent = submissions.get('filings', {}).get('recent', {})
            if recent and 'form' in recent:
                all_forms.extend(recent.get('form', []))
                all_accession_numbers.extend(recent.get('accessionNumber', []))
                all_filing_dates.extend(recent.get('filingDate', []))
                all_primary_documents.extend(recent.get('primaryDocument', []))
                logger.info(f"SEC recentæ–‡ä»¶æ•°: {len(recent.get('form', []))}")
            
            # å¤„ç†filesæ•°æ®ï¼ˆå†å²æ–‡ä»¶ï¼‰
            files = submissions.get('filings', {}).get('files', [])
            if files:
                logger.info(f"SECéœ€è¦å¤„ç† {len(files)} ä¸ªå†å²æ–‡ä»¶æ‰¹æ¬¡")
                max_batches = 100  # æœ€å¤šå¤„ç†10ä¸ªæ‰¹æ¬¡ï¼Œé¿å…è¿‡å¤šAPIè°ƒç”¨
                processed_batches = 0
                
                for idx, file_info in enumerate(files):
                    if processed_batches >= max_batches:
                        logger.info(f"SECå·²å¤„ç† {max_batches} ä¸ªæ‰¹æ¬¡ï¼Œåœæ­¢å¤„ç†æ›´å¤šå†å²æ–‡ä»¶")
                        break
                        
                    file_name = file_info.get('name', '')
                    if file_name:
                        try:
                            if status_callback:
                                status_callback(f"æ­£åœ¨è·å–ç¬¬ {idx + 1}/{min(len(files), max_batches)} æ‰¹SECå†å²æ–‡ä»¶...")
                            # è·å–å†å²æ–‡ä»¶æ•°æ®
                            historical_data = edgar.get_submissions(cik=str(cik).zfill(10), file_name=file_name)
                            if historical_data and 'form' in historical_data:
                                batch_size = len(historical_data.get('form', []))
                                logger.info(f"SECå†å²æ–‡ä»¶æ‰¹æ¬¡ {file_name}: {batch_size} ä¸ªæ–‡ä»¶")
                                
                                # æ£€æŸ¥æ‰¹æ¬¡ä¸­æœ€æ—©çš„æ—¥æœŸï¼Œå¦‚æœå¤ªæ—©åˆ™åœæ­¢
                                batch_dates = historical_data.get('filingDate', [])
                                if batch_dates:
                                    earliest_date = min(batch_dates)
                                    earliest_datetime = datetime.strptime(earliest_date, '%Y-%m-%d').date()
                                    if earliest_datetime < start_date.date():
                                        logger.info(f"SECå†å²æ–‡ä»¶æ‰¹æ¬¡ {file_name} æœ€æ—©æ—¥æœŸ {earliest_datetime} æ—©äºæˆªæ­¢æ—¥æœŸ {start_date.date()}ï¼Œåœæ­¢å¤„ç†")
                                        break
                                
                                all_forms.extend(historical_data.get('form', []))
                                all_accession_numbers.extend(historical_data.get('accessionNumber', []))
                                all_filing_dates.extend(historical_data.get('filingDate', []))
                                all_primary_documents.extend(historical_data.get('primaryDocument', []))
                                processed_batches += 1
                        except Exception as e:
                            logger.warning(f"è·å–SECå†å²æ–‡ä»¶æ‰¹æ¬¡ {file_name} å¤±è´¥: {e}")
                            continue
            
            if not all_forms:
                logger.warning(f"æœªæ‰¾åˆ°SECæ–‡ä»¶æ•°æ®")
                return []
            
            logger.info(f"SECæ€»æ–‡ä»¶æ•°: {len(all_forms)}")
            
            documents = []
            cutoff_reached = False
            
            for i in range(len(all_forms)):
                form_type = all_forms[i]
                if form_type in forms_to_include:
                    filing_date = datetime.strptime(all_filing_dates[i], '%Y-%m-%d').date()
                    
                    # æ£€æŸ¥æ˜¯å¦æ—©äºæˆªæ­¢æ—¥æœŸ
                    if filing_date < start_date.date():
                        logger.info(f"SECæ–‡ä»¶æ—¥æœŸ {filing_date} æ—©äºæˆªæ­¢æ—¥æœŸ {start_date.date()}ï¼Œåœæ­¢å¤„ç†")
                        cutoff_reached = True
                        break
                    
                    if start_date.date() <= filing_date < end_date.date():  # ä¸åŒ…å«ç»“æŸæ—¥æœŸ
                        accession_no_no_dashes = all_accession_numbers[i].replace('-', '')
                        filing_url = f"https://www.sec.gov/Archives/edgar/data/{cik}/{accession_no_no_dashes}/{all_primary_documents[i]}"
                        
                        documents.append(Document(
                            type='SEC Filing',
                            title=f"{ticker} {form_type}",
                            date=filing_date,
                            url=filing_url,
                            form_type=form_type
                        ))
            
            # æŒ‰æ—¥æœŸæ’åºï¼ˆæ–°åˆ°æ—§ï¼‰
            documents.sort(key=lambda x: x.date, reverse=True)
            
            logger.info(f"æ‰¾åˆ° {len(documents)} ä¸ªSECæ–‡ä»¶")
            self.cache_manager.set(cache_key, documents)
            return documents
            
        except Exception as e:
            logger.error(f"è·å–SECæ–‡ä»¶å¤±è´¥: {e}")
            raise DataRetrievalError(f"è·å–SECæ–‡ä»¶å¤±è´¥: {e}")
    
    @retry_on_failure(max_retries=3)
    def download_filing(self, filing_url: str) -> str:
        """ä¸‹è½½SECæ–‡ä»¶å†…å®¹"""
        self.rate_limiter.wait_if_needed()
        
        try:
            response = httpx.get(
                filing_url, 
                headers={"User-Agent": config.SEC_USER_AGENT},
                timeout=config.REQUEST_TIMEOUT
            )
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'lxml')
            document_tag = soup.find('document')
            
            content = document_tag.get_text(separator='\n', strip=True) if document_tag else soup.get_text(separator='\n', strip=True)
            
            # é™åˆ¶å†…å®¹é•¿åº¦
            if len(content) > config.MAX_CONTENT_LENGTH:
                content = content[:config.MAX_CONTENT_LENGTH] + "\n[å†…å®¹å·²æˆªæ–­]"
            
            return content
            
        except Exception as e:
            logger.error(f"ä¸‹è½½SECæ–‡ä»¶å¤±è´¥: {e}")
            return f"ä¸‹è½½æ–‡ä»¶æ—¶å‡ºé”™: {e}"

# æ¸¯è‚¡æœåŠ¡
class HKStockService:
    """æ¸¯è‚¡æ–‡ä»¶æœåŠ¡"""
    
    def __init__(self, cache_manager: CacheManager):
        self.rate_limiter = RateLimiter(max_calls=30, window=60)
        self.cache_manager = cache_manager
        self.downloader = HKStockFilingsDownloader()
    
    def parse_hk_date(self, date_str: str) -> Optional[datetime.date]:
        """è§£ææ¸¯è‚¡æ—¥æœŸæ ¼å¼"""
        try:
            # æ¸¯è‚¡æ—¥æœŸæ ¼å¼ï¼šDD/MM/YYYY HH:MM æˆ– Release Time:DD/MM/YYYY HH:MM
            date_part = date_str
            if 'Release Time:' in date_str:
                date_part = date_str.replace('Release Time:', '').strip()
            
            if '/' in date_part and ' ' in date_part:
                date_only = date_part.split(' ')[0]  # å–æ—¥æœŸéƒ¨åˆ†
                parts = date_only.split('/')
                if len(parts) == 3:
                    day, month, year = parts
                    return datetime(int(year), int(month), int(day)).date()
            return None
        except Exception as e:
            logger.warning(f"è§£ææ¸¯è‚¡æ—¥æœŸå¤±è´¥: {date_str} - {e}")
            return None
    
    @retry_on_failure(max_retries=3)
    def get_hk_filings(self, ticker: str, years: int = 3, forms_to_include: Optional[List[str]] = None, status_callback=None) -> List[Document]:
        """è·å–æ¸¯è‚¡æ–‡ä»¶åˆ—è¡¨"""
        self.rate_limiter.wait_if_needed()
        
        # å¦‚æœæœªæä¾›è¦åŒ…å«çš„è¡¨å•ï¼Œåˆ™ä½¿ç”¨é»˜è®¤çš„å­£æŠ¥å¹´æŠ¥
        if forms_to_include is None:
            forms_to_include = ['quarterly_annual']
        
        cache_key = self.cache_manager.get_cache_key("hk_filings", ticker, years, tuple(sorted(forms_to_include)))
        cached_result = self.cache_manager.get(cache_key)
        if cached_result:
            return cached_result
        
        try:
            # è·å–è‚¡ç¥¨ID
            if status_callback:
                status_callback("æ­£åœ¨è·å–æ¸¯è‚¡ID...")
            stock_id, stock_code, stock_name = self.downloader.get_stock_id(ticker)
            if not stock_id:
                logger.warning(f"æœªæ‰¾åˆ°æ¸¯è‚¡ticker {ticker} çš„ä¿¡æ¯")
                return []
            
            # è®¡ç®—æ—¥æœŸèŒƒå›´
            current_year = datetime.now().year
            end_date = datetime(current_year + 1, 1, 1)
            start_date = datetime(current_year - years + 1, 1, 1)
            
            # è½¬æ¢ä¸ºæ¸¯è‚¡APIæ ¼å¼
            from_date = start_date.strftime("%Y%m%d")
            to_date = end_date.strftime("%Y%m%d")
            
            logger.info(f"æ¸¯è‚¡æ–‡ä»¶æ—¥æœŸèŒƒå›´: {start_date.date()} åˆ° {end_date.date()}")
            
            # è·å–å…¬å‘Šåˆ—è¡¨
            if status_callback:
                status_callback("æ­£åœ¨è·å–æ¸¯è‚¡å…¬å‘Šåˆ—è¡¨...")
            all_filings = self.downloader.get_filings_list(stock_id, from_date, to_date, from_date, status_callback)
            
            if not all_filings:
                logger.warning(f"æœªæ‰¾åˆ°æ¸¯è‚¡ {ticker} çš„å…¬å‘Š")
                return []
            
            # åˆ†ç±»å…¬å‘Š
            quarterly_annual_filings, other_filings = self.downloader.categorize_filings(all_filings)
            
            # æ ¹æ®ç”¨æˆ·é€‰æ‹©å†³å®šä½¿ç”¨å“ªäº›å…¬å‘Š
            selected_filings = []
            if 'quarterly_annual' in forms_to_include:
                selected_filings.extend(quarterly_annual_filings)
            if 'others' in forms_to_include:
                selected_filings.extend(other_filings)
            
            # è½¬æ¢ä¸ºDocumentå¯¹è±¡
            documents = []
            for filing in selected_filings:
                # è§£ææ—¥æœŸ
                filing_date = self.parse_hk_date(filing['release_time'])
                if filing_date and start_date.date() <= filing_date < end_date.date():
                    documents.append(Document(
                        type='HK Stock Filing',
                        title=f"{stock_code} {filing['doc_type']} - {filing['doc_title']}",
                        date=filing_date,
                        url=filing['url'],
                        form_type=filing['doc_type'],
                        content=None  # å†…å®¹å°†åœ¨éœ€è¦æ—¶ä¸‹è½½
                    ))
            
            # æŒ‰æ—¥æœŸæ’åºï¼ˆæ–°åˆ°æ—§ï¼‰
            documents.sort(key=lambda x: x.date, reverse=True)
            
            logger.info(f"æ‰¾åˆ° {len(documents)} ä¸ªæ¸¯è‚¡æ–‡ä»¶")
            self.cache_manager.set(cache_key, documents)
            return documents
            
        except Exception as e:
            logger.error(f"è·å–æ¸¯è‚¡æ–‡ä»¶å¤±è´¥: {e}")
            raise DataRetrievalError(f"è·å–æ¸¯è‚¡æ–‡ä»¶å¤±è´¥: {e}")
    
    @retry_on_failure(max_retries=3)
    def download_hk_filing(self, filing_url: str) -> str:
        """ä¸‹è½½æ¸¯è‚¡æ–‡ä»¶å†…å®¹"""
        self.rate_limiter.wait_if_needed()
        
        try:
            # æ„å»ºfiling_infoå¯¹è±¡
            filing_info = {'url': filing_url}
            
            # ä¸‹è½½PDFå†…å®¹
            pdf_content = self.downloader.download_filing_content(filing_info)
            
            if not pdf_content:
                return "ä¸‹è½½æ¸¯è‚¡æ–‡ä»¶å¤±è´¥"
            
            # ä½¿ç”¨PyMuPDFå¤„ç†PDF
            with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as temp_file:
                temp_file.write(pdf_content)
                temp_file_path = temp_file.name
            
            try:
                # ä½¿ç”¨PyMuPDFæå–æ–‡æœ¬
                doc = fitz.open(temp_file_path)
                text = ""
                
                for page_num in range(doc.page_count):
                    page = doc[page_num]
                    page_text = page.get_text()
                    if page_text.strip():
                        text += f"\n--- ç¬¬ {page_num + 1} é¡µ ---\n"
                        text += page_text
                        text += "\n"
                
                doc.close()
                
                # æ¸…ç†æ–‡æœ¬
                text = re.sub(r'\n{3,}', '\n\n', text)
                
                # é™åˆ¶å†…å®¹é•¿åº¦
                if len(text) > config.MAX_CONTENT_LENGTH:
                    text = text[:config.MAX_CONTENT_LENGTH] + "\n[å†…å®¹å·²æˆªæ–­]"
                
                return text.strip()
                
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                os.unlink(temp_file_path)
            
        except Exception as e:
            logger.error(f"ä¸‹è½½æ¸¯è‚¡æ–‡ä»¶å¤±è´¥: {e}")
            return f"ä¸‹è½½æ¸¯è‚¡æ–‡ä»¶æ—¶å‡ºé”™: {e}"

# æ²™ç‰¹äº¤æ˜“æ‰€æœåŠ¡
class SaudiExchangeService:
    """æ²™ç‰¹äº¤æ˜“æ‰€å…¬å‘ŠæœåŠ¡"""
    
    def __init__(self, cache_manager: CacheManager):
        self.rate_limiter = RateLimiter(max_calls=30, window=60)
        self.cache_manager = cache_manager
        self.session = requests.Session()
        self.base_url = "https://www.saudiexchange.sa"
        self.announcement_api = "https://www.saudiexchange.sa/wps/portal/saudiexchange/newsandreports/issuer-news/issuer-announcements/!ut/p/z1/lY_NDoIwHMOfhQcwqxD-zOPUODAgTBjiLmYHY0h0ejA-v8Qb-BHsrcmvacsMa5hx9tGe7L29Onvu_N7QIRQEP-bIEVcLEEpJuuLTpU9s1wd4JglqI1TuRyFkDWb-yqMsQqhVkQUptpCgcXl8kRjRb_pILmZRt2A9l0kqAk7REPhwcVDy_uEF_BhZHh27XbRu0CYT4XlP_MzK5g!!/p0/IZ7_5A602H80O0HTC060SG6UT81DI1=CZ6_5A602H80O0HTC060SG6UT81D26=NJgetAnnouncementListData=/"
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.headers = {
            "accept": "*/*",
            "accept-language": "en-US,en;q=0.9,zh-TW;q=0.8,zh;q=0.7",
            "cache-control": "no-cache",
            "content-type": "application/x-www-form-urlencoded; charset=UTF-8",
            "pragma": "no-cache",
            "priority": "u=1, i",
            "sec-ch-ua": '"Not)A;Brand";v="8", "Chromium";v="138", "Google Chrome";v="138"',
            "sec-ch-ua-mobile": "?0",
            "sec-ch-ua-platform": '"macOS"',
            "sec-fetch-dest": "empty",
            "sec-fetch-mode": "cors",
            "sec-fetch-site": "same-origin",
            "x-requested-with": "XMLHttpRequest",
            "referer": "https://www.saudiexchange.sa/wps/portal/saudiexchange/newsandreports/issuer-news/issuer-announcements?locale=en&page=1",
            "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36"
        }
    
    def parse_saudi_date(self, date_str: str) -> Optional[datetime.date]:
        """è§£ææ²™ç‰¹äº¤æ˜“æ‰€æ—¥æœŸæ ¼å¼: 27/05/2025 15:51:58 (dd/MM/yyyy HH:mm:ss)"""
        try:
            # æ ¼å¼: dd/MM/yyyy HH:mm:ss
            dt = datetime.strptime(date_str, '%d/%m/%Y %H:%M:%S')
            return dt.date()
        except Exception as e:
            logger.warning(f"è§£ææ²™ç‰¹æ—¥æœŸå¤±è´¥: {date_str}, é”™è¯¯: {e}")
            return None
    
    @retry_on_failure(max_retries=3)
    def get_saudi_filings(self, symbol: str, years: int = 3, status_callback=None) -> List[Document]:
        """è·å–æ²™ç‰¹äº¤æ˜“æ‰€å…¬å‘Šåˆ—è¡¨"""
        self.rate_limiter.wait_if_needed()
        
        cache_key = self.cache_manager.get_cache_key("saudi_filings", symbol, years)
        cached_result = self.cache_manager.get(cache_key)
        if cached_result:
            return cached_result
        
        try:
            # è®¡ç®—æ—¥æœŸèŒƒå›´
            current_date = datetime.now()
            cutoff_date = current_date - timedelta(days=years * 365)
            
            logger.info(f"è·å–æ²™ç‰¹äº¤æ˜“æ‰€ {symbol} å…¬å‘Šï¼Œæ—¥æœŸèŒƒå›´: {cutoff_date.date()} åˆ° {current_date.date()}")
            
            all_documents = []
            page = 1
            page_size = 50  # æ¯é¡µè·å–æ›´å¤šæ•°æ®
            
            while True:
                if status_callback:
                    status_callback(f"æ­£åœ¨è·å–ç¬¬ {page} é¡µå…¬å‘Š...")
                
                # æ„å»ºè¯·æ±‚æ•°æ®
                post_data = {
                    "annoucmentType": "1_-1",
                    "symbol": symbol,
                    "sectorDpId": "",
                    "searchType": "",
                    "fromDate": "",
                    "toDate": "",
                    "datePeriod": "",
                    "productType": "",
                    "advisorsList": "",
                    "textSearch": "",
                    "pageNumberDb": str(page),
                    "pageSize": str(page_size)
                }
                
                logger.info(f"ğŸ” [SAUDI] è¯·æ±‚ç¬¬ {page} é¡µï¼Œsymbol: {symbol}")
                
                response = self.session.post(
                    self.announcement_api,
                    data=post_data,
                    headers=self.headers,
                    timeout=30
                )
                response.raise_for_status()
                
                # è§£æå“åº”
                try:
                    data = response.json()
                    logger.info(f"ğŸ” [SAUDI] ç¬¬ {page} é¡µå“åº”æˆåŠŸï¼Œæ•°æ®ç±»å‹: {type(data)}")
                except json.JSONDecodeError as e:
                    logger.error(f"JSONè§£æå¤±è´¥: {e}")
                    logger.error(f"å“åº”å†…å®¹: {response.text[:500]}")
                    break
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å…¬å‘Šæ•°æ®
                announcements = data.get('announcementList', [])
                total_count = data.get('totalCount', 0)
                
                logger.info(f"ğŸ” [SAUDI] ç¬¬ {page} é¡µæ‰¾åˆ° {len(announcements)} ä¸ªå…¬å‘Šï¼Œæ€»æ•°: {total_count}")
                
                if not announcements:
                    logger.info(f"ç¬¬ {page} é¡µæ²¡æœ‰å…¬å‘Šï¼Œåœæ­¢è·å–")
                    break
                
                # å¤„ç†å½“å‰é¡µçš„å…¬å‘Š
                page_documents = []
                for announcement in announcements:
                    try:
                        # è§£ææ—¥æœŸ
                        date_str = announcement.get('newsDateStr', '')
                        filing_date = self.parse_saudi_date(date_str)
                        
                        if not filing_date:
                            logger.warning(f"è·³è¿‡æ—¥æœŸè§£æå¤±è´¥çš„å…¬å‘Š: {date_str}")
                            continue
                        
                        # æ£€æŸ¥æ—¥æœŸæ˜¯å¦åœ¨èŒƒå›´å†…
                        if filing_date < cutoff_date.date():
                            logger.info(f"å…¬å‘Šæ—¥æœŸ {filing_date} æ—©äºæˆªæ­¢æ—¥æœŸ {cutoff_date.date()}ï¼Œåœæ­¢å¤„ç†")
                            # å…ˆå°†å½“å‰é¡µå·²å¤„ç†çš„æ–‡æ¡£æ·»åŠ åˆ°all_documentsï¼Œç„¶ååœæ­¢
                            if page_documents:
                                logger.info(f"ğŸ” [SAUDI] åœæ­¢å‰æ·»åŠ å½“å‰é¡µæ–‡æ¡£: {len(page_documents)} ä¸ª")
                                all_documents.extend(page_documents)
                                logger.info(f"ğŸ” [SAUDI] åœæ­¢å‰all_documentsé•¿åº¦: {len(all_documents)}")
                            return all_documents
                        
                        # è·å–å…¬å‘ŠURL
                        announcement_url = announcement.get('announcementUrl', '')
                        if not announcement_url:
                            logger.warning("å…¬å‘ŠURLä¸ºç©ºï¼Œè·³è¿‡")
                            continue
                        
                        # æ„å»ºå®Œæ•´URL
                        if not announcement_url.startswith('http'):
                            announcement_url = urljoin(self.base_url, announcement_url)
                        
                        # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
                        raw_title = announcement.get('announcementTitle', '').strip()
                        if not raw_title:
                            title = f"{symbol} Announcement"
                        elif raw_title.startswith(symbol):
                            title = raw_title  # å¦‚æœæ ‡é¢˜å·²ç»åŒ…å«symbolï¼Œç›´æ¥ä½¿ç”¨
                        else:
                            title = f"{symbol} - {raw_title}"  # å¦åˆ™æ·»åŠ å‰ç¼€
                        
                        document = Document(
                            type='Saudi Exchange Filing',
                            title=title,
                            date=filing_date,
                            url=announcement_url,
                            form_type="Saudi Announcement"
                        )
                        
                        page_documents.append(document)
                        logger.info(f"âœ… [SAUDI] æ·»åŠ å…¬å‘Š: {title}, æ—¥æœŸ: {filing_date}")
                        
                    except Exception as e:
                        logger.warning(f"å¤„ç†å…¬å‘Šå¤±è´¥: {e}")
                        continue
                
                logger.info(f"ğŸ” [SAUDI] ç¬¬{page}é¡µå¤„ç†å®Œæˆ: page_documentsé•¿åº¦={len(page_documents)}")
                all_documents.extend(page_documents)
                logger.info(f"ğŸ” [SAUDI] extendå: all_documentsé•¿åº¦={len(all_documents)}")
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦ç»§ç»­ç¿»é¡µ
                if len(announcements) < page_size:
                    logger.info(f"ç¬¬ {page} é¡µå…¬å‘Šæ•°é‡ {len(announcements)} å°äºé¡µé¢å¤§å° {page_size}ï¼Œåœæ­¢ç¿»é¡µ")
                    break
                
                page += 1
                
                # é˜²æ­¢æ— é™å¾ªç¯
                if page > 100:
                    logger.warning("å·²è·å–100é¡µï¼Œåœæ­¢ç¿»é¡µ")
                    break
                
                # çŸ­æš‚å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(0.5)
            
            # æŒ‰æ—¥æœŸæ’åºï¼ˆæ–°åˆ°æ—§ï¼‰
            all_documents.sort(key=lambda x: x.date, reverse=True)
            
            logger.info(f"âœ… [SAUDI] å…±è·å– {len(all_documents)} ä¸ªæ²™ç‰¹äº¤æ˜“æ‰€å…¬å‘Š")
            logger.info(f"ğŸ” [SAUDI] è¿”å›å‰è°ƒè¯•: all_documentsç±»å‹={type(all_documents)}, é•¿åº¦={len(all_documents)}")
            if all_documents:
                for i, doc in enumerate(all_documents[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
                    logger.info(f"ğŸ” [SAUDI] all_documents[{i}]: {doc.title}, ç±»å‹={doc.type}, æ—¥æœŸ={doc.date}")
            
            # ç¼“å­˜ç»“æœ
            self.cache_manager.set(cache_key, all_documents)
            return all_documents
            
        except Exception as e:
            logger.error(f"è·å–æ²™ç‰¹äº¤æ˜“æ‰€å…¬å‘Šå¤±è´¥: {e}")
            raise DataRetrievalError(f"è·å–æ²™ç‰¹äº¤æ˜“æ‰€å…¬å‘Šå¤±è´¥: {e}")
    
    @retry_on_failure(max_retries=3)
    def download_saudi_filing(self, filing_url: str) -> str:
        """ä¸‹è½½æ²™ç‰¹äº¤æ˜“æ‰€å…¬å‘Šå†…å®¹ï¼ŒåŒ…æ‹¬PDFé™„ä»¶"""
        self.rate_limiter.wait_if_needed()
        
        try:
            logger.info(f"ğŸ” [SAUDI] å¼€å§‹ä¸‹è½½å…¬å‘Šå†…å®¹: {filing_url}")
            
            # é¦–å…ˆè·å–HTMLé¡µé¢
            response = self.session.get(filing_url, headers=self.headers, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æå–ä¸»è¦å†…å®¹
            content = ""
            
            # ä½¿ç”¨ newspaper3k æå–æ–‡ç« å†…å®¹
            try:
                article = Article(filing_url)
                article.download()
                article.parse()
                main_content = article.text
                
                if main_content and len(main_content.strip()) > 50:
                    content += "=== å…¬å‘Šä¸»è¦å†…å®¹ ===\n"
                    content += main_content.strip()
                    content += "\n\n"
            except Exception as e:
                logger.warning(f"newspaper3k æå–å¤±è´¥: {e}")
            
            # å¦‚æœä¸»è¦å†…å®¹ä¸ºç©ºï¼Œå°è¯•ç›´æ¥è§£æHTML
            if not content:
                # ç§»é™¤è„šæœ¬å’Œæ ·å¼
                for script in soup(["script", "style"]):
                    script.decompose()
                
                main_content = soup.get_text(separator='\n', strip=True)
                if main_content:
                    content += "=== å…¬å‘Šä¸»è¦å†…å®¹ ===\n"
                    content += main_content
                    content += "\n\n"
            
            # æ£€æŸ¥PDFé™„ä»¶
            pdf_attachments = self._extract_pdf_attachments(soup, filing_url)
            
            if pdf_attachments:
                logger.info(f"ğŸ” [SAUDI-PDF] å‘ç° {len(pdf_attachments)} ä¸ªPDFé™„ä»¶")
                
                for i, pdf_info in enumerate(pdf_attachments):
                    try:
                        pdf_content = self._download_and_extract_pdf(pdf_info['url'], pdf_info['filename'])
                        if pdf_content:
                            content += f"=== PDFé™„ä»¶ {i+1}: {pdf_info['filename']} ===\n"
                            content += pdf_content
                            content += "\n\n"
                            logger.info(f"âœ… [SAUDI-PDF] æˆåŠŸæå–PDFå†…å®¹: {pdf_info['filename']}")
                        else:
                            logger.warning(f"âŒ [SAUDI-PDF] PDFå†…å®¹æå–å¤±è´¥: {pdf_info['filename']}")
                    except Exception as e:
                        logger.error(f"âŒ [SAUDI-PDF] å¤„ç†PDFé™„ä»¶å¤±è´¥: {pdf_info['filename']} - {e}")
                        content += f"=== PDFé™„ä»¶ {i+1}: {pdf_info['filename']} (å¤„ç†å¤±è´¥) ===\n"
                        content += f"PDFå¤„ç†å¤±è´¥: {str(e)}\n\n"
            
            # é™åˆ¶å†…å®¹é•¿åº¦
            if len(content) > config.MAX_CONTENT_LENGTH:
                content = content[:config.MAX_CONTENT_LENGTH] + "\n[å†…å®¹å·²æˆªæ–­]"
            
            logger.info(f"âœ… [SAUDI] æˆåŠŸæå–å†…å®¹ï¼Œé•¿åº¦: {len(content)}")
            return content.strip() if content.strip() else "æœªèƒ½æå–åˆ°å†…å®¹"
            
        except Exception as e:
            logger.error(f"ä¸‹è½½æ²™ç‰¹äº¤æ˜“æ‰€å…¬å‘Šå¤±è´¥: {e}")
            return f"ä¸‹è½½å¤±è´¥: {str(e)}"
    
    def download_saudi_filings_batch(self, documents: List[Document], max_workers: int = 5, status_callback=None) -> List[Document]:
        """æ‰¹é‡å¹¶å‘ä¸‹è½½æ²™ç‰¹äº¤æ˜“æ‰€å…¬å‘Šå†…å®¹"""
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import streamlit as st
        
        def download_single_filing(doc_with_index):
            index, document = doc_with_index
            try:
                content = self.download_saudi_filing(document.url)
                document.content = content
                logger.info(f"âœ… [SAUDI-BATCH] å®Œæˆä¸‹è½½ {index+1}/{len(saudi_docs)}: {document.title[:50]}...")
                return index, document
            except Exception as e:
                logger.error(f"âŒ [SAUDI-BATCH] ä¸‹è½½å¤±è´¥ {index+1}/{len(saudi_docs)}: {document.title[:50]}... - {e}")
                document.content = f"ä¸‹è½½å¤±è´¥: {str(e)}"
                return index, document
        
        # è¿‡æ»¤å‡ºéœ€è¦ä¸‹è½½å†…å®¹çš„æ²™ç‰¹æ–‡æ¡£
        saudi_docs = [doc for doc in documents if doc.type == 'Saudi Exchange Filing' and not doc.content]
        
        if not saudi_docs:
            logger.info("ğŸ” [SAUDI-BATCH] æ²¡æœ‰éœ€è¦ä¸‹è½½çš„æ²™ç‰¹æ–‡æ¡£")
            return documents
        
        logger.info(f"ğŸš€ [SAUDI-BATCH] å¼€å§‹æ‰¹é‡ä¸‹è½½ {len(saudi_docs)} ä¸ªæ²™ç‰¹å…¬å‘Šï¼Œå¹¶å‘æ•°: {max_workers}")
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘ä¸‹è½½
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä¸‹è½½ä»»åŠ¡
            future_to_index = {
                executor.submit(download_single_filing, (i, doc)): i 
                for i, doc in enumerate(saudi_docs)
            }
            
            completed_count = 0
            results = {}
            
            # åˆå§‹åŒ–çŠ¶æ€æ˜¾ç¤º
            if status_callback:
                status_callback("ğŸš€ å¼€å§‹æ‰¹é‡ä¸‹è½½æ²™ç‰¹äº¤æ˜“æ‰€å…¬å‘Š...", 0, len(saudi_docs))
            
            for future in as_completed(future_to_index):
                completed_count += 1
                try:
                    index, result_doc = future.result()
                    results[index] = result_doc
                    logger.info(f"ğŸ“Š [SAUDI-BATCH] è¿›åº¦: {completed_count}/{len(saudi_docs)} å®Œæˆ")
                    
                    # åœ¨ä¸»çº¿ç¨‹ä¸­æ›´æ–°çŠ¶æ€
                    if status_callback:
                        progress_percent = (completed_count / len(saudi_docs)) * 100
                        status_callback(f"ğŸ“Š Downloading: {completed_count}/{len(saudi_docs)} ({progress_percent:.0f}%)", completed_count, len(saudi_docs))
                        
                except Exception as e:
                    logger.error(f"âŒ [SAUDI-BATCH] ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
        
        # æœ€ç»ˆçŠ¶æ€æ›´æ–°
        if status_callback:
            status_callback(f"ğŸ‰ æ‰¹é‡ä¸‹è½½å®Œæˆ! å…± {len(saudi_docs)} ä¸ªæ–‡æ¡£", len(saudi_docs), len(saudi_docs))
        
        logger.info(f"ğŸ‰ [SAUDI-BATCH] æ‰¹é‡ä¸‹è½½å®Œæˆ! æ€»è®¡: {len(saudi_docs)} ä¸ªæ–‡æ¡£")
        return documents
    
    def _extract_pdf_attachments(self, soup: BeautifulSoup, base_url: str) -> List[Dict]:
        """ä»HTMLé¡µé¢æå–PDFé™„ä»¶é“¾æ¥"""
        pdf_attachments = []
        
        try:
            # æŸ¥æ‰¾æ‰€æœ‰trå…ƒç´ 
            for tr in soup.find_all('tr'):
                tds = tr.find_all('td')
                
                # æ£€æŸ¥æ˜¯å¦æœ‰è‡³å°‘ä¸¤ä¸ªtdï¼Œä¸”ç¬¬ä¸€ä¸ªtdåŒ…å«"Attached Documents"
                if len(tds) >= 2:
                    first_td = tds[0].get_text(strip=True)
                    if "Attached Documents" in first_td:
                        # æ£€æŸ¥ç¬¬äºŒä¸ªtdä¸­æ˜¯å¦æœ‰PDFé“¾æ¥
                        second_td = tds[1]
                        pdf_links = second_td.find_all('a', href=True)
                        
                        for link in pdf_links:
                            href = link['href']
                            if href.endswith('.pdf'):
                                # æ„å»ºå®Œæ•´URL
                                if href.startswith('/'):
                                    full_url = urljoin(self.base_url, href)
                                else:
                                    full_url = urljoin(base_url, href)
                                
                                # æå–æ–‡ä»¶å
                                filename = href.split('/')[-1]
                                
                                pdf_attachments.append({
                                    'url': full_url,
                                    'filename': filename,
                                    'original_href': href
                                })
                                
                                logger.info(f"ğŸ” [SAUDI-PDF] å‘ç°PDFé™„ä»¶: {filename} -> {full_url}")
            
            return pdf_attachments
            
        except Exception as e:
            logger.error(f"æå–PDFé™„ä»¶å¤±è´¥: {e}")
            return []
    
    def _download_and_extract_pdf(self, pdf_url: str, filename: str) -> str:
        """ä¸‹è½½PDFæ–‡ä»¶å¹¶æå–æ–‡æœ¬å†…å®¹"""
        import tempfile
        import os
        import fitz  # PyMuPDF
        
        try:
            logger.info(f"ğŸ” [SAUDI-PDF] å¼€å§‹ä¸‹è½½PDF: {filename}")
            
            # ä¸‹è½½PDFæ–‡ä»¶
            response = self.session.get(pdf_url, headers=self.headers, timeout=30)
            response.raise_for_status()
            
            # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as temp_file:
                temp_file.write(response.content)
                temp_file_path = temp_file.name
            
            try:
                # ä½¿ç”¨PyMuPDFæå–æ–‡æœ¬
                doc = fitz.open(temp_file_path)
                text = ""
                
                for page_num in range(doc.page_count):
                    page = doc[page_num]
                    page_text = page.get_text()
                    if page_text.strip():
                        text += f"\n--- ç¬¬ {page_num + 1} é¡µ ---\n"
                        text += page_text
                        text += "\n"
                
                doc.close()
                
                # æ¸…ç†æ–‡æœ¬
                import re
                text = re.sub(r'\n{3,}', '\n\n', text)
                
                logger.info(f"âœ… [SAUDI-PDF] PDFæ–‡æœ¬æå–æˆåŠŸ: {filename}, é•¿åº¦: {len(text)}")
                return text.strip()
                
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    os.unlink(temp_file_path)
                except:
                    pass
            
        except Exception as e:
            logger.error(f"ä¸‹è½½æˆ–æå–PDFå¤±è´¥ {filename}: {e}")
            return ""

# è´¢æŠ¥ä¼šè®®è®°å½•æœåŠ¡
class EarningsService:
    """è´¢æŠ¥ä¼šè®®è®°å½•æœåŠ¡"""
    
    def __init__(self, cache_manager: CacheManager):
        self.rate_limiter = RateLimiter(max_calls=30, window=60)
        self.cache_manager = cache_manager
        self.session = requests.Session() # ä½¿ç”¨æŒä¹…åŒ–ä¼šè¯å¤„ç†cookies
        self._lock = threading.Lock()  # æ·»åŠ çº¿ç¨‹é”ç”¨äºå¹¶è¡Œå¤„ç†

    @staticmethod
    def parse_transcript_url(url_path: str) -> Optional[Tuple[str, int, str]]:
        """
        ä»URLè·¯å¾„ä¸­è§£æè‚¡ç¥¨ä»£ç ã€å¹´ä»½å’Œå­£åº¦ä¿¡æ¯
        
        Args:
            url_path (str): transcript URLè·¯å¾„, ä¾‹å¦‚ '/company/BABA/transcripts/2025/4/'
        
        Returns:
            tuple: (ticker, year, quarter) æˆ– None
        """
        # å…è®¸tickerä¸­åŒ…å«ç‚¹(.)å’Œæ•°å­—
        pattern = r'/company/([A-Z0-9\.]+)/transcripts/(\d{4})/(\d+)/'
        match = re.match(pattern, url_path)
        
        if match:
            ticker, year, quarter = match.groups()
            return ticker, int(year), str(quarter)
        
        return None

    def get_earnings_transcript_batch(self, url_paths: List[str], max_workers: int = 1) -> List[Optional[Dict]]:
        """
        å¹¶è¡Œè·å–å¤šä¸ªè´¢æŠ¥ä¼šè®®è®°å½•
        
        Args:
            url_paths: URLè·¯å¾„åˆ—è¡¨
            max_workers: æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°
        
        Returns:
            List[Optional[Dict]]: è´¢æŠ¥è®°å½•ä¿¡æ¯åˆ—è¡¨ï¼Œä¸è¾“å…¥é¡ºåºä¸€è‡´
        """
        results = [None] * len(url_paths)  # é¢„åˆ†é…ç»“æœåˆ—è¡¨
        
        def process_single_transcript(index_url_pair):
            """å¤„ç†å•ä¸ªè´¢æŠ¥è®°å½•çš„å†…éƒ¨å‡½æ•°"""
            index, url_path = index_url_pair
            try:
                with self._lock:
                    # ä½¿ç”¨é”æ¥é™åˆ¶å¹¶å‘è¯·æ±‚
                    self.rate_limiter.wait_if_needed()
                
                result = self.get_earnings_transcript(url_path)
                return index, result
            except Exception as e:
                logger.error(f"å¹¶è¡Œå¤„ç†è´¢æŠ¥è®°å½•å¤±è´¥ {url_path}: {e}")
                return index, None
        
        # ä½¿ç”¨ThreadPoolExecutorè¿›è¡Œå¹¶è¡Œå¤„ç†
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_index = {
                executor.submit(process_single_transcript, (i, url_path)): i 
                for i, url_path in enumerate(url_paths)
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_index):
                try:
                    index, result = future.result()
                    results[index] = result
                except Exception as e:
                    index = future_to_index[future]
                    logger.error(f"å¹¶è¡Œå¤„ç†ä»»åŠ¡å¤±è´¥ index {index}: {e}")
                    results[index] = None
        
        return results

    @retry_on_failure(max_retries=3)
    def get_available_quarters(self, ticker: str) -> List[str]:
        """è·å–æŒ‡å®šå…¬å¸æ‰€æœ‰å¯ç”¨çš„å­£åº¦URLåˆ—è¡¨ (ä¸æµ‹è¯•è„šæœ¬å¯¹é½)"""
        self.rate_limiter.wait_if_needed()
        
        cache_key = self.cache_manager.get_cache_key("earnings_quarters_urls", ticker)
        cached_result = self.cache_manager.get(cache_key)
        if cached_result:
            logger.info(f"ä»ç¼“å­˜åŠ è½½ {ticker} çš„å¯ç”¨å­£åº¦åˆ—è¡¨")
            return cached_result
        
        try:
            ticker_upper = ticker.upper()
            logger.info(f"è·å– {ticker_upper} çš„æ‰€æœ‰å¯ç”¨å­£åº¦...")
            url = f"https://discountingcashflows.com/company/{ticker_upper}/transcripts/?org.htmx.cache-buster=baseContent"
            headers = {
                "accept": "*/*",
                "accept-language": "en-US,en;q=0.9,zh-TW;q=0.8,zh;q=0.7,zh-CN;q=0.6,ja;q=0.5",
                "hx-boosted": "true",
                "hx-current-url": f"https://discountingcashflows.com/company/{ticker_upper}/chart/",
                "hx-request": "true",
                "hx-target": "baseContent",
                "priority": "u=1, i",
                "sec-ch-ua": '"Not)A;Brand";v="8", "Chromium";v="138", "Google Chrome";v="138"',
                "sec-ch-ua-mobile": "?0",
                "sec-ch-ua-platform": '"macOS"',
                "sec-fetch-dest": "empty",
                "sec-fetch-mode": "cors",
                "sec-fetch-site": "same-origin",
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36"
            }
            
            response = self.session.get(url, headers=headers, timeout=config.REQUEST_TIMEOUT)
            logger.info(f"è·å–å¯ç”¨å­£åº¦åˆ—è¡¨ å“åº”çŠ¶æ€ç : {response.status_code}")
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            transcript_links = []
            
            pattern = re.compile(rf'/company/{ticker_upper}/transcripts/(\d{{4}})/(\d+)/')
            
            for link in soup.find_all('a', href=pattern):
                href = link.get('href')
                if href and href != f'/company/{ticker_upper}/transcripts/':
                    transcript_links.append(href)
            
            transcript_links = sorted(list(set(transcript_links)), reverse=True)
            logger.info(f"æ‰¾åˆ° {len(transcript_links)} ä¸ªå¯ç”¨å­£åº¦ for {ticker_upper}")
            
            self.cache_manager.set(cache_key, transcript_links)
            return transcript_links
            
        except Exception as e:
            logger.error(f"è·å–è´¢æŠ¥ä¼šè®®è®°å½•å­£åº¦åˆ—è¡¨å¤±è´¥: {e}")
            raise DataRetrievalError(f"è·å–è´¢æŠ¥ä¼šè®®è®°å½•å­£åº¦åˆ—è¡¨å¤±è´¥: {e}")
    
    def parse_date_text(self, date_text: str) -> Optional[datetime.date]:
        """è§£ææ—¥æœŸæ–‡æœ¬"""
        if not date_text:
            return None
        
        for fmt in config.DATE_FORMATS:
            try:
                return datetime.strptime(date_text, fmt).date()
            except ValueError:
                continue
        
        logger.warning(f"æ— æ³•è§£ææ—¥æœŸæ ¼å¼: {date_text}")
        return None
    
    @retry_on_failure(max_retries=3)
    def get_earnings_transcript(self, url_path: str) -> Optional[Dict]:
        """
        è·å–å•ä¸ªè´¢æŠ¥ä¼šè®®è®°å½•ï¼ŒåŒ…å«æ—¥æœŸè§£æå’Œæ–‡ä»¶ä¿å­˜ï¼Œä¸æµ‹è¯•è„šæœ¬é€»è¾‘å®Œå…¨å¯¹é½ã€‚
        """
        self.rate_limiter.wait_if_needed()

        parsed_info = self.parse_transcript_url(url_path)
        if not parsed_info:
            logger.error(f"æ— æ³•ä»URLè·¯å¾„è§£æä¿¡æ¯: {url_path}")
            return None
        ticker, year, quarter_num = parsed_info
        logger.info(f"--- å¼€å§‹è·å–: {ticker} Q{quarter_num} {year} ---")
        logger.info(f"é€‰æ‹©çš„URL: {url_path}")

        try:
            base_url = f"https://discountingcashflows.com{url_path}"
            cache_buster_url = f"{base_url}?org.htmx.cache-buster=transcriptsContent"
            
            headers = {
                "accept": "*/*",
                "accept-language": "en-US,en;q=0.9,zh-TW;q=0.8,zh;q=0.7,zh-CN;q=0.6,ja;q=0.5",
                "hx-boosted": "true",
                "hx-current-url": base_url,
                "hx-request": "true",
                "hx-target": "transcriptsContent",
                "priority": "u=1, i",
                "sec-ch-ua": '"Google Chrome";v="137", "Chromium";v="137", "Not/A)Brand";v="24"',
                "sec-ch-ua-mobile": "?0",
                "sec-ch-ua-platform": '"macOS"',
                "sec-fetch-dest": "empty",
                "sec-fetch-mode": "cors",
                "sec-fetch-site": "same-origin",
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36"
            }

            logger.info(f"Getting main page: {base_url}")
            main_response = self.session.get(base_url, headers={"User-Agent": headers["User-Agent"]})
            logger.info(f"Main page response status: {main_response.status_code}")
            main_response.raise_for_status()

            csrf_match = re.search(r'csrftoken["\']?\s*:\s*["\']([^"\']+)["\']', main_response.text)
            if not csrf_match:
                csrf_match = re.search(r'name=["\']csrfmiddlewaretoken["\'] value=["\']([^"\']+)["\']', main_response.text)
            if csrf_match:
                csrf_token = csrf_match.group(1)
                headers["x-csrftoken"] = csrf_token
                self.session.cookies['csrftoken'] = csrf_token
                logger.info(f"Extracted CSRF token: {csrf_token[:20]}...")

            logger.info(f"Making transcript request: {cache_buster_url}")
            response = self.session.get(cache_buster_url, headers=headers, timeout=config.REQUEST_TIMEOUT)
            logger.info(f"Transcript response status code: {response.status_code}")
            response.raise_for_status()
            
            transcript_data, fiscal_info = self._parse_html_transcript(response.text)
            
            if transcript_data:
                # è§£ææ—¥æœŸ
                parsed_date = self.parse_date_text(fiscal_info.get('date'))

                # ä¿å­˜æ–‡ä»¶ (å¯é€‰)
                txt_filename = None
                html_filename = None
                if SAVE_TRANSCRIPT_FILES:
                    txt_filename = self._save_transcript_as_txt(
                        transcript_data,
                        fiscal_info,
                        f"transcript_{ticker}_FY{fiscal_info.get('fiscal_year', year)}_Q{quarter_num}.txt"
                    )
                    html_filename = self._save_raw_html(response.text, ticker, fiscal_info, quarter_num)
                
                # è¿”å›ç»“æ„åŒ–æ•°æ®
                return {
                    'ticker': ticker,
                    'year': year,
                    'quarter': quarter_num,
                    'date': parsed_date,
                    'content': transcript_data.get('content', ''),
                    'txt_filename': txt_filename,
                    'html_filename': html_filename,
                    'parsed_successfully': True
                }
            else:
                logger.error(f"æœªèƒ½ä»HTMLè§£æè´¢æŠ¥æ•°æ®: {url_path}")
                return None
            
        except Exception as e:
            logger.error(f"è·å–è´¢æŠ¥ä¼šè®®è®°å½•æ—¶å‡ºé”™: {e} from {url_path}")
            raise DataRetrievalError(f"è·å–è´¢æŠ¥ä¼šè®®è®°å½•æ—¶å‡ºé”™: {e}")

    def _parse_html_transcript(self, html_content: str) -> Tuple[Optional[Dict], Dict]:
        """ä»HTMLå†…å®¹ä¸­è§£æç»“æ„åŒ–æ•°æ®"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            fiscal_info = {}
            
            # æŸ¥æ‰¾è´¢å¹´ä¿¡æ¯
            fiscal_span = soup.find('span', string=re.compile(r'Fiscal Year.*Quarter'))
            if fiscal_span:
                fiscal_text = fiscal_span.get_text()
                fy_match = re.search(r'Fiscal Year \(FY\) (\d+), Quarter (\d+)', fiscal_text)
                if fy_match:
                    fiscal_info['fiscal_year'] = fy_match.group(1)
                    fiscal_info['quarter'] = fy_match.group(2)
            
            # æŸ¥æ‰¾æ—¥æœŸä¿¡æ¯
            date_span = soup.find('span', class_='text-xs')
            if date_span:
                date_text = date_span.get_text().strip()
                fiscal_info['date'] = date_text
            
            # ä»textareaä¸­æå–å†…å®¹
            textarea = soup.find('textarea', id='AIInsightsContent')
            if not textarea:
                return None, fiscal_info
            
            json_content = html.unescape(textarea.get_text())
            transcript_data = json.loads(json_content)
            return transcript_data, fiscal_info
            
        except Exception as e:
            logger.error(f"è§£æHTMLæ—¶å‡ºé”™: {e}")
            return None, {}

    def _extract_speaker_content(self, transcript_content: str) -> List[Dict]:
        """ä»è®°å½•ä¸­æå–å‘è¨€äººå’Œå†…å®¹"""
        speakers_content = []
        lines = transcript_content.split('\n')
        current_speaker = None
        current_content = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            speaker_match = re.match(r'^([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s*[-:](.*)$', line)
            if speaker_match:
                if current_speaker and current_content:
                    speakers_content.append({
                        'speaker': current_speaker,
                        'content': ' '.join(current_content).strip()
                    })
                current_speaker = speaker_match.group(1).strip()
                current_content = [speaker_match.group(2).strip()] if speaker_match.group(2).strip() else []
            else:
                if current_speaker:
                    current_content.append(line)
        
        if current_speaker and current_content:
            speakers_content.append({
                'speaker': current_speaker,
                'content': ' '.join(current_content).strip()
            })
        return speakers_content

    def _save_raw_html(self, html_content: str, ticker: str, fiscal_info: Dict, quarter_num: str) -> Optional[str]:
        """ä¿å­˜åŸå§‹HTMLæ–‡ä»¶"""
        try:
            folder_name = f"{ticker}_transcripts"
            os.makedirs(folder_name, exist_ok=True)
            html_filename = f"{folder_name}/transcript_{ticker}_FY{fiscal_info.get('fiscal_year', 'UNKNOWN')}_Q{quarter_num}.html"
            with open(html_filename, 'w', encoding='utf-8') as f:
                f.write(html_content)
            logger.info(f"åŸå§‹HTMLå·²ä¿å­˜åˆ°: {html_filename}")
            return html_filename
        except Exception as e:
            logger.error(f"ä¿å­˜HTMLæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return None

    def _save_transcript_as_txt(self, transcript_data: Dict, fiscal_info: Dict, filename: str) -> Optional[str]:
        """å°†è§£æåçš„è®°å½•ä¿å­˜ä¸ºTXTæ–‡ä»¶"""
        try:
            ticker = transcript_data.get('symbol', 'UNKNOWN')
            folder_name = f"{ticker}_transcripts"
            os.makedirs(folder_name, exist_ok=True)
            txt_filename = f"{folder_name}/{filename}"
            
            with open(txt_filename, 'w', encoding='utf-8') as f:
                f.write("="*60 + "\nEARNINGS CALL TRANSCRIPT\n" + "="*60 + "\n\n")
                if fiscal_info:
                    f.write(f"Company: {transcript_data.get('symbol', 'N/A')}\n")
                    f.write(f"Fiscal Year: {fiscal_info.get('fiscal_year', 'N/A')}\n")
                    f.write(f"Quarter: {fiscal_info.get('quarter', 'N/A')}\n")
                    f.write(f"Date: {fiscal_info.get('date', 'N/A')}\n")
                    f.write(f"Transcript Date: {transcript_data.get('date', 'N/A')}\n")
                    f.write("\n" + "="*60 + "\n\n")
                
                content = transcript_data.get('content', '')
                speakers_content = self._extract_speaker_content(content)
                
                if speakers_content:
                    for item in speakers_content:
                        f.write(f"[{item['speaker']}]\n{item['content']}\n\n" + "-"*40 + "\n\n")
                else:
                    f.write("RAW TRANSCRIPT CONTENT:\n" + "-"*40 + "\n" + content)
            
            logger.info(f"Transcript saved to: {txt_filename}")
            return txt_filename
        except Exception as e:
            logger.error(f"ä¿å­˜transcript TXTæ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return None

# ä¸»ä¸šåŠ¡é€»è¾‘
class SECEarningsAnalyzer:
    """SEC & è´¢æŠ¥åˆ†æå™¨ä¸»ç±»"""
    
    def __init__(self):
        self.gemini_service = GeminiService()
        self.cache_manager = CacheManager()
        self.sec_service = SECService(self.cache_manager)
        self.hk_service = HKStockService(self.cache_manager)
        self.saudi_service = SaudiExchangeService(self.cache_manager)
        self.earnings_service = EarningsService(self.cache_manager)
        self.session_manager = SessionManager()
        self.document_manager = DocumentManager()

    def analyze_question(self, question: str, ticker: str, model_type: str) -> Tuple[str, str]:
        """åˆ†æç”¨æˆ·é—®é¢˜å¹¶ç”Ÿæˆæç¤ºè¯"""
        # è·å–å½“å‰è¯­è¨€è®¾ç½®
        language = st.session_state.get("selected_language", "English")
        
        if language == "English":
            analysis_prompt = f"""
            You are a professional financial analyst assistant, specialized in analyzing user questions and breaking them down into two key processing steps.

            User Question: {question}
            Stock Ticker: {ticker}

            Your Task:
            1. Analyze the user's question and understand its core intent
            2. Generate two high-quality prompts:
               - Document Processing Prompt: Used to extract information related to the user's question from a single document
               - Integration Prompt: Used to integrate analysis results from multiple document processing responses

            Requirements:
            - The document processing prompt must be specific and professional, capable of extracting all relevant information from a single document
            - The integration prompt must be able to integrate results from multiple documents, providing complete analysis and insights
            - Both prompts should be concise and clear, highlighting key points
            - The generated prompts should produce professional, readable analysis results
            - The user's original question MUST appear in both the document processing prompt and integration prompt
            - Always answer in English
            - **Must return only JSON format, do not include any other text or explanations.**

            Please return directly in JSON format:
            ```json
            {{
                "processing_prompt": "Document processing prompt",
                "integration_prompt": "Integration prompt"
            }}
            ```
            """
        elif language == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©":
            analysis_prompt = f"""
            Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù…Ø­Ù„Ù„ Ù…Ø§Ù„ÙŠ Ù…Ø­ØªØ±ÙØŒ Ù…ØªØ®ØµØµ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ÙˆØªÙ‚Ø³ÙŠÙ…Ù‡Ø§ Ø¥Ù„Ù‰ Ø®Ø·ÙˆØªÙŠÙ† Ø±Ø¦ÙŠØ³ÙŠØªÙŠÙ† Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©.

            Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {question}
            Ø±Ù…Ø² Ø§Ù„Ø³Ù‡Ù…: {ticker}

            Ù…Ù‡Ù…ØªÙƒ:
            1. ØªØ­Ù„ÙŠÙ„ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙˆÙÙ‡Ù… Ù‚ØµØ¯Ù‡ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
            2. ØªÙˆÙ„ÙŠØ¯ Ù…Ø·Ø§Ù„Ø¨ØªÙŠÙ† Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø¬ÙˆØ¯Ø©:
               - Ù…Ø·Ø§Ù„Ø¨Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯: ØªØ³ØªØ®Ø¯Ù… Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù…Ù† Ù…Ø³ØªÙ†Ø¯ ÙˆØ§Ø­Ø¯
               - Ù…Ø·Ø§Ù„Ø¨Ø© Ø§Ù„ØªÙƒØ§Ù…Ù„: ØªØ³ØªØ®Ø¯Ù… Ù„Ø¯Ù…Ø¬ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ù…Ù† Ø±Ø¯ÙˆØ¯ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©

            Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª:
            - ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ù…Ø·Ø§Ù„Ø¨Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯ Ù…Ø­Ø¯Ø¯Ø© ÙˆÙ…Ù‡Ù†ÙŠØ©ØŒ Ù‚Ø§Ø¯Ø±Ø© Ø¹Ù„Ù‰ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ù…Ù† Ù…Ø³ØªÙ†Ø¯ ÙˆØ§Ø­Ø¯
            - ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ù…Ø·Ø§Ù„Ø¨Ø© Ø§Ù„ØªÙƒØ§Ù…Ù„ Ù‚Ø§Ø¯Ø±Ø© Ø¹Ù„Ù‰ Ø¯Ù…Ø¬ Ù†ØªØ§Ø¦Ø¬ Ù…Ù† Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø©ØŒ ÙˆØªÙ‚Ø¯ÙŠÙ… ØªØ­Ù„ÙŠÙ„ ÙˆØ±Ø¤Ù‰ ÙƒØ§Ù…Ù„Ø©
            - ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† ÙƒÙ„Ø§ Ø§Ù„Ù…Ø·Ø§Ù„Ø¨ØªÙŠÙ† Ù…ÙˆØ¬Ø²Ø© ÙˆÙˆØ§Ø¶Ø­Ø©ØŒ Ù…Ø¹ ØªØ³Ù„ÙŠØ· Ø§Ù„Ø¶ÙˆØ¡ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
            - ÙŠØ¬Ø¨ Ø£Ù† ØªÙ†ØªØ¬ Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø§Øª Ø§Ù„Ù…ÙˆÙ„Ø¯Ø© Ù†ØªØ§Ø¦Ø¬ ØªØ­Ù„ÙŠÙ„ Ù…Ù‡Ù†ÙŠØ© ÙˆÙ‚Ø§Ø¨Ù„Ø© Ù„Ù„Ù‚Ø±Ø§Ø¡Ø©
            - ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ¸Ù‡Ø± Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙÙŠ ÙƒÙ„ Ù…Ù† Ù…Ø·Ø§Ù„Ø¨Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯ ÙˆÙ…Ø·Ø§Ù„Ø¨Ø© Ø§Ù„ØªÙƒØ§Ù…Ù„
            - Ø§Ø¬Ø¨ Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
            - **ÙŠØ¬Ø¨ Ø¥Ø±Ø¬Ø§Ø¹ ØªÙ†Ø³ÙŠÙ‚ JSON ÙÙ‚Ø·ØŒ Ù„Ø§ ØªØ´Ù…Ù„ Ø£ÙŠ Ù†Øµ Ø£Ùˆ ØªÙØ³ÙŠØ±Ø§Øª Ø£Ø®Ø±Ù‰.**

            ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø¥Ø±Ø¬Ø§Ø¹ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¨ØªÙ†Ø³ÙŠÙ‚ JSON:
            ```json
            {{
                "processing_prompt": "Ù…Ø·Ø§Ù„Ø¨Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯",
                "integration_prompt": "Ù…Ø·Ø§Ù„Ø¨Ø© Ø§Ù„ØªÙƒØ§Ù…Ù„"
            }}
            ```
            """
        else:  # ä¸­æ–‡
            analysis_prompt = f"""
            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆåŠ©æ‰‹ï¼Œä¸“é—¨è´Ÿè´£åˆ†æç”¨æˆ·çš„é—®é¢˜å¹¶å°†å…¶åˆ†è§£ä¸ºä¸¤ä¸ªå…³é”®çš„å¤„ç†æ­¥éª¤ã€‚

            ç”¨æˆ·é—®é¢˜: {question}
            è‚¡ç¥¨ä»£ç : {ticker}

            ä½ çš„ä»»åŠ¡ï¼š
            1. åˆ†æç”¨æˆ·çš„é—®é¢˜ï¼Œç†è§£å…¶æ ¸å¿ƒæ„å›¾
            2. ç”Ÿæˆä¸¤ä¸ªé«˜è´¨é‡çš„æç¤ºè¯ï¼š
               - å¤„ç†èµ„æ–™promptï¼šç”¨äºä»å•ä¸ªæ–‡æ¡£ä¸­æå– èˆ‡ç”¨æˆ·é—®é¢˜ ç›¸å…³çš„ä¿¡æ¯
               - ç»Ÿæ•´promptï¼šç”¨äºæ•´åˆå¤šä¸ª å¤„ç†èµ„æ–™å›ç­” çš„åˆ†æç»“æœ

            è¦æ±‚ï¼š
            - å¤„ç†èµ„æ–™promptå¿…é¡»å…·ä½“ã€ä¸“ä¸šï¼Œèƒ½å¤Ÿä»å•ä¸ªæ–‡æ¡£ä¸­æå–æ‰€æœ‰ç›¸å…³ä¿¡æ¯
            - ç»Ÿæ•´promptå¿…é¡»èƒ½å¤Ÿæ•´åˆå¤šä¸ªæ–‡æ¡£çš„ç»“æœï¼Œæä¾›å®Œæ•´çš„åˆ†æå’Œæ´å¯Ÿ
            - ä¸¤ä¸ªpromptéƒ½è¦ç®€æ´æ˜äº†ï¼Œé‡ç‚¹çªå‡º
            - ç”Ÿæˆçš„promptåº”è¯¥èƒ½å¤Ÿäº§ç”Ÿä¸“ä¸šã€æ˜“è¯»çš„åˆ†æç»“æœ
            - ç”¨æˆ¶åŸå§‹å•é¡Œ å¿…é ˆåœ¨è™•ç†è³‡æ–™prompt å’Œ çµ±æ•´prompt ä¸­éƒ½å‡ºç¾
            - **å¿…é¡»åªè¿”å›JSONæ ¼å¼ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡æœ¬æˆ–è§£é‡Šã€‚**

            è¯·ç›´æ¥è¿”å›JSONæ ¼å¼ï¼š
            ```json
            {{
                "processing_prompt": "å¤„ç†èµ„æ–™prompt",
                "integration_prompt": "ç»Ÿæ•´prompt"
            }}
            ```
            """
        
        try:
            result = self.gemini_service.call_api(analysis_prompt, model_type)
            # å°è¯•ä»Markdownä»£ç å—ä¸­æå–JSON
            match = re.search(r"```json\s*(\{.*?\})\s*```", result, re.DOTALL)
            if match:
                json_str = match.group(1)
            else:
                json_str = result # å¦‚æœæ²¡æœ‰markdownï¼Œåˆ™å‡å®šæ•´ä¸ªå­—ç¬¦ä¸²æ˜¯JSON
            
            prompt_data = json.loads(json_str)
            
            processing_prompt = prompt_data.get("processing_prompt", "")
            integration_prompt = prompt_data.get("integration_prompt", "")
            
            if not processing_prompt or not integration_prompt:
                raise ValueError("ç”Ÿæˆçš„æç¤ºè¯ä¸ºç©º")
            
            return processing_prompt, integration_prompt
            
        except (json.JSONDecodeError, ValueError) as e:
            logger.error(f"è§£æprompt JSONå¤±è´¥: {e}. æ¨¡å‹è¿”å›: {result}")
            # ä½¿ç”¨é»˜è®¤æç¤ºè¯
            if language == "English":
                processing_prompt = f"Please extract information related to '{question}' from the following document and provide detailed analysis. Keep answers concise and to the point. Start with conclusions, use appropriate emojis and markdown format. If not found, briefly state 'Not mentioned in document'. Always answer in English."
                integration_prompt = f"Please integrate all the following analysis results to answer the user's question: '{question}', and provide a complete analysis report. Keep answers concise and to the point. Start with conclusions, use appropriate emojis and markdown format. If not found, briefly state 'Not mentioned in documents'. Always answer in English."
            elif language == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©":
                processing_prompt = f"ÙŠØ±Ø¬Ù‰ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ù€ '{question}' Ù…Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯ Ø§Ù„ØªØ§Ù„ÙŠ ÙˆØªÙ‚Ø¯ÙŠÙ… ØªØ­Ù„ÙŠÙ„ Ù…ÙØµÙ„. Ø§Ø¬Ø¹Ù„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ù…ÙˆØ¬Ø²Ø© ÙˆÙ…Ø¨Ø§Ø´Ø±Ø©. Ø§Ø¨Ø¯Ø£ Ø¨Ø§Ù„Ù†ØªØ§Ø¦Ø¬ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ØªØ¹Ø¨ÙŠØ±ÙŠØ© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© ÙˆØªÙ†Ø³ÙŠÙ‚ markdown. Ø¥Ø°Ø§ Ù„Ù… ØªÙˆØ¬Ø¯ØŒ Ø§Ø°ÙƒØ± Ø¨Ø¥ÙŠØ¬Ø§Ø² 'ØºÙŠØ± Ù…Ø°ÙƒÙˆØ± ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯'. Ø§Ø¬Ø¨ Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©."
                integration_prompt = f"ÙŠØ±Ø¬Ù‰ Ø¯Ù…Ø¬ Ø¬Ù…ÙŠØ¹ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ§Ù„ÙŠØ© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: '{question}'ØŒ ÙˆØªÙ‚Ø¯ÙŠÙ… ØªÙ‚Ø±ÙŠØ± ØªØ­Ù„ÙŠÙ„ ÙƒØ§Ù…Ù„. Ø§Ø¬Ø¹Ù„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ù…ÙˆØ¬Ø²Ø© ÙˆÙ…Ø¨Ø§Ø´Ø±Ø©. Ø§Ø¨Ø¯Ø£ Ø¨Ø§Ù„Ù†ØªØ§Ø¦Ø¬ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ØªØ¹Ø¨ÙŠØ±ÙŠØ© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© ÙˆØªÙ†Ø³ÙŠÙ‚ markdown. Ø¥Ø°Ø§ Ù„Ù… ØªÙˆØ¬Ø¯ØŒ Ø§Ø°ÙƒØ± Ø¨Ø¥ÙŠØ¬Ø§Ø² 'ØºÙŠØ± Ù…Ø°ÙƒÙˆØ± ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª'. Ø§Ø¬Ø¨ Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©."
            else:  # ä¸­æ–‡
                processing_prompt = f"è¯·ä»ä»¥ä¸‹æ–‡æ¡£ä¸­æå–ä¸'{question}'ç›¸å…³çš„ä¿¡æ¯ï¼Œå¹¶è¿›è¡Œè¯¦ç»†åˆ†æï¼Œåªå›ç­”é‡é»å°±å¥½ï¼Œè¨˜å¾—ä¸å»¢è©±ã€‚å›ç­”è¦çµè«–å…ˆèªªï¼Œå¯ä»¥é©ç•¶ä½¿ç”¨emojiï¼Œmarkdownæ ¼å¼ï¼Œå¦‚æœæ²’æ‰¾åˆ°å°±ç°¡çŸ­å›ç­”ï¼ŒèªªæœªæåŠå°±å¥½ã€‚"
                integration_prompt = f"è¯·æ•´åˆä»¥ä¸‹æ‰€æœ‰åˆ†æç»“æœï¼Œå›ç­”ç”¨æˆ·é—®é¢˜ï¼š'{question}'ï¼Œå¹¶æä¾›å®Œæ•´çš„åˆ†ææŠ¥å‘Šï¼Œåªå›ç­”é‡é»å°±å¥½ï¼Œè¨˜å¾—ä¸å»¢è©±ã€‚å›ç­”è¦çµè«–å…ˆèªªï¼Œå¯ä»¥é©ç•¶ä½¿ç”¨emojiï¼Œmarkdownæ ¼å¼ï¼Œå¦‚æœæ²’æ‰¾åˆ°å°±ç°¡çŸ­å›ç­”ï¼ŒèªªæœªæåŠå°±å¥½ã€‚"
            
            return processing_prompt, integration_prompt
    
    def generate_fast_mode_prompt(self, question: str, ticker: str, model_type: str) -> str:
        """ç”Ÿæˆå¿«é€Ÿæ¨¡å¼çš„åˆ†ææç¤ºè¯"""
        # è·å–å½“å‰è¯­è¨€è®¾ç½®
        language = st.session_state.get("selected_language", "English")
        
        if language == "English":
            analysis_prompt = f"""
            You are a professional financial analyst assistant, specialized in analyzing user questions and generating optimized prompts for fast document analysis.

            User Question: {question}
            Stock Ticker: {ticker}

            Your Task:
            Generate a single, comprehensive prompt that can analyze all provided documents at once to answer the user's question.

            Requirements:
            - The prompt should be optimized for processing multiple documents simultaneously
            - It should extract and synthesize information from all documents to provide a complete answer
            - The prompt should be professional and capable of producing structured, readable analysis
            - The user's original question MUST appear in the generated prompt
            - Always answer in English
            - **Must return only JSON format, do not include any other text or explanations.**

            Please return directly in JSON format:
            ```json
            {{
                "fast_mode_prompt": "Comprehensive analysis prompt for all documents"
            }}
            ```
            """
        elif language == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©":
            analysis_prompt = f"""
            Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù…Ø­Ù„Ù„ Ù…Ø§Ù„ÙŠ Ù…Ø­ØªØ±ÙØŒ Ù…ØªØ®ØµØµ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ÙˆØªÙˆÙ„ÙŠØ¯ Ù…Ø·Ø§Ù„Ø¨Ø§Øª Ù…Ø­Ø³Ù†Ø© Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø±ÙŠØ¹ Ù„Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª.

            Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {question}
            Ø±Ù…Ø² Ø§Ù„Ø³Ù‡Ù…: {ticker}

            Ù…Ù‡Ù…ØªÙƒ:
            ØªÙˆÙ„ÙŠØ¯ Ù…Ø·Ø§Ù„Ø¨Ø© Ø´Ø§Ù…Ù„Ø© ÙˆØ§Ø­Ø¯Ø© ÙŠÙ…ÙƒÙ†Ù‡Ø§ ØªØ­Ù„ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ø¯ÙØ¹Ø© ÙˆØ§Ø­Ø¯Ø© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù….

            Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª:
            - ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø© Ù…Ø­Ø³Ù†Ø© Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© ÙÙŠ Ù†ÙØ³ Ø§Ù„ÙˆÙ‚Øª
            - ÙŠØ¬Ø¨ Ø£Ù† ØªØ³ØªØ®Ø±Ø¬ ÙˆØªØ¯Ù…Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù„ØªÙ‚Ø¯ÙŠÙ… Ø¥Ø¬Ø§Ø¨Ø© ÙƒØ§Ù…Ù„Ø©
            - ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø© Ù…Ù‡Ù†ÙŠØ© ÙˆÙ‚Ø§Ø¯Ø±Ø© Ø¹Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬ ØªØ­Ù„ÙŠÙ„ Ù…Ù†Ø¸Ù… ÙˆÙ‚Ø§Ø¨Ù„ Ù„Ù„Ù‚Ø±Ø§Ø¡Ø©
            - ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ¸Ù‡Ø± Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙÙŠ Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø© Ø§Ù„Ù…ÙˆÙ„Ø¯Ø©
            - Ø§Ø¬Ø¨ Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
            - **ÙŠØ¬Ø¨ Ø¥Ø±Ø¬Ø§Ø¹ ØªÙ†Ø³ÙŠÙ‚ JSON ÙÙ‚Ø·ØŒ Ù„Ø§ ØªØ´Ù…Ù„ Ø£ÙŠ Ù†Øµ Ø£Ùˆ ØªÙØ³ÙŠØ±Ø§Øª Ø£Ø®Ø±Ù‰.**

            ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø¥Ø±Ø¬Ø§Ø¹ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¨ØªÙ†Ø³ÙŠÙ‚ JSON:
            ```json
            {{
                "fast_mode_prompt": "Ù…Ø·Ø§Ù„Ø¨Ø© ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„Ø© Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª"
            }}
            ```
            """
        else:  # ä¸­æ–‡
            analysis_prompt = f"""
            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆåŠ©æ‰‹ï¼Œä¸“é—¨è´Ÿè´£åˆ†æç”¨æˆ·é—®é¢˜å¹¶ç”Ÿæˆä¼˜åŒ–çš„å¿«é€Ÿæ–‡æ¡£åˆ†ææç¤ºè¯ã€‚

            ç”¨æˆ·é—®é¢˜: {question}
            è‚¡ç¥¨ä»£ç : {ticker}

            ä½ çš„ä»»åŠ¡ï¼š
            ç”Ÿæˆä¸€ä¸ªç»¼åˆæ€§çš„æç¤ºè¯ï¼Œèƒ½å¤Ÿä¸€æ¬¡æ€§åˆ†ææ‰€æœ‰æä¾›çš„æ–‡æ¡£æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

            è¦æ±‚ï¼š
            - æç¤ºè¯åº”è¯¥é’ˆå¯¹åŒæ—¶å¤„ç†å¤šä¸ªæ–‡æ¡£è¿›è¡Œä¼˜åŒ–
            - åº”è¯¥èƒ½å¤Ÿä»æ‰€æœ‰æ–‡æ¡£ä¸­æå–å’Œç»¼åˆä¿¡æ¯ï¼Œæä¾›å®Œæ•´çš„ç­”æ¡ˆ
            - æç¤ºè¯åº”è¯¥ä¸“ä¸šï¼Œèƒ½å¤Ÿäº§ç”Ÿç»“æ„åŒ–ã€æ˜“è¯»çš„åˆ†æç»“æœ
            - ç”¨æˆ·åŸå§‹é—®é¢˜å¿…é¡»å‡ºç°åœ¨ç”Ÿæˆçš„æç¤ºè¯ä¸­
            - **å¿…é¡»åªè¿”å›JSONæ ¼å¼ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡æœ¬æˆ–è§£é‡Šã€‚**

            è¯·ç›´æ¥è¿”å›JSONæ ¼å¼ï¼š
            ```json
            {{
                "fast_mode_prompt": "ç»¼åˆåˆ†ææ‰€æœ‰æ–‡æ¡£çš„æç¤ºè¯"
            }}
            ```
            """
        
        try:
            result = self.gemini_service.call_api(analysis_prompt, model_type)
            # å°è¯•ä»Markdownä»£ç å—ä¸­æå–JSON
            match = re.search(r"```json\s*(\{.*?\})\s*```", result, re.DOTALL)
            if match:
                json_str = match.group(1)
            else:
                json_str = result
            
            prompt_data = json.loads(json_str)
            
            fast_mode_prompt = prompt_data.get("fast_mode_prompt", "")
            
            if not fast_mode_prompt:
                raise ValueError("ç”Ÿæˆçš„å¿«é€Ÿæ¨¡å¼æç¤ºè¯ä¸ºç©º")
            
            return fast_mode_prompt
            
        except (json.JSONDecodeError, ValueError) as e:
            logger.error(f"è§£æå¿«é€Ÿæ¨¡å¼prompt JSONå¤±è´¥: {e}. æ¨¡å‹è¿”å›: {result}")
            # ä½¿ç”¨é»˜è®¤æç¤ºè¯
            if language == "English":
                return f"Please analyze all the provided documents to answer the user's question: '{question}'. Extract relevant information from all documents and provide a comprehensive analysis. Keep answers concise and to the point. Start with conclusions, use appropriate emojis and markdown format. If not found, briefly state 'Not mentioned in documents'. Always answer in English."
            elif language == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©":
                return f"ÙŠØ±Ø¬Ù‰ ØªØ­Ù„ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: '{question}'. Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙˆÙ‚Ø¯Ù… ØªØ­Ù„ÙŠÙ„Ø§Ù‹ Ø´Ø§Ù…Ù„Ø§Ù‹. Ø§Ø¬Ø¹Ù„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ù…ÙˆØ¬Ø²Ø© ÙˆÙ…Ø¨Ø§Ø´Ø±Ø©. Ø§Ø¨Ø¯Ø£ Ø¨Ø§Ù„Ù†ØªØ§Ø¦Ø¬ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ØªØ¹Ø¨ÙŠØ±ÙŠØ© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© ÙˆØªÙ†Ø³ÙŠÙ‚ markdown. Ø¥Ø°Ø§ Ù„Ù… ØªÙˆØ¬Ø¯ØŒ Ø§Ø°ÙƒØ± Ø¨Ø¥ÙŠØ¬Ø§Ø² 'ØºÙŠØ± Ù…Ø°ÙƒÙˆØ± ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª'. Ø§Ø¬Ø¨ Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©."
            else:  # ä¸­æ–‡
                return f"è¯·åˆ†ææ‰€æœ‰æä¾›çš„æ–‡æ¡£æ¥å›ç­”ç”¨æˆ·é—®é¢˜ï¼š'{question}'ã€‚ä»æ‰€æœ‰æ–‡æ¡£ä¸­æå–ç›¸å…³ä¿¡æ¯å¹¶æä¾›ç»¼åˆåˆ†æã€‚åªå›ç­”é‡ç‚¹å°±å¥½ï¼Œè®°å¾—ä¸åºŸè¯ã€‚å›ç­”è¦ç»“è®ºå…ˆè¯´ï¼Œå¯ä»¥é€‚å½“ä½¿ç”¨emojiï¼Œmarkdownæ ¼å¼ï¼Œå¦‚æœæ²¡æ‰¾åˆ°å°±ç®€çŸ­å›ç­”ï¼Œè¯´æœªæåŠå°±å¥½ã€‚"
    
    def process_document(self, document: Document, processing_prompt: str, model_type: str) -> str:
        """å¤„ç†å•ä¸ªæ–‡æ¡£"""
        try:
            # è·å–å½“å‰è¯­è¨€è®¾ç½®
            language = st.session_state.get("selected_language", "English")
            
            # å¦‚æœæ–‡æ¡£å†…å®¹ä¸ºç©ºï¼Œåˆ™ä¸‹è½½
            if not document.content:
                if document.type == 'SEC Filing':
                    # æ£€æŸ¥æ˜¯å¦ä¸º6-Kæ–‡ä»¶
                    if hasattr(document, 'form_type') and document.form_type == '6-K':
                        # 6-Kæ–‡ä»¶åº”è¯¥å·²ç»åœ¨SixKProcessorä¸­å¤„ç†è¿‡äº†ã€‚å¦‚æœè¿™é‡Œå†…å®¹ä»ç„¶ä¸ºç©ºï¼Œè¯´æ˜æ²¡æœ‰æ‰¾åˆ°ex99é™„ä»¶
                        logger.info(f"â„¹ï¸ [6K-PROCESS] 6-Kæ–‡ä»¶å†…å®¹ä¸ºç©º: {document.title} (å¯èƒ½æ²¡æœ‰ex99é™„ä»¶)")
                        if language == "ä¸­æ–‡":
                            document.content = "6-Kæ–‡ä»¶æœªåŒ…å«ex99é™„ä»¶"
                        elif language == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©":
                            document.content = "Ù…Ù„Ù 6-K Ù„Ø§ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ø±ÙÙ‚Ø§Øª ex99"
                        else:
                            document.content = "6-K file contains no ex99 attachments"
                    else:
                        # æ™®é€šSECæ–‡ä»¶å¤„ç†
                        document.content = self.sec_service.download_filing(document.url)
                elif document.type == 'HK Stock Filing':
                    # æ¸¯è‚¡æ–‡ä»¶å¤„ç†
                    document.content = self.hk_service.download_hk_filing(document.url)
                elif document.type == 'Saudi Exchange Filing':
                    # æ²™ç‰¹äº¤æ˜“æ‰€æ–‡ä»¶å¤„ç†
                    document.content = self.saudi_service.download_saudi_filing(document.url)
                elif document.type == 'Earnings Call':
                    # åœ¨æ–°çš„æµç¨‹ä¸­ï¼Œå†…å®¹å·²é¢„å…ˆè·å–
                    logger.warning(f"å¤„ç†æ–‡æ¡£æ—¶å‘ç°è´¢æŠ¥è®°å½•å†…å®¹ä¸ºç©º: {document.title}")
                    document.content = "å†…å®¹æœªæ‰¾åˆ°" if language == "ä¸­æ–‡" else "Content not found"
            
            # å‡†å¤‡prompt - æ ¹æ®è¯­è¨€é€‰æ‹©
            if language == "English":
                prompt = f"""
                You are a professional document analyst, specialized in extracting and analyzing information from financial documents.

                Document Title: {document.title}
                Document Date: {document.date}
                Document Type: {document.type}
                
                Processing Requirements: {processing_prompt}
                Also answer similar requirements, don't miss anything
                
                Requirements:
                - Carefully read the provided document content
                - Extract relevant information according to the user's specific requirements
                - Provide accurate, professional analysis
                - Ensure answers come from document content, don't imagine
                - I don't have time to read, ensure answers are direct and to the point, no need for polite conversation
                - Always answer in English
                - when markdown output, Escape all dollar signs $ for currency as ï¼„ to prevent Markdown from rendering them as math.
                
                Answer Requirements:
                - Start with ğŸ“ emoji, followed by what type of document this is and its purpose, 
                - second line Start with ğŸ’¡ on next new line row, directly state conclusions, answer conclusions related to my processing requirements, all in short sentences
                - Please provide structured analysis results, only answer key points, remember no nonsense.
                - First sentence should state key points without pleasantries. Don't say "According to the document content you provided..." such nonsense, directly state key points
                - Answer should start with conclusions, can use emojis to help users read, markdown format
                - If the document doesn't contain information related to my question, just say "Not mentioned in document" period, one sentence only, no nonsense, I don't have time to read

                Document Content:
                {document.content}
                """
            else:  # ä¸­æ–‡
                prompt = f"""
                ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ†æå¸ˆï¼Œä¸“é—¨è´Ÿè´£ä»è´¢åŠ¡æ–‡æ¡£ä¸­æå–å’Œåˆ†æä¿¡æ¯ã€‚

                æ–‡æ¡£æ ‡é¢˜: {document.title}
                æ–‡æ¡£æ—¥æœŸ: {document.date}
                æ–‡æ¡£ç±»å‹: {document.type}
                
                å¤„ç†è¦æ±‚: {processing_prompt}
                æœ‰èˆ‡ä»¥ä¸Šè¦æ±‚é¡ä¼¼çš„ä¹Ÿä¸€èµ·å›ç­”ï¼Œä¸è¦æ¼æ‰
                
                è¦æ±‚ï¼š
                - ä»”ç»†é˜…è¯»æä¾›çš„æ–‡æ¡£å†…å®¹
                - æ ¹æ®ç”¨æˆ·çš„å…·ä½“è¦æ±‚æå–ç›¸å…³ä¿¡æ¯
                - æä¾›å‡†ç¡®ã€ä¸“ä¸šçš„åˆ†æ
                - ç¢ºä¿å›ç­”éƒ½ä¾†è‡ªæ–‡æª”å…§å®¹ï¼Œä¸è¦æ†‘ç©ºæƒ³åƒ
                - æˆ‘æ²’æ™‚é–“çœ‹ ç¢ºä¿å›ç­”ç›´æ¥èªªé‡é» ä¸ç”¨åƒäººä¸€æ¨£é‚„è¦å®¢å¥—è©±
                - markdownè¼¸å‡ºï¼Œå°‡æ‰€æœ‰è¡¨ç¤ºé‡‘é¡çš„ $ æ”¹ç‚º ï¼„ï¼Œä»¥é¿å… Markdown è¢«èª¤åˆ¤ç‚ºæ•¸å­¸å…¬å¼ã€‚
                
                å›ç­”è¦æ±‚ï¼š
                - é–‹é ­ä»¥ğŸ“è¿™ä¸ªemojiå¼€é ­ï¼Œ ğŸ“å¾Œé¢æ¥é€™æ˜¯ä¸€ä»½ä»€éº¼æ–‡ä»¶ï¼Œæ–‡ä»¶ç›®çš„æ˜¯ä»€éº¼ï¼Œ
                - ç¬¬äºŒå¥ä¸‹ä¸€è¡Œï¼Œé–‹é ­ä»¥ ğŸ’¡ï¼Œè¨˜å¾—æ›è¡Œï¼Œç›´æ¥èªªçµè«–ï¼Œå›ç­”è·Ÿæˆ‘å¤„ç†è¦æ±‚æœ‰é—œçš„çµè«– éƒ½æ˜¯ç°¡çŸ­ä¸€å¥è©±
                - è¯·æä¾›ç»“æ„åŒ–çš„åˆ†æç»“æœï¼Œåªå›ç­”é‡é»å°±å¥½ï¼Œè¨˜å¾—ä¸å»¢è©±ã€‚
                - ç¬¬ä¸€å¥å°±èªªé‡é»ä¸ç”¨å®¢å¥—ã€‚ ä¸ç”¨èªª æ ¹æ®æ‚¨æä¾›çš„æ–‡æ¡£å†…å®¹... é€™ç¨®å»¢è©±ï¼Œç›´æ¥èªªé‡é»
                - å›ç­”è¦çµè«–å…ˆèªªï¼Œå¯ä»¥ä½¿ç”¨emojiå¹«åŠ©ä½¿ç”¨è€…é–±è®€ï¼Œmarkdownæ ¼å¼
                - å¦‚æœæ–‡æª”å…§æ²’æœ‰è·Ÿæˆ‘çš„å•é¡Œæœ‰é—œçš„è³‡è¨Šï¼Œå°±èªªä¸€å¥ æ–‡æª”å…§æœªæåŠ å¥è™Ÿ ä¸€å¥è©±å°±å¥½  ä¸å‡†å»¢è©± æˆ‘æ²’æ™‚é–“çœ‹

                æ–‡æ¡£å†…å®¹:
                {document.content}
                """
            
            logger.info("================================================")
            logger.info(f"Processing document: {document.title} in {language}")
            
            return self.gemini_service.call_api(prompt, model_type)
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡æ¡£å¤±è´¥: {e}")
            error_msg = f"å¤„ç†æ–‡æ¡£æ—¶å‡ºé”™: {e}" if language == "ä¸­æ–‡" else f"Error processing document: {e}"
            return error_msg
    
    def process_document_stream(self, document: Document, processing_prompt: str, model_type: str):
        """å¤„ç†å•ä¸ªæ–‡æ¡£ - æµå¼å“åº”ç‰ˆæœ¬"""
        try:
            # è·å–å½“å‰è¯­è¨€è®¾ç½®
            language = st.session_state.get("selected_language", "English")
            
            # å¦‚æœæ–‡æ¡£å†…å®¹ä¸ºç©ºï¼Œåˆ™ä¸‹è½½
            if not document.content:
                if document.type == 'SEC Filing':
                    # æ£€æŸ¥æ˜¯å¦ä¸º6-Kæ–‡ä»¶
                    if hasattr(document, 'form_type') and document.form_type == '6-K':
                        # 6-Kæ–‡ä»¶åº”è¯¥å·²ç»åœ¨SixKProcessorä¸­å¤„ç†è¿‡äº†ã€‚å¦‚æœè¿™é‡Œå†…å®¹ä»ç„¶ä¸ºç©ºï¼Œè¯´æ˜æ²¡æœ‰æ‰¾åˆ°ex99é™„ä»¶
                        logger.info(f"â„¹ï¸ [6K-PROCESS] 6-Kæ–‡ä»¶å†…å®¹ä¸ºç©º: {document.title} (å¯èƒ½æ²¡æœ‰ex99é™„ä»¶)")
                        if language == "ä¸­æ–‡":
                            document.content = "6-Kæ–‡ä»¶æœªåŒ…å«ex99é™„ä»¶"
                        elif language == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©":
                            document.content = "Ù…Ù„Ù 6-K Ù„Ø§ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ø±ÙÙ‚Ø§Øª ex99"
                        else:
                            document.content = "6-K file contains no ex99 attachments"
                    else:
                        # æ™®é€šSECæ–‡ä»¶å¤„ç†
                        document.content = self.sec_service.download_filing(document.url)
                elif document.type == 'HK Stock Filing':
                    # æ¸¯è‚¡æ–‡ä»¶å¤„ç†
                    document.content = self.hk_service.download_hk_filing(document.url)
                elif document.type == 'Saudi Exchange Filing':
                    # æ²™ç‰¹äº¤æ˜“æ‰€æ–‡ä»¶å¤„ç†
                    document.content = self.saudi_service.download_saudi_filing(document.url)
                elif document.type == 'Earnings Call':
                    # åœ¨æ–°çš„æµç¨‹ä¸­ï¼Œå†…å®¹å·²é¢„å…ˆè·å–
                    logger.warning(f"å¤„ç†æ–‡æ¡£æ—¶å‘ç°è´¢æŠ¥è®°å½•å†…å®¹ä¸ºç©º: {document.title}")
                    document.content = "å†…å®¹æœªæ‰¾åˆ°" if language == "ä¸­æ–‡" else "Content not found"
            
            # å‡†å¤‡prompt - æ ¹æ®è¯­è¨€é€‰æ‹©
            if language == "English":
                prompt = f"""
                You are a professional document analyst, specialized in extracting and analyzing information from financial documents.

                Document Title: {document.title}
                Document Date: {document.date}
                Document Type: {document.type}
                
                Processing Requirements: {processing_prompt}
                Also answer similar requirements, don't miss anything
                
                Requirements:
                - Carefully read the provided document content
                - Extract relevant information according to the user's specific requirements
                - Provide accurate, professional analysis
                - Ensure answers come from document content, don't imagine
                - I don't have time to read, ensure answers are direct and to the point, no need for polite conversation
                - Always answer in English
                - when markdown output, Escape all dollar signs $ for currency as ï¼„ to prevent Markdown from rendering them as math.
                
                Answer Requirements:
                - Start with ğŸ“ emoji, followed by what type of document this is and its purpose, 
                - second line Start with ğŸ’¡ on next new line row, directly state conclusions, answer conclusions related to my processing requirements, all in short sentences
                - Please provide structured analysis results, only answer key points, remember no nonsense.
                - First sentence should state key points without pleasantries. Don't say "According to the document content you provided..." such nonsense, directly state key points
                - Answer should start with conclusions, can use emojis to help users read, markdown format
                - If the document doesn't contain information related to my question, just say "Not mentioned in document" period, one sentence only, no nonsense, I don't have time to read

                Document Content:
                {document.content}
                """
            else:  # ä¸­æ–‡
                prompt = f"""
                ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ†æå¸ˆï¼Œä¸“é—¨è´Ÿè´£ä»è´¢åŠ¡æ–‡æ¡£ä¸­æå–å’Œåˆ†æä¿¡æ¯ã€‚

                æ–‡æ¡£æ ‡é¢˜: {document.title}
                æ–‡æ¡£æ—¥æœŸ: {document.date}
                æ–‡æ¡£ç±»å‹: {document.type}
                
                å¤„ç†è¦æ±‚: {processing_prompt}
                æœ‰èˆ‡ä»¥ä¸Šè¦æ±‚é¡ä¼¼çš„ä¹Ÿä¸€èµ·å›ç­”ï¼Œä¸è¦æ¼æ‰
                
                è¦æ±‚ï¼š
                - ä»”ç»†é˜…è¯»æä¾›çš„æ–‡æ¡£å†…å®¹
                - æ ¹æ®ç”¨æˆ·çš„å…·ä½“è¦æ±‚æå–ç›¸å…³ä¿¡æ¯
                - æä¾›å‡†ç¡®ã€ä¸“ä¸šçš„åˆ†æ
                - ç¢ºä¿å›ç­”éƒ½ä¾†è‡ªæ–‡æª”å…§å®¹ï¼Œä¸è¦æ†‘ç©ºæƒ³åƒ
                - æˆ‘æ²’æ™‚é–“çœ‹ ç¢ºä¿å›ç­”ç›´æ¥èªªé‡é» ä¸ç”¨åƒäººä¸€æ¨£é‚„è¦å®¢å¥—è©±
                - markdownè¼¸å‡ºï¼Œå°‡æ‰€æœ‰è¡¨ç¤ºé‡‘é¡çš„ $ æ”¹ç‚º ï¼„ï¼Œä»¥é¿å… Markdown è¢«èª¤åˆ¤ç‚ºæ•¸å­¸å…¬å¼ã€‚


                
                å›ç­”è¦æ±‚ï¼š
                - é–‹é ­ä»¥ğŸ“è¿™ä¸ªemojiå¼€é ­ï¼Œ ğŸ“å¾Œé¢æ¥é€™æ˜¯ä¸€ä»½ä»€éº¼æ–‡ä»¶ï¼Œæ–‡ä»¶ç›®çš„æ˜¯ä»€éº¼ï¼Œ
                - ç¬¬äºŒå¥ä¸‹ä¸€è¡Œï¼Œé–‹é ­ä»¥ ğŸ’¡ï¼Œè¨˜å¾—æ›è¡Œï¼Œç›´æ¥èªªçµè«–ï¼Œå›ç­”è·Ÿæˆ‘å¤„ç†è¦æ±‚æœ‰é—œçš„çµè«– éƒ½æ˜¯ç°¡çŸ­ä¸€å¥è©±
                - è¯·æä¾›ç»“æ„åŒ–çš„åˆ†æç»“æœï¼Œåªå›ç­”é‡é»å°±å¥½ï¼Œè¨˜å¾—ä¸å»¢è©±ã€‚
                - ç¬¬ä¸€å¥å°±èªªé‡é»ä¸ç”¨å®¢å¥—ã€‚ ä¸ç”¨èªª æ ¹æ®æ‚¨æä¾›çš„æ–‡æ¡£å†…å®¹... é€™ç¨®å»¢è©±ï¼Œç›´æ¥èªªé‡é»
                - å›ç­”è¦çµè«–å…ˆèªªï¼Œå¯ä»¥ä½¿ç”¨emojiå¹«åŠ©ä½¿ç”¨è€…é–±è®€ï¼Œmarkdownæ ¼å¼
                - å¦‚æœæ–‡æª”å…§æ²’æœ‰è·Ÿæˆ‘çš„å•é¡Œæœ‰é—œçš„è³‡è¨Šï¼Œå°±èªªä¸€å¥ æ–‡æª”å…§æœªæåŠ å¥è™Ÿ ä¸€å¥è©±å°±å¥½  ä¸å‡†å»¢è©± æˆ‘æ²’æ™‚é–“çœ‹

                æ–‡æ¡£å†…å®¹:
                {document.content}
                """
            
            logger.info("================================================")
            logger.info(f"Processing document (streaming): {document.title} in {language}")
            
            # è¿”å›æµå¼å“åº”ç”Ÿæˆå™¨
            return self.gemini_service.call_api_stream(prompt, model_type)
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡æ¡£å¤±è´¥: {e}")
            error_msg = f"å¤„ç†æ–‡æ¡£æ—¶å‡ºé”™: {e}" if language == "ä¸­æ–‡" else f"Error processing document: {e}"
            # å¯¹äºé”™è¯¯ï¼Œè¿”å›ä¸€ä¸ªç®€å•çš„ç”Ÿæˆå™¨
            def error_generator():
                yield error_msg
            return error_generator()

    def integrate_results(self, document_results: List[Dict], integration_prompt: str, user_question: str, ticker: str, model_type: str) -> str:
        """æ•´åˆåˆ†æç»“æœ"""
        try:
            # è·å–å½“å‰è¯­è¨€è®¾ç½®
            language = st.session_state.get("selected_language", "English")
            
            # æ„å»ºæ•´åˆæç¤ºè¯
            if language == "English":
                integration_input = f"""
                You are a professional financial analyst, specialized in integrating analysis results from multiple documents.

                User Question: {user_question}
                Stock Ticker: {ticker}
                
                Integration Requirements: {integration_prompt}
                
                Requirements:
                - If the content contains numbers for the same indicator at different time points, place a pivot table at the very beginning of the answer. Format: pivot table row names are different indicators, column names are the time when indicators were published, cells are the indicator numbers. Then explain below the pivot table after generation.
                - If the content contains business descriptions for the same indicator at different time points, place a pivot table at the very beginning of the answer. Format: pivot table row names are different indicators, column names are the time when indicators were published, cells are the indicator descriptions. Then explain below the pivot table after generation.
                - For example: row1 would be Indicator, 2025Q1, 2025Q2. row2 would be AI commercialization, Q2 expected to resume double-digit year-over-year growth, confident in achieving significant revenue growth for full year 2025
                - table output use markdown format, ensure markdown format is correct, no errors
                - Comprehensively analyze all provided document analysis results
                - Identify trends, patterns, and key changes
                - Provide deep insights and professional recommendations
                - Use tables, lists, and other formats to enhance readability
                - Highlight key information and critical findings
                - This is a comprehensive summary, don't repeat detailed content from individual documents
                - Focus on cross-document trends and correlations
                - Always answer in English
                
                Document Analysis Results:
                """
            else:  # ä¸­æ–‡
                integration_input = f"""
                ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆï¼Œä¸“é—¨è´Ÿè´£æ•´åˆå¤šä¸ªæ–‡æ¡£çš„åˆ†æç»“æœã€‚

                ç”¨æˆ·é—®é¢˜: {user_question}
                è‚¡ç¥¨ä»£ç : {ticker}
                
                æ•´åˆè¦æ±‚: {integration_prompt}
                
                è¦æ±‚ï¼š
                - å¦‚æœå…§æ–‡æœ‰ åŒæŒ‡æ¨™ä¸åŒæ™‚é–“é»çš„ æ•¸å­—ï¼Œå›ç­”çš„æœ€ä¸€é–‹å§‹ ä¸€å®šè¦æ”¾ä¸Šä¸€å€‹pivot tableï¼Œæ ¼å¼æ˜¯ pivot table row name æ˜¯ä¸åŒæŒ‡æ¨™ ï¼Œ column æŒ‡æ¨™å…¬å¸ƒçš„æ™‚é–“ï¼Œcell æ˜¯æŒ‡æ¨™çš„æ•¸å­—ã€‚ç„¶å¾Œpivot table ç”Ÿæˆå®Œ è¡¨æ ¼ä¸‹æ–¹è§£é‡‹ä¸€ä¸‹
                - å¦‚æœå…§æ–‡æœ‰ åŒæŒ‡æ¨™ä¸åŒæ™‚é–“é»çš„ æ¥­å‹™çš„æè¿°ï¼Œå›ç­”çš„æœ€ä¸€é–‹å§‹ ä¸€å®šè¦æ”¾ä¸Šä¸€å€‹pivot tableï¼Œæ ¼å¼æ˜¯ pivot table row name æ˜¯ä¸åŒæŒ‡æ¨™ ï¼Œ column æŒ‡æ¨™å…¬å¸ƒçš„æ™‚é–“ï¼Œcell æ˜¯æŒ‡æ¨™çš„æ•¸å­—ã€‚ç„¶å¾Œpivot table ç”Ÿæˆå®Œ è¡¨æ ¼ä¸‹æ–¹è§£é‡‹ä¸€ä¸‹
                - - èˆ‰ä¾‹é¡ä¼¼åƒæ˜¯  row1æœƒæ˜¯ æŒ‡æ¨™, 2025Q1, 2025Q2 ã€‚ row2æœƒæ˜¯ AIå•†ä¸šåŒ–, Q2é¢„è®¡å°†æ¢å¤ä¸¤ä½æ•°åŒæ¯”å¢é•¿, æœ‰ä¿¡å¿ƒåœ¨2025å…¨å¹´å¹´å®ç°æ˜¾è‘—æ”¶å…¥å¢é•¿
                - table éƒ½ç”¨markdownæ ¼å¼ï¼Œè¦ç¢ºä¿markdownæ ¼å¼æ­£ç¢ºï¼Œä¸è¦æœ‰éŒ¯èª¤
                - ç»¼åˆåˆ†ææ‰€æœ‰æä¾›çš„æ–‡æ¡£åˆ†æç»“æœ
                - è¯†åˆ«è¶‹åŠ¿ã€æ¨¡å¼å’Œå…³é”®å˜åŒ–
                - æä¾›æ·±å…¥çš„æ´å¯Ÿå’Œä¸“ä¸šå»ºè®®
                - ä½¿ç”¨è¡¨æ ¼ã€åˆ—è¡¨ç­‰æ ¼å¼å¢å¼ºå¯è¯»æ€§
                - çªå‡ºé‡ç‚¹ä¿¡æ¯å’Œå…³é”®å‘ç°
                - è¿™æ˜¯ä¸€ä¸ªç»¼åˆæ€»ç»“ï¼Œä¸è¦é‡å¤å•ä¸ªæ–‡æ¡£çš„è¯¦ç»†å†…å®¹
                - é‡ç‚¹å…³æ³¨è·¨æ–‡æ¡£çš„è¶‹åŠ¿å’Œå…³è”æ€§
                
                æ–‡æ¡£åˆ†æç»“æœ:
                """
            
            for result in document_results:
                integration_input += f"""
                
                === {result['title']} ({result['date']}) ===
                {result['analysis']}
                """
            
            completion_text = "Please provide a complete, professional comprehensive analysis report and summary." if language == "English" else "è¯·æä¾›å®Œæ•´ã€ä¸“ä¸šçš„ç»¼åˆåˆ†ææŠ¥å‘Šå’Œæ€»ç»“ã€‚"
            integration_input += f"\n\n{completion_text}"
            
            return self.gemini_service.call_api(integration_input, model_type)
            
        except Exception as e:
            logger.error(f"æ•´åˆç»“æœå¤±è´¥: {e}")
            error_msg = f"æ•´åˆç»“æœæ—¶å‡ºé”™: {e}" if st.session_state.get("selected_language", "English") == "ä¸­æ–‡" else f"Error integrating results: {e}"
            return error_msg
    
    def integrate_results_stream(self, document_results: List[Dict], integration_prompt: str, user_question: str, ticker: str, model_type: str):
        """æ•´åˆåˆ†æç»“æœ - æµå¼å“åº”ç‰ˆæœ¬"""
        try:
            # è·å–å½“å‰è¯­è¨€è®¾ç½®
            language = st.session_state.get("selected_language", "English")
            
            # æ„å»ºæ•´åˆæç¤ºè¯
            if language == "English":
                integration_input = f"""
                You are a professional financial analyst, specialized in integrating analysis results from multiple documents.

                User Question: {user_question}
                Stock Ticker: {ticker}
                
                Integration Requirements: {integration_prompt}
                
                Requirements:
                - If the content contains numbers for the same indicator at different time points, place a pivot table at the very beginning of the answer. Format: pivot table row names are different indicators, column names are the time when indicators were published, cells are the indicator numbers. Then explain below the pivot table after generation.
                - If the content contains business descriptions for the same indicator at different time points, place a pivot table at the very beginning of the answer. Format: pivot table row names are different indicators, column names are the time when indicators were published, cells are the indicator descriptions. Then explain below the pivot table after generation.
                - For example: row1 would be Indicator, 2025Q1, 2025Q2. row2 would be AI commercialization, Q2 expected to resume double-digit year-over-year growth, confident in achieving significant revenue growth for full year 2025
                - table output use markdown format, ensure markdown format is correct, no errors
                - Comprehensively analyze all provided document analysis results
                - Identify trends, patterns, and key changes
                - Provide deep insights and professional recommendations
                - Use tables, lists, and other formats to enhance readability
                - Highlight key information and critical findings
                - This is a comprehensive summary, don't repeat detailed content from individual documents
                - Focus on cross-document trends and correlations
                - Always answer in English
                
                Document Analysis Results:
                """
            else:  # ä¸­æ–‡
                integration_input = f"""
                ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆï¼Œä¸“é—¨è´Ÿè´£æ•´åˆå¤šä¸ªæ–‡æ¡£çš„åˆ†æç»“æœã€‚

                ç”¨æˆ·é—®é¢˜: {user_question}
                è‚¡ç¥¨ä»£ç : {ticker}
                
                æ•´åˆè¦æ±‚: {integration_prompt}
                
                è¦æ±‚ï¼š
                - å¦‚æœå…§æ–‡æœ‰ åŒæŒ‡æ¨™ä¸åŒæ™‚é–“é»çš„ æ•¸å­—ï¼Œå›ç­”çš„æœ€ä¸€é–‹å§‹ ä¸€å®šè¦æ”¾ä¸Šä¸€å€‹pivot tableï¼Œæ ¼å¼æ˜¯ pivot table row name æ˜¯ä¸åŒæŒ‡æ¨™ ï¼Œ column æŒ‡æ¨™å…¬å¸ƒçš„æ™‚é–“ï¼Œcell æ˜¯æŒ‡æ¨™çš„æ•¸å­—ã€‚ç„¶å¾Œpivot table ç”Ÿæˆå®Œ è¡¨æ ¼ä¸‹æ–¹è§£é‡‹ä¸€ä¸‹
                - å¦‚æœå…§æ–‡æœ‰ åŒæŒ‡æ¨™ä¸åŒæ™‚é–“é»çš„ æ¥­å‹™çš„æè¿°ï¼Œå›ç­”çš„æœ€ä¸€é–‹å§‹ ä¸€å®šè¦æ”¾ä¸Šä¸€å€‹pivot tableï¼Œæ ¼å¼æ˜¯ pivot table row name æ˜¯ä¸åŒæŒ‡æ¨™ ï¼Œ column æŒ‡æ¨™å…¬å¸ƒçš„æ™‚é–“ï¼Œcell æ˜¯æŒ‡æ¨™çš„æ•¸å­—ã€‚ç„¶å¾Œpivot table ç”Ÿæˆå®Œ è¡¨æ ¼ä¸‹æ–¹è§£é‡‹ä¸€ä¸‹
                - - èˆ‰ä¾‹é¡ä¼¼åƒæ˜¯  row1æœƒæ˜¯ æŒ‡æ¨™, 2025Q1, 2025Q2 ã€‚ row2æœƒæ˜¯ AIå•†ä¸šåŒ–, Q2é¢„è®¡å°†æ¢å¤ä¸¤ä½æ•°åŒæ¯”å¢é•¿, æœ‰ä¿¡å¿ƒåœ¨2025å…¨å¹´å¹´å®ç°æ˜¾è‘—æ”¶å…¥å¢é•¿
                - table éƒ½ç”¨markdownæ ¼å¼ï¼Œè¦ç¢ºä¿markdownæ ¼å¼æ­£ç¢ºï¼Œä¸è¦æœ‰éŒ¯èª¤
                - ç»¼åˆåˆ†ææ‰€æœ‰æä¾›çš„æ–‡æ¡£åˆ†æç»“æœ
                - è¯†åˆ«è¶‹åŠ¿ã€æ¨¡å¼å’Œå…³é”®å˜åŒ–
                - æä¾›æ·±å…¥çš„æ´å¯Ÿå’Œä¸“ä¸šå»ºè®®
                - ä½¿ç”¨è¡¨æ ¼ã€åˆ—è¡¨ç­‰æ ¼å¼å¢å¼ºå¯è¯»æ€§
                - çªå‡ºé‡ç‚¹ä¿¡æ¯å’Œå…³é”®å‘ç°
                - è¿™æ˜¯ä¸€ä¸ªç»¼åˆæ€»ç»“ï¼Œä¸è¦é‡å¤å•ä¸ªæ–‡æ¡£çš„è¯¦ç»†å†…å®¹
                - é‡ç‚¹å…³æ³¨è·¨æ–‡æ¡£çš„è¶‹åŠ¿å’Œå…³è”æ€§
                
                æ–‡æ¡£åˆ†æç»“æœ:
                """
            
            for result in document_results:
                integration_input += f"""
                
                === {result['title']} ({result['date']}) ===
                {result['analysis']}
                """
            
            completion_text = "Please provide a complete, professional comprehensive analysis report and summary." if language == "English" else "è¯·æä¾›å®Œæ•´ã€ä¸“ä¸šçš„ç»¼åˆåˆ†ææŠ¥å‘Šå’Œæ€»ç»“ã€‚"
            integration_input += f"\n\n{completion_text}"
            
            # è¿”å›æµå¼å“åº”ç”Ÿæˆå™¨
            return self.gemini_service.call_api_stream(integration_input, model_type)
            
        except Exception as e:
            logger.error(f"æ•´åˆç»“æœå¤±è´¥: {e}")
            error_msg = f"æ•´åˆç»“æœæ—¶å‡ºé”™: {e}" if st.session_state.get("selected_language", "English") == "ä¸­æ–‡" else f"Error integrating results: {e}"
            # å¯¹äºé”™è¯¯ï¼Œè¿”å›ä¸€ä¸ªç®€å•çš„ç”Ÿæˆå™¨
            def error_generator():
                yield error_msg
            return error_generator()
    
    def process_all_documents_fast(self, documents: List[Document], fast_mode_prompt: str, model_type: str):
        """å¿«é€Ÿæ¨¡å¼ï¼šä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰æ–‡æ¡£ - æµå¼å“åº”ç‰ˆæœ¬"""
        try:
            # è·å–å½“å‰è¯­è¨€è®¾ç½®
            language = st.session_state.get("selected_language", "English")
            
            # é¢„å¤„ç†6-Kæ–‡ä»¶
            logger.info(f"ğŸ” [6K-FAST] å¼€å§‹é¢„å¤„ç†æ–‡æ¡£ï¼Œæ€»æ•°: {len(documents)}")
            
            # è°ƒè¯•æ—¥å¿—ï¼šæ˜¾ç¤ºä¼ å…¥çš„æ–‡æ¡£
            for i, doc in enumerate(documents):
                logger.info(f"ğŸ” [6K-FAST] ä¼ å…¥æ–‡æ¡£{i+1}: {doc.title}, ç±»å‹: {doc.type}, form_type: {getattr(doc, 'form_type', 'None')}")
            
            processed_documents = []
            
            for i, document in enumerate(documents):
                logger.info(f"ğŸ” [6K-FAST] å¤„ç†æ–‡æ¡£{i+1}: {document.title}, ç±»å‹: {document.type}")
                logger.info(f"ğŸ” [6K-FAST] æ–‡æ¡£å±æ€§: form_type={getattr(document, 'form_type', 'None')}, date={document.date}")
                
                # ç‰¹æ®Šå¤„ç†6-Kæ–‡ä»¶
                if hasattr(document, 'form_type') and document.form_type == '6-K':
                    logger.info(f"ğŸ” [6K-FAST] âœ… è¿›å…¥6-Kå¤„ç†åˆ†æ”¯: {document.title}")
                    logger.info(f"ğŸ” [6K-FAST] æ£€æµ‹åˆ°6-Kæ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†: {document.title}")
                    
                    # åˆå§‹åŒ–6-Kå¤„ç†å™¨
                    self.sec_service._init_sixk_processor(self.document_manager.temp_dir)
                    
                    # è·å–tickerå’ŒCIK
                    ticker = st.session_state.analyzer_ticker
                    ticker_map = self.sec_service.get_cik_map()
                    cik = ticker_map.get(ticker.upper(), '')
                    
                    logger.info(f"ğŸ” [6K-FAST] 6-Kå¤„ç†å‚æ•°: Ticker={ticker}, CIK={cik}")
                    
                    # å¤„ç†6-Kæ–‡ä»¶
                    processed_6k_docs = self.sec_service.sixk_processor.process_6k_filing(
                        ticker, cik, document.url, document
                    )
                    
                    logger.info(f"ğŸ” [6K-FAST] 6-Kå¤„ç†ç»“æœ: {len(processed_6k_docs)} ä¸ªæ–‡æ¡£")
                    
                    if processed_6k_docs:
                        processed_documents.extend(processed_6k_docs)
                        logger.info(f"âœ… [6K-FAST] 6-Kæ–‡ä»¶å¤„ç†æˆåŠŸï¼Œæ·»åŠ  {len(processed_6k_docs)} ä¸ªæ–‡æ¡£")
                    else:
                        logger.info(f"â„¹ï¸ [6K-FAST] 6-Kæ–‡ä»¶æ²¡æœ‰ex99é™„ä»¶ï¼Œè·³è¿‡")
                        # ä¸æ·»åŠ ç©ºçš„6-Kæ–‡æ¡£
                else:
                    # é6-Kæ–‡ä»¶ï¼Œç›´æ¥æ·»åŠ 
                    logger.info(f"ğŸ” [6K-FAST] âœ… é6-Kæ–‡ä»¶ï¼Œç›´æ¥æ·»åŠ : {document.title}")
                    processed_documents.append(document)
            
            logger.info(f"ğŸ” [6K-FAST] é¢„å¤„ç†å®Œæˆï¼Œæœ€ç»ˆæ–‡æ¡£æ•°: {len(processed_documents)}")
            
            # ä¸‹è½½æ‰€æœ‰æ–‡æ¡£å†…å®¹
            all_content = ""
            for i, document in enumerate(processed_documents):
                if not document.content:
                    if document.type == 'SEC Filing':
                        document.content = self.sec_service.download_filing(document.url)
                    elif document.type == 'HK Stock Filing':
                        document.content = self.hk_service.download_hk_filing(document.url)
                    elif document.type == 'Saudi Exchange Filing':
                        document.content = self.saudi_service.download_saudi_filing(document.url)
                    elif document.type == 'Earnings Call':
                        logger.warning(f"å¤„ç†æ–‡æ¡£æ—¶å‘ç°è´¢æŠ¥è®°å½•å†…å®¹ä¸ºç©º: {document.title}")
                        document.content = "å†…å®¹æœªæ‰¾åˆ°" if language == "ä¸­æ–‡" else "Content not found"
                
                # æ·»åŠ æ–‡æ¡£åˆ†éš”ç¬¦å’Œæ ‡é¢˜
                separator = f"\n\n{'='*60}\n"
                doc_header = f"æ–‡æ¡£ {i+1}: {document.title}\næ—¥æœŸ: {document.date}\nç±»å‹: {document.type}\n" if language == "ä¸­æ–‡" else f"Document {i+1}: {document.title}\nDate: {document.date}\nType: {document.type}\n"
                separator += doc_header
                separator += f"{'='*60}\n\n"
                
                all_content += separator + (document.content or "")
            
            # é™åˆ¶æ€»å†…å®¹é•¿åº¦
            if len(all_content) > config.MAX_CONTENT_LENGTH:
                all_content = all_content[:config.MAX_CONTENT_LENGTH] + f"\n[å†…å®¹å·²æˆªæ–­]" if language == "ä¸­æ–‡" else f"\n[Content truncated]"
            
            # æ„å»ºå®Œæ•´çš„æç¤ºè¯
            if language == "English":
                prompt = f"""
                You are a professional financial analyst, specialized in analyzing multiple financial documents simultaneously.

                Analysis Requirements: {fast_mode_prompt}
                
                Requirements:
                - Analyze all provided documents comprehensively
                - Extract relevant information according to the user's specific requirements
                - Provide accurate, professional analysis
                - Ensure answers come from document content, don't imagine
                - I don't have time to read, ensure answers are direct and to the point, no need for polite conversation
                - Always answer in English
                - when markdown output, Escape all dollar signs $ for currency as ï¼„ to prevent Markdown from rendering them as math.
                
                Answer Requirements:
                - Start with ğŸ“Š emoji, followed by a brief summary of what documents were analyzed
                - Second line Start with ğŸ’¡ on next new line row, directly state conclusions related to the user's question
                - Please provide structured analysis results, only answer key points, remember no nonsense
                - First sentence should state key points without pleasantries
                - Answer should start with conclusions, can use emojis to help users read, markdown format
                - If the documents don't contain information related to the question, just say "Not mentioned in documents" period, one sentence only
                
                Table Requirements:
                - If the content contains numbers for the same indicator at different time points, or question is about guidance, place a pivot table at the very beginning of the answer. Format: pivot table row names are different indicators, column names are the time when indicators were published, cells are the indicator numbers. put both guidance and actual numbers from next quarter report.Then explain below the pivot table after generation.
                - If the content contains business descriptions for the same indicator at different time points, or question is about guidance, place a pivot table at the very beginning of the answer. Format: pivot table row names are different indicators, column names are the time when indicators were published, cells are the indicator descriptions. put both guidance and actual numbers from next quarter report.Then explain below the pivot table after generation.
                - table output use markdown format, ensure markdown format is correct, no errors
                - å¦‚æœå•guidanceï¼Œtable æ ¼å¼å¦‚ä¸‹ï¼Œæ³¨æ„ å¦‚æœå•æ•´ç†guidance éƒ½æ˜¯è¦æ•´ç†å‡ºæ–‡ä»¶å…§æ‰€æœ‰çš„å­£åº¦çš„
                - - column name æ˜¯ä¸åŒæ–‡ä»¶çš„æ™‚é–“ï¼Œæ‰€æœ‰æœ‰æåˆ°çš„æ–‡ä»¶éƒ½è¦ç´å…¥ï¼ŒåŒä¸€å¤©çš„çµ±æ•´åœ¨ä¸€èµ·ã€‚
                - - row name å°±æ˜¯æŒ‡è¡¨ åƒæ˜¯ æ”¶å…¥ æ¯›åˆ©ç‡ æˆ–æ˜¯ DAU MAU é€™ç¨®æŒ‡æ¨™ï¼Œåˆ†æˆactual å’Œ guidanceã€‚actual å°±æ˜¯æ”¾è²¡å ±å…§å¯¦éš›å…¬å¸ƒçš„æ•¸å­—ï¼Œguidanceå°±æ˜¯æ”¾earning callæˆ–æ˜¯è­‰äº¤æ‰€å…¬å‘Šå…§çµ¦çš„å°æœªä¾†çš„guidanceæ•¸å­—æˆ–æ˜¯æè¿°
                example:
                |              | 2025Q1 (2025/1/20) | 2025Q2 (2025/4/20) | 2025Q3 (2025/7/20) | 8-K (2025/8/20) |
                |--------------|--------------------|--------------------|--------------------|-----------------|
                | DAU Actual   | 3äº¿                | 3.5äº¿              | 4äº¿                |                 |
                | DAU Guidance | é¢„è®¡Q2é”åˆ°3.5å„„    | é¢„è®¡Q3é”åˆ°4å„„      | é¢„è®¡Q4é”åˆ°4.5å„„    | é¢„è®¡Q4é”åˆ°6å„„   |
                | GMV Actual   | 100äº¿              | 200äº¿              | 300äº¿              |                 |
                | GMV Guidance | é¢„è®¡Q2é”åˆ°200å„„    | é¢„è®¡Q3é”åˆ°250å„„    | é¢„è®¡Q4é”åˆ°300å„„    | é¢„è®¡Q4é”åˆ°500å„„ |
                - å¦‚æœæœ‰å…¶ä»–æ•´ç†æˆè¡¨æ ¼çš„ä»»å‹™ï¼Œç¬¬ä¸€å€‹table æ ¼å¼ä¹Ÿè¦è·Ÿä¸Šè®Šä¸€æ¨£çš„ column name æ˜¯ä¸åŒæ™‚é–“ï¼Œrow name æ˜¯ä¸åŒæŒ‡æ¨™ã€‚ç„¶å¾Œç¬¬äºŒå€‹table æ‰æ˜¯è©³ç´°çš„è¡¨æ ¼
                - è®ŠåŒ–é€™å€‹å­—çœ¼ï¼Œå¦‚æœå•çš„æ˜¯è®ŠåŒ–ï¼Œå°±æ˜¯æŠŠå¤šå€‹æ™‚é–“é»çš„è³‡æ–™ï¼Œæ•´ç†æˆè¡¨æ ¼ï¼Œä¸‹é¢å†æ–‡å­—èªªæ˜ï¼Œæ³¨æ„æ˜¯å¤šå€‹æ™‚é–“é»çš„è³‡æ–™ï¼Œä¸æ˜¯åªæœ‰å…©å€‹æ™‚é–“é»çš„è³‡æ–™

                All Document Contents:
                {all_content}
                """
            elif language == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©":
                prompt = f"""
                Ø£Ù†Øª Ù…Ø­Ù„Ù„ Ù…Ø§Ù„ÙŠ Ù…Ø­ØªØ±ÙØŒ Ù…ØªØ®ØµØµ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…Ø§Ù„ÙŠØ© Ù…ØªØ¹Ø¯Ø¯Ø© ÙÙŠ Ù†ÙØ³ Ø§Ù„ÙˆÙ‚Øª.

                Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„: {fast_mode_prompt}
                
                Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª:
                - ØªØ­Ù„ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ø¨Ø´ÙƒÙ„ Ø´Ø§Ù…Ù„
                - Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© ÙˆÙÙ‚Ø§Ù‹ Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©
                - ØªÙ‚Ø¯ÙŠÙ… ØªØ­Ù„ÙŠÙ„ Ø¯Ù‚ÙŠÙ‚ ÙˆÙ…Ù‡Ù†ÙŠ
                - ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª ØªØ£ØªÙŠ Ù…Ù† Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§ØªØŒ Ù„Ø§ ØªØªØ®ÙŠÙ„
                - Ù„ÙŠØ³ Ù„Ø¯ÙŠ ÙˆÙ‚Øª Ù„Ù„Ù‚Ø±Ø§Ø¡Ø©ØŒ ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ù…Ø¨Ø§Ø´Ø±Ø© ÙˆÙ…Ø­Ø¯Ø¯Ø©ØŒ Ù„Ø§ Ø­Ø§Ø¬Ø© Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ù…Ù‡Ø°Ø¨Ø©
                - Ø§Ø¬Ø¨ Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
                - Ø¹Ù†Ø¯ Ø¥Ø®Ø±Ø§Ø¬ markdownØŒ ØªØ¬Ù†Ø¨ Ø§Ø³ØªØ®Ø¯Ø§Ù… $ Ù„Ù„Ø¹Ù…Ù„Ø© ÙˆØ§Ø³ØªØ®Ø¯Ù… ï¼„ Ù„Ù…Ù†Ø¹ Markdown Ù…Ù† Ø¹Ø±Ø¶Ù‡Ø§ ÙƒØ±ÙŠØ§Ø¶ÙŠØ§Øª.
                
                Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:
                - Ø§Ø¨Ø¯Ø£ Ø¨Ù€ ğŸ“Š emojiØŒ Ù…ØªØ¨ÙˆØ¹Ø§Ù‹ Ø¨Ù…Ù„Ø®Øµ Ù…ÙˆØ¬Ø² Ù„Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… ØªØ­Ù„ÙŠÙ„Ù‡Ø§
                - Ø§Ù„Ø³Ø·Ø± Ø§Ù„Ø«Ø§Ù†ÙŠ ÙŠØ¨Ø¯Ø£ Ø¨Ù€ ğŸ’¡ ÙÙŠ Ø³Ø·Ø± Ø¬Ø¯ÙŠØ¯ØŒ Ø§Ø°ÙƒØ± Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ø¨Ø§Ø´Ø±Ø© Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
                - ÙŠØ±Ø¬Ù‰ ØªÙ‚Ø¯ÙŠÙ… Ù†ØªØ§Ø¦Ø¬ ØªØ­Ù„ÙŠÙ„ Ù…Ù†Ø¸Ù…Ø©ØŒ Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙÙ‚Ø·ØŒ ØªØ°ÙƒØ± Ø¹Ø¯Ù… Ø§Ù„Ø«Ø±Ø«Ø±Ø©
                - Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ ÙŠØ¬Ø¨ Ø£Ù† ØªØ°ÙƒØ± Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø¨Ø¯ÙˆÙ† Ù…Ø¬Ø§Ù…Ù„Ø§Øª
                - ÙŠØ¬Ø¨ Ø£Ù† ØªØ¨Ø¯Ø£ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„Ù†ØªØ§Ø¦Ø¬ØŒ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ØªØ¹Ø¨ÙŠØ±ÙŠØ© Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©ØŒ ØªÙ†Ø³ÙŠÙ‚ markdown
                - Ø¥Ø°Ø§ Ù„Ù… ØªØ­ØªÙˆ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„Ø³Ø¤Ø§Ù„ØŒ ÙÙ‚Ø· Ù‚Ù„ "ØºÙŠØ± Ù…Ø°ÙƒÙˆØ± ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª" Ù†Ù‚Ø·Ø©ØŒ Ø¬Ù…Ù„Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·
                
                Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø¬Ø¯ÙˆÙ„:
                - Ø¥Ø°Ø§ Ø§Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¹Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù… Ù„Ù†ÙØ³ Ø§Ù„Ù…Ø¤Ø´Ø± ÙÙŠ Ù†Ù‚Ø§Ø· Ø²Ù…Ù†ÙŠØ© Ù…Ø®ØªÙ„ÙØ©ØŒ Ø£Ùˆ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø­ÙˆÙ„ Ø§Ù„ØªÙˆØ¬ÙŠÙ‡Ø§ØªØŒ Ø¶Ø¹ Ø¬Ø¯ÙˆÙ„Ø§Ù‹ Ù…Ø­ÙˆØ±ÙŠØ§Ù‹ ÙÙŠ Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©. Ø§Ù„ØªÙ†Ø³ÙŠÙ‚: Ø£Ø³Ù…Ø§Ø¡ ØµÙÙˆÙ Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ø­ÙˆØ±ÙŠ Ù‡ÙŠ Ù…Ø¤Ø´Ø±Ø§Øª Ù…Ø®ØªÙ„ÙØ©ØŒ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ù‡ÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø°ÙŠ ØªÙ… Ù†Ø´Ø± Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª ÙÙŠÙ‡ØŒ Ø§Ù„Ø®Ù„Ø§ÙŠØ§ Ù‡ÙŠ Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª. Ø¶Ø¹ ÙƒÙ„Ø§Ù‹ Ù…Ù† Ø£Ø±Ù‚Ø§Ù… Ø§Ù„ØªÙˆØ¬ÙŠÙ‡ ÙˆØ§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„ÙØ¹Ù„ÙŠØ© Ù…Ù† ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø±Ø¨Ø¹ Ø§Ù„ØªØ§Ù„ÙŠ. Ø«Ù… Ø§Ø´Ø±Ø­ Ø£Ø³ÙÙ„ Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ø­ÙˆØ±ÙŠ Ø¨Ø¹Ø¯ Ø§Ù„Ø¥Ù†Ø´Ø§Ø¡.
                - Ø¥Ø°Ø§ Ø§Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¹Ù„Ù‰ Ø£ÙˆØµØ§Ù ØªØ¬Ø§Ø±ÙŠØ© Ù„Ù†ÙØ³ Ø§Ù„Ù…Ø¤Ø´Ø± ÙÙŠ Ù†Ù‚Ø§Ø· Ø²Ù…Ù†ÙŠØ© Ù…Ø®ØªÙ„ÙØ©ØŒ Ø£Ùˆ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø­ÙˆÙ„ Ø§Ù„ØªÙˆØ¬ÙŠÙ‡Ø§ØªØŒ Ø¶Ø¹ Ø¬Ø¯ÙˆÙ„Ø§Ù‹ Ù…Ø­ÙˆØ±ÙŠØ§Ù‹ ÙÙŠ Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©. Ø§Ù„ØªÙ†Ø³ÙŠÙ‚: Ø£Ø³Ù…Ø§Ø¡ ØµÙÙˆÙ Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ø­ÙˆØ±ÙŠ Ù‡ÙŠ Ù…Ø¤Ø´Ø±Ø§Øª Ù…Ø®ØªÙ„ÙØ©ØŒ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ù‡ÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø°ÙŠ ØªÙ… Ù†Ø´Ø± Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª ÙÙŠÙ‡ØŒ Ø§Ù„Ø®Ù„Ø§ÙŠØ§ Ù‡ÙŠ Ø£ÙˆØµØ§Ù Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª. Ø¶Ø¹ ÙƒÙ„Ø§Ù‹ Ù…Ù† Ø£Ø±Ù‚Ø§Ù… Ø§Ù„ØªÙˆØ¬ÙŠÙ‡ ÙˆØ§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„ÙØ¹Ù„ÙŠØ© Ù…Ù† ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø±Ø¨Ø¹ Ø§Ù„ØªØ§Ù„ÙŠ. Ø«Ù… Ø§Ø´Ø±Ø­ Ø£Ø³ÙÙ„ Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ø­ÙˆØ±ÙŠ Ø¨Ø¹Ø¯ Ø§Ù„Ø¥Ù†Ø´Ø§Ø¡.
                - Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ†Ø³ÙŠÙ‚ markdownØŒ ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† ØªÙ†Ø³ÙŠÙ‚ markdown ØµØ­ÙŠØ­ØŒ Ø¨Ø¯ÙˆÙ† Ø£Ø®Ø·Ø§Ø¡
                
                Ø¬Ù…ÙŠØ¹ Ù…Ø­ØªÙˆÙŠØ§Øª Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª:
                {all_content}
                """
            else:  # ä¸­æ–‡
                prompt = f"""
                ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆï¼Œä¸“é—¨è´Ÿè´£åŒæ—¶åˆ†æå¤šä¸ªé‡‘èæ–‡æ¡£ã€‚

                åˆ†æè¦æ±‚: {fast_mode_prompt}
                
                è¦æ±‚ï¼š
                - å…¨é¢åˆ†ææ‰€æœ‰æä¾›çš„æ–‡æ¡£
                - æ ¹æ®ç”¨æˆ·çš„å…·ä½“è¦æ±‚æå–ç›¸å…³ä¿¡æ¯
                - æä¾›å‡†ç¡®ã€ä¸“ä¸šçš„åˆ†æ
                - ç¡®ä¿å›ç­”éƒ½æ¥è‡ªæ–‡æ¡£å†…å®¹ï¼Œä¸è¦å‡­ç©ºæƒ³è±¡
                - æˆ‘æ²¡æ—¶é—´çœ‹ï¼Œç¡®ä¿å›ç­”ç›´æ¥è¯´é‡ç‚¹ï¼Œä¸ç”¨åƒäººä¸€æ ·è¿˜è¦å®¢å¥—è¯
                - markdownè¾“å‡ºï¼Œå°†æ‰€æœ‰è¡¨ç¤ºé‡‘é¢çš„ $ æ”¹ä¸º ï¼„ï¼Œä»¥é¿å… Markdown è¢«è¯¯åˆ¤ä¸ºæ•°å­¦å…¬å¼
                
                å›ç­”è¦æ±‚ï¼š
                - å¼€å¤´ä»¥ğŸ“Šè¿™ä¸ªemojiå¼€å¤´ï¼ŒğŸ“Šåé¢æ¥ç®€è¦è¯´æ˜åˆ†æäº†å“ªäº›æ–‡æ¡£
                - ç¬¬äºŒå¥ä¸‹ä¸€è¡Œï¼Œå¼€å¤´ä»¥ğŸ’¡ï¼Œè®°å¾—æ¢è¡Œï¼Œç›´æ¥è¯´ç»“è®ºï¼Œå›ç­”è·Ÿç”¨æˆ·é—®é¢˜æœ‰å…³çš„ç»“è®ºï¼Œéƒ½æ˜¯ç®€çŸ­ä¸€å¥è¯
                - è¯·æä¾›ç»“æ„åŒ–çš„åˆ†æç»“æœï¼Œåªå›ç­”é‡ç‚¹å°±å¥½ï¼Œè®°å¾—ä¸åºŸè¯
                - ç¬¬ä¸€å¥å°±è¯´é‡ç‚¹ä¸ç”¨å®¢å¥—ï¼Œç›´æ¥è¯´é‡ç‚¹
                - å›ç­”è¦ç»“è®ºå…ˆè¯´ï¼Œå¯ä»¥ä½¿ç”¨emojiå¸®åŠ©ä½¿ç”¨è€…é˜…è¯»ï¼Œmarkdownæ ¼å¼
                - å¦‚æœæ–‡æ¡£å†…æ²¡æœ‰è·Ÿé—®é¢˜æœ‰å…³çš„ä¿¡æ¯ï¼Œå°±è¯´ä¸€å¥"æ–‡æ¡£å†…æœªæåŠ"å¥å·ï¼Œä¸€å¥è¯å°±å¥½ï¼Œä¸å‡†åºŸè¯
                
                è¡¨æ ¼è¦æ±‚ï¼š
                - å¦‚æœå…§æ–‡æœ‰ åŒæŒ‡æ¨™ä¸åŒæ™‚é–“é»çš„ æ•¸å­— æˆ–æ˜¯å•guidanceï¼Œå•é¡Œé©åˆæ•´ç†æˆè¡¨æ ¼çš„ å›ç­”çš„æœ€ä¸€é–‹å§‹ ä¸€å®šè¦æ”¾ä¸Šä¸€å€‹pivot tableï¼Œæ ¼å¼æ˜¯ pivot table row name æ˜¯ä¸åŒæŒ‡æ¨™ ï¼Œ column æŒ‡æ¨™å…¬å¸ƒçš„æ™‚é–“ï¼Œcell æ˜¯æŒ‡æ¨™çš„æ•¸å­—ã€‚guidanceæ•¸å­— é‚„æœ‰ä¸‹ä¸€å­£å ±å‡ºçš„ actual æ•¸å­— éƒ½è¦æ”¾ã€‚ç„¶å¾Œpivot table ç”Ÿæˆå®Œ è¡¨æ ¼ä¸‹æ–¹è§£é‡‹ä¸€ä¸‹
                - å¦‚æœå…§æ–‡æœ‰ åŒæŒ‡æ¨™ä¸åŒæ™‚é–“é»çš„ æ¥­å‹™çš„æè¿° æˆ–æ˜¯å•guidanceï¼Œå•é¡Œé©åˆæ•´ç†æˆè¡¨æ ¼çš„ å›ç­”çš„æœ€ä¸€é–‹å§‹ ä¸€å®šè¦æ”¾ä¸Šä¸€å€‹pivot tableï¼Œæ ¼å¼æ˜¯ pivot table row name æ˜¯ä¸åŒæŒ‡æ¨™ ï¼Œ column æŒ‡æ¨™å…¬å¸ƒçš„æ™‚é–“ï¼Œcell æ˜¯æŒ‡æ¨™çš„æ•¸å­—ã€‚guidanceæ•¸å­— é‚„æœ‰ä¸‹ä¸€å­£å ±å‡ºçš„ actual æ•¸å­— éƒ½è¦æ”¾ã€‚ç„¶å¾Œpivot table ç”Ÿæˆå®Œ è¡¨æ ¼ä¸‹æ–¹è§£é‡‹ä¸€ä¸‹
                - table éƒ½ç”¨markdownæ ¼å¼ï¼Œè¦ç¢ºä¿markdownæ ¼å¼æ­£ç¢ºï¼Œä¸è¦æœ‰éŒ¯èª¤
                - å¦‚æœå•guidanceï¼Œtable æ ¼å¼å¦‚ä¸‹ï¼Œæ³¨æ„ å¦‚æœå•æ•´ç†guidance éƒ½æ˜¯è¦æ•´ç†å‡ºæ–‡ä»¶å…§æ‰€æœ‰çš„å­£åº¦çš„
                - - column name æ˜¯ä¸åŒæ–‡ä»¶çš„æ™‚é–“ï¼Œæ‰€æœ‰æœ‰æåˆ°çš„æ–‡ä»¶éƒ½è¦ç´å…¥ï¼ŒåŒä¸€å¤©çš„çµ±æ•´åœ¨ä¸€èµ·ã€‚
                - - row name å°±æ˜¯æŒ‡è¡¨ åƒæ˜¯ æ”¶å…¥ æ¯›åˆ©ç‡ æˆ–æ˜¯ DAU MAU é€™ç¨®æŒ‡æ¨™ï¼Œåˆ†æˆactual å’Œ guidanceã€‚actual å°±æ˜¯æ”¾è²¡å ±å…§å¯¦éš›å…¬å¸ƒçš„æ•¸å­—ï¼Œguidanceå°±æ˜¯æ”¾earning callæˆ–æ˜¯è­‰äº¤æ‰€å…¬å‘Šå…§çµ¦çš„å°æœªä¾†çš„guidanceæ•¸å­—æˆ–æ˜¯æè¿°
                example:
                |              | 2025Q1 (2025/1/20) | 2025Q2 (2025/4/20) | 2025Q3 (2025/7/20) | 8-K (2025/8/20) |
                |--------------|--------------------|--------------------|--------------------|-----------------|
                | DAU Actual   | 3äº¿                | 3.5äº¿              | 4äº¿                |                 |
                | DAU Guidance | é¢„è®¡Q2é”åˆ°3.5å„„    | é¢„è®¡Q3é”åˆ°4å„„      | é¢„è®¡Q4é”åˆ°4.5å„„    | é¢„è®¡Q4é”åˆ°6å„„   |
                | GMV Actual   | 100äº¿              | 200äº¿              | 300äº¿              |                 |
                | GMV Guidance | é¢„è®¡Q2é”åˆ°200å„„    | é¢„è®¡Q3é”åˆ°250å„„    | é¢„è®¡Q4é”åˆ°300å„„    | é¢„è®¡Q4é”åˆ°500å„„ |
                - å¦‚æœæœ‰å…¶ä»–æ•´ç†æˆè¡¨æ ¼çš„ä»»å‹™ï¼Œç¬¬ä¸€å€‹table æ ¼å¼ä¹Ÿè¦è·Ÿä¸Šè®Šä¸€æ¨£çš„ column name æ˜¯ä¸åŒæ™‚é–“ï¼Œrow name æ˜¯ä¸åŒæŒ‡æ¨™ã€‚ç„¶å¾Œç¬¬äºŒå€‹table æ‰æ˜¯è©³ç´°çš„è¡¨æ ¼
                - è®ŠåŒ–é€™å€‹å­—çœ¼ï¼Œå¦‚æœå•çš„æ˜¯è®ŠåŒ–ï¼Œå°±æ˜¯æŠŠå¤šå€‹æ™‚é–“é»çš„è³‡æ–™ï¼Œæ•´ç†æˆè¡¨æ ¼ï¼Œä¸‹é¢å†æ–‡å­—èªªæ˜ï¼Œæ³¨æ„æ˜¯å¤šå€‹æ™‚é–“é»çš„è³‡æ–™ï¼Œä¸æ˜¯åªæœ‰å…©å€‹æ™‚é–“é»çš„è³‡æ–™
                
                æ‰€æœ‰æ–‡æ¡£å†…å®¹:
                {all_content}
                """
            
            logger.info("================================================")
            logger.info(f"Processing all documents in fast mode in {language}")
            
            # è¿”å›æµå¼å“åº”ç”Ÿæˆå™¨
            return self.gemini_service.call_api_stream(prompt, model_type)
            
        except Exception as e:
            logger.error(f"å¿«é€Ÿæ¨¡å¼å¤„ç†å¤±è´¥: {e}")
            error_msg = f"å¿«é€Ÿæ¨¡å¼å¤„ç†æ—¶å‡ºé”™: {e}" if language == "ä¸­æ–‡" else f"Error in fast mode processing: {e}"
            # å¯¹äºé”™è¯¯ï¼Œè¿”å›ä¸€ä¸ªç®€å•çš„ç”Ÿæˆå™¨
            def error_generator():
                yield error_msg
            return error_generator()

# åˆå§‹åŒ–åº”ç”¨
@st.cache_resource
def initialize_app():
    """åˆå§‹åŒ–åº”ç”¨"""
    analyzer = SECEarningsAnalyzer()
    # ç¡®ä¿æ‰€æœ‰æœåŠ¡éƒ½å·²æ­£ç¡®åˆå§‹åŒ–
    logger.info(f"ğŸ”§ [INIT] åˆå§‹åŒ–å®Œæˆï¼ŒæœåŠ¡åˆ—è¡¨: {[attr for attr in dir(analyzer) if attr.endswith('_service')]}")
    return analyzer

# ä¸»é¡µé¢
def main():
    """ä¸»é¡µé¢å‡½æ•°"""
    # åœ¨åº”ç”¨åˆå§‹åŒ–ä¹‹å‰ï¼Œç¡®ä¿session stateå·²ç»å­˜åœ¨
    try:
        SessionManager.init_session_state()
    except Exception as e:
        # é™é»˜å¤„ç†åˆå§‹åŒ–é”™è¯¯
        logger.warning(f"Session state initialization warning: {e}")
        # ç¡®ä¿åŸºæœ¬çš„session stateå­˜åœ¨
        if "analyzer_messages" not in st.session_state:
            st.session_state.analyzer_messages = []
        if "analyzer_ticker" not in st.session_state:
            st.session_state.analyzer_ticker = ""
    
    # å®šä¹‰å›é€€å‡½æ•°ç”¨äºè·å–å¯†é’¥
    def get_secret_value(key: str, default=None):
        """ä» st.secrets æˆ–ç¯å¢ƒå˜é‡ä¸­è·å–å¯†é’¥å€¼"""
        import os
        import json
        from pathlib import Path
        
        # æ£€æŸ¥ secrets.toml æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        secrets_paths = [
            Path(".streamlit/secrets.toml"),
            Path("/root/.streamlit/secrets.toml"),
            Path("/app/.streamlit/secrets.toml")
        ]
        
        secrets_file_exists = any(path.exists() for path in secrets_paths)
        
        if secrets_file_exists:
            try:
                return st.secrets[key]
            except KeyError:
                # å¦‚æœ secrets.toml å­˜åœ¨ä½†æ²¡æœ‰è¯¥é”®ï¼Œå›é€€åˆ°ç¯å¢ƒå˜é‡
                pass
        
        # ç›´æ¥ä»ç¯å¢ƒå˜é‡è¯»å–
        env_value = os.environ.get(key, default)
        if env_value is None:
            return default
            
        # å°è¯•è§£æ JSON æ ¼å¼çš„ç¯å¢ƒå˜é‡ï¼ˆç”¨äºåˆ—è¡¨ç±»å‹çš„å¯†é’¥ï¼‰
        if isinstance(env_value, str) and env_value.startswith('[') and env_value.endswith(']'):
            try:
                return json.loads(env_value)
            except json.JSONDecodeError:
                return env_value
        
        return env_value
    
    # è™•ç†URLåƒæ•¸
    query_params = st.query_params
    if "p" in query_params:
        param_value = query_params["p"]
        access_code = get_secret_value("ACCESS_CODE", "")
        if param_value.lower() == access_code.lower():
            st.session_state.use_premium_api = True
            st.session_state.premium_access_code = param_value
            st.success("ğŸ‰ å·²å•Ÿç”¨ä»˜è²»APIæœå‹™ï¼")

    # åˆå§‹åŒ–åº”ç”¨
    analyzer = initialize_app()
    
    # æ£€æŸ¥analyzeræ˜¯å¦æœ‰æ‰€éœ€çš„æœåŠ¡
    if not hasattr(analyzer, 'saudi_service'):
        st.error("âŒ åº”ç”¨åˆå§‹åŒ–é”™è¯¯ï¼šç¼ºå°‘æ²™ç‰¹äº¤æ˜“æ‰€æœåŠ¡")
        st.info("ğŸ”„ æ­£åœ¨æ¸…é™¤ç¼“å­˜å¹¶é‡æ–°åˆå§‹åŒ–...")
        st.cache_resource.clear()
        st.rerun()
    
    # è·å–å½“å‰è¯­è¨€è®¾ç½®
    current_language = st.session_state.get("selected_language", "English")
    lang_config = LANGUAGE_CONFIG[current_language]
    
    # é¡µé¢æ ‡é¢˜
    st.title(lang_config["title"])

    # å·¦ä¾§è¾¹æ 
    with st.sidebar:
        st.header(lang_config["sidebar_header"])
        
        # Tickerè¾“å…¥
        ticker_input = st.text_input(
            lang_config["ticker_label"],
            value=st.session_state.analyzer_ticker,
            placeholder=lang_config["ticker_placeholder"]
        )
        
        # æ™ºèƒ½å¤„ç†tickeræ ¼å¼
        if ticker_input:
            if is_saudi_stock(ticker_input):
                ticker = normalize_saudi_ticker(ticker_input)
                st.info(f"{lang_config['saudi_stock_info']} - {ticker}")
            elif is_hk_stock(ticker_input):
                ticker = normalize_hk_ticker(ticker_input)
                st.info(lang_config["hk_stock_info"].format(ticker))
            else:
                ticker = ticker_input.upper()
                st.info(lang_config["us_stock_info"])
        else:
            ticker = ""
        
        # å¹´ä»½é€‰æ‹©
        years = st.number_input(
            lang_config["years_label"],
            min_value=1,
            max_value=10,
            value=st.session_state.analyzer_years,
            step=1
        )
        
        # åŠ¨æ€æ˜¾ç¤º cutoff date
        current_year = datetime.now().year
        cutoff_date = datetime(current_year - years + 1, 1, 1).date()
        
        # æ·»åŠ è¯´æ˜
        if current_language == "ä¸­æ–‡":
            st.caption(f"ğŸ“… Data Date: {cutoff_date} ~ now")
        else:
            st.caption(f"ğŸ“… Data Date: {cutoff_date} ~ now")
        
        # æ•°æ®ç±»å‹é€‰æ‹© - æ ¹æ®è‚¡ç¥¨ç±»å‹æ˜¾ç¤ºä¸åŒé€‰é¡¹
        st.subheader(lang_config["data_type_header"])
        
        if is_saudi_stock(ticker):
            # æ²™ç‰¹äº¤æ˜“æ‰€é€‰é¡¹ - ç®€åŒ–ä¸ºæ‰€æœ‰å…¬å‘Šç±»å‹
            use_sec_reports = True  # è‡ªåŠ¨é€‰æ‹©æ‰€æœ‰å…¬å‘Š
            use_sec_others = True   # è‡ªåŠ¨é€‰æ‹©æ‰€æœ‰å…¬å‘Š
            use_earnings = False    # æ²™ç‰¹äº¤æ˜“æ‰€æ²¡æœ‰earnings call
            
            # æ˜¾ç¤ºä¿¡æ¯ç»™ç”¨æˆ·
            if current_language == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©":
                st.info("ğŸ‡¸ğŸ‡¦ Ø³ÙŠØªÙ… ØªØ­Ù„ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ø³ÙˆÙ‚ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©")
            elif current_language == "ä¸­æ–‡":
                st.info("ğŸ‡¸ğŸ‡¦ å°†åˆ†ææ²™ç‰¹äº¤æ˜“æ‰€çš„æ‰€æœ‰å…¬å‘Šç±»å‹")
            else:
                st.info("ğŸ‡¸ğŸ‡¦ Will analyze all announcement types from Saudi Exchange")
                
        elif is_hk_stock(ticker):
            # æ¸¯è‚¡é€‰é¡¹
            use_sec_reports = st.checkbox(lang_config["sec_reports_hk"], value=st.session_state.analyzer_use_sec_reports)
            use_sec_others = st.checkbox(lang_config["sec_others_hk"], value=st.session_state.analyzer_use_sec_others)
            
            use_earnings = st.checkbox(lang_config["earnings_label"], value=st.session_state.analyzer_use_earnings)
            st.caption(lang_config["earnings_caption"])
        else:
            # ç¾è‚¡é€‰é¡¹
            use_sec_reports = st.checkbox(lang_config["sec_reports_us"], value=st.session_state.analyzer_use_sec_reports)
            use_sec_others = st.checkbox(lang_config["sec_others_us"], value=st.session_state.analyzer_use_sec_others)
            
            use_earnings = st.checkbox(lang_config["earnings_label"], value=st.session_state.analyzer_use_earnings)
            st.caption(lang_config["earnings_caption"])
        
        # æ¨¡å‹é€‰æ‹©
        st.subheader(lang_config["model_header"])
        model_type = st.selectbox(
            lang_config["model_label"],
            list(config.MODELS.keys()),
            index=list(config.MODELS.keys()).index(st.session_state.analyzer_model),
            format_func=lambda x: config.MODELS[x]
        )
        
        # åˆ†ææ¨¡å¼é€‰æ‹©
        st.subheader(lang_config["analysis_mode_header"])
        analysis_mode = st.selectbox(
            lang_config["analysis_mode_label"],
            options=["fast_mode", "detailed_mode"],
            index=0 if st.session_state.analyzer_analysis_mode == "fast_mode" else 1,
            format_func=lambda x: lang_config["fast_mode"] if x == "fast_mode" else lang_config["detailed_mode"]
        )
        
        # æ˜¾ç¤ºæ¨¡å¼è¯´æ˜
        if analysis_mode == "fast_mode":
            st.caption(lang_config["fast_mode_caption"])
        else:
            st.caption(lang_config["detailed_mode_caption"])
        
        # ä»˜è²»APIè¨­ç½®
        st.subheader(lang_config["api_header"])
        
        # è¼¸å…¥æ¡†
        access_code = st.text_input(
            lang_config["access_code_label"],
            value=st.session_state.get("premium_access_code", ""),
            placeholder=lang_config["access_code_placeholder"]
        )
        
        # é¡¯ç¤ºç•¶å‰ç‹€æ…‹
        if st.session_state.get("use_premium_api", False):
            st.success(lang_config["premium_enabled"])
        else:
            st.info(lang_config["free_api"])
        
        # æª¢æŸ¥è¼¸å…¥
        if access_code:
            valid_access_code = get_secret_value("ACCESS_CODE", "")
            if access_code.lower() == valid_access_code.lower():
                if not st.session_state.get("use_premium_api", False):
                    st.session_state.use_premium_api = True
                    st.session_state.premium_access_code = access_code
                    # å½“å¯ç”¨ä»˜è´¹APIæ—¶ï¼Œé»˜è®¤é€‰æ‹©2.5 proæ¨¡å‹
                    st.session_state.analyzer_model = "gemini-2.5-pro"
                    st.success(lang_config["premium_success"])
                    st.rerun()
            else:
                st.error(lang_config["access_code_error"])
        
        # è¯­è¨€é€‰æ‹©
        st.subheader(lang_config["language_header"])
        selected_language = st.selectbox(
            lang_config["language_label"],
            options=["English", "ä¸­æ–‡", "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"],
            index=0 if st.session_state.get("selected_language", "English") == "English" else (1 if st.session_state.get("selected_language", "English") == "ä¸­æ–‡" else 2)
        )
        
        # å¦‚æœè¯­è¨€æ”¹å˜ï¼Œæ›´æ–°session stateå¹¶é‡æ–°è¿è¡Œ
        if selected_language != st.session_state.get("selected_language", "English"):
            st.session_state.selected_language = selected_language
            st.rerun()
        
        # è°ƒè¯•ï¼šæ¸…é™¤ç¼“å­˜æŒ‰é’®
        if st.button("ğŸ”„ æ¸…é™¤åº”ç”¨ç¼“å­˜", help="å¦‚æœé‡åˆ°åˆå§‹åŒ–é—®é¢˜ï¼Œç‚¹å‡»æ­¤æŒ‰é’®"):
            st.cache_resource.clear()
            st.success("âœ… ç¼“å­˜å·²æ¸…é™¤ï¼Œé¡µé¢å°†é‡æ–°åŠ è½½")
            st.rerun()
        
        # æ›´æ–°session state
        st.session_state.analyzer_ticker = ticker
        st.session_state.analyzer_years = years
        st.session_state.analyzer_use_sec_reports = use_sec_reports
        st.session_state.analyzer_use_sec_others = use_sec_others
        st.session_state.analyzer_use_earnings = use_earnings
        st.session_state.analyzer_model = model_type
        st.session_state.analyzer_analysis_mode = analysis_mode
    
    # ä¸»å†…å®¹åŒºåŸŸ

    
    # æ˜¾ç¤ºå†å²å¯¹è¯å’Œå¤„ç†çŠ¶æ€
    for i, message in enumerate(st.session_state.analyzer_messages):
        with st.chat_message(message["role"], avatar=message.get("avatar")):
            st.markdown(message["content"])
            
            # å¦‚æœæ¶ˆæ¯åŒ…å«æ–‡æ¡£æ–‡ä»¶è·¯å¾„ï¼Œæ˜¾ç¤ºåŸæ–‡é¢„è§ˆ
            if message.get("temp_file_path") and os.path.exists(message["temp_file_path"]):
                file_content = analyzer.document_manager.get_download_content(message["temp_file_path"])
                if file_content:
                    # è§£ç æ–‡ä»¶å†…å®¹
                    content_text = file_content.decode('utf-8')
                    
                    # ä½¿ç”¨ expander æ¥æ˜¾ç¤ºåŸæ–‡å†…å®¹
                    with st.expander("ğŸ“„ æŸ¥çœ‹åŸæ–‡", expanded=False):
                        # æ·»åŠ ä¸€äº›æ ·å¼æ¥æ”¹å–„æ˜¾ç¤ºæ•ˆæœ
                        st.markdown("---")
                        
                        # æ˜¾ç¤ºæ–‡æ¡£ä¿¡æ¯
                        st.caption(f"ğŸ“‹ æ–‡æ¡£ï¼š{message.get('document_title', 'æœªçŸ¥æ–‡æ¡£')}")
                        
                        # ä½¿ç”¨å¯æ»šåŠ¨çš„æ–‡æœ¬åŒºåŸŸæ˜¾ç¤ºå†…å®¹ï¼Œæ·»åŠ å”¯ä¸€key
                        st.text_area(
                            label="åŸæ–‡å†…å®¹",
                            value=content_text,
                            height=400,
                            disabled=True,
                            label_visibility="collapsed",
                            key=f"text_area_{hash(message['temp_file_path'])}"
                        )
                        
                        # ä¸‹è½½æŒ‰é’®
                        filename = f"{message.get('document_title', 'document')}.txt"
                        st.download_button(
                            label="ğŸ’¾ ä¸‹è½½åŸæ–‡",
                            data=file_content,
                            file_name=filename,
                            mime="text/plain",
                            key=f"download_{hash(message['temp_file_path'])}",
                            help="ä¸‹è½½åŸæ–‡åˆ°æœ¬åœ°æ–‡ä»¶"
                        )
        
        # å¦‚æœè¿™æ˜¯æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ï¼Œå¹¶ä¸”æ­£åœ¨å¤„ç†ï¼Œæ˜¾ç¤ºçŠ¶æ€
        if (message["role"] == "user" and 
            i == len(st.session_state.analyzer_messages) - 1 and 
            analyzer.session_manager.get_processing_status().is_processing):
            
            # é€™è£¡ä¸å†é¡¯ç¤ºstatusï¼Œçµ±ä¸€åœ¨ä¸‹æ–¹è™•ç†
            pass
    
    # ä¸»èŠå¤©è¾“å…¥
    if prompt := st.chat_input(lang_config["chat_placeholder"]):
        # å°†ç”¨æˆ·æ¶ˆæ¯æ·»åŠ åˆ°å†å²è®°å½•
        st.session_state.analyzer_messages.append({"role": "user", "content": prompt})
        
        # å¯åŠ¨å¤„ç†æµç¨‹
        status = analyzer.session_manager.get_processing_status()
        status.is_processing = True
        status.user_question = prompt
        status.processing_step = 1
        status.stop_requested = False  # é‡ç½®åœæ­¢è¯·æ±‚
        analyzer.session_manager.update_processing_status(status)
        
        # å…³é”®æ”¹åŠ¨ï¼šåœ¨è¿™é‡Œè°ƒç”¨rerunæ¥ç«‹å³å¯åŠ¨å¤„ç†æµç¨‹å¹¶æ›´æ–°UI
        st.rerun()

    # åœ¨æ¯æ¬¡é‡æ–°è¿è¡Œè„šæœ¬æ—¶ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦å¤„ç†
    status = analyzer.session_manager.get_processing_status()
    if status.is_processing:
        # å¦‚æœæ­£åœ¨å¤„ç†ï¼Œæ˜¾ç¤ºstatus
        current_step = status.current_status_label or (lang_config.get("processing_status", "Processing..."))
        
        # with st.expander(lang_config["status_header"], expanded=False):
        with st.expander(status.current_status_label, expanded=False):
            st.markdown(f"**{status.current_status_label}**")
            
            if status.total_documents > 0:
                progress_text = lang_config["progress_text"].format(status.completed_documents, status.total_documents)
                # st.progress(status.progress_percentage / 100, text=progress_text)
            
            # åœæ­¢æŒ‰é’®
            if st.button(lang_config["stop_button"], key="stop_processing"):
                status.stop_requested = True
                status.is_processing = False
                status.current_status_label = lang_config["stop_success"]
                analyzer.session_manager.update_processing_status(status)
                
                # æ·»åŠ åœæ­¢æ¶ˆæ¯åˆ°èŠå¤©å†å²
                st.session_state.analyzer_messages.append({
                    "role": "assistant", 
                    "content": lang_config["processing_stopped"],
                    "avatar": "â¹ï¸"
                })
                
                st.rerun()
            
            # æ˜¾ç¤ºæ–‡æ¡£åˆ—è¡¨å’Œå¤„ç†çŠ¶æ€
            if status.documents:
                st.markdown("---")
                for idx, doc in enumerate(status.documents):
                    if idx < status.completed_documents:
                        status_icon = "âœ…"
                    elif idx == status.completed_documents:
                        status_icon = "ğŸ”„"
                    else:
                        status_icon = "â³"
                    
                    # å„ªåŒ–æ–‡æ¡£æ ‡é¢˜æ˜¾ç¤ºï¼Œå¢åŠ é•·åº¦é™åˆ¶
                    doc_title = doc.title
                    if len(doc_title) > 80:
                        doc_title = doc_title[:77] + "..."
                    
                    st.markdown(f"{status_icon} {doc_title} ({doc.date})")
            
            # æ˜¾ç¤ºé”™è¯¯æ¶ˆæ¯
            if status.error_message:
                st.error(f"âŒ {status.error_message}")
        
        # å°†ä¸»å¤„ç†é€»è¾‘ç§»åˆ° st.status ä¸­ï¼Œä»¥æä¾›å®æ—¶åé¦ˆ
        process_user_question_new(
            analyzer, ticker, years, 
            st.session_state.analyzer_use_sec_reports,
            st.session_state.analyzer_use_sec_others,
            use_earnings, model_type, st.session_state.analyzer_analysis_mode
        )

def process_user_question_new(analyzer: SECEarningsAnalyzer, ticker: str, years: int, use_sec_reports: bool, use_sec_others: bool, use_earnings: bool, model_type: str, analysis_mode: str = "detailed_mode"):
    """å¤„ç†ç”¨æˆ·é—®é¢˜çš„å®Œæ•´æµç¨‹ - æ–°ç‰ˆï¼Œå¸¦å®æ—¶çŠ¶æ€æ›´æ–°å’Œå¹¶è¡Œå¤„ç†"""
    status = analyzer.session_manager.get_processing_status()
    language = st.session_state.get("selected_language", "English")
    
    # è°ƒè¯•å‚æ•°
    logger.info(f"ğŸ” [DEBUG] å‡½æ•°å‚æ•°: ticker={ticker}, use_sec_reports={use_sec_reports}, use_sec_others={use_sec_others}, use_earnings={use_earnings}")
    
    # æ£€æŸ¥å¯¹è¯ç¼“å­˜ - åŒä¸€ä¸ªtickerå’Œyearsçš„æ•°æ®å¯ä»¥å¤ç”¨
    conversation_cache_key = f"conversation_data_{ticker}_{years}_{use_sec_reports}_{use_sec_others}_{use_earnings}"
    cached_documents = analyzer.cache_manager.get(conversation_cache_key)
    
    if cached_documents and len(cached_documents) > 0:
        logger.info(f"ğŸ”„ [CACHE] æ‰¾åˆ°ç¼“å­˜çš„å¯¹è¯æ•°æ®: {ticker}, {years}å¹´, å…±{len(cached_documents)}ä¸ªæ–‡æ¡£")
        # ç›´æ¥ä½¿ç”¨ç¼“å­˜çš„æ–‡æ¡£ï¼Œè·³è¿‡æ•°æ®è·å–æ­¥éª¤
        status.documents = cached_documents
        status.total_documents = len(cached_documents)
        
        # æ ¹æ®åˆ†ææ¨¡å¼è®¾ç½®æ­£ç¡®çš„å¤„ç†æ­¥éª¤
        if analysis_mode == "fast_mode":
            status.processing_step = 5  # å¿«é€Ÿæ¨¡å¼ï¼šè·³è½¬åˆ°å¿«é€Ÿæ¨¡å¼å¤„ç†
        else:
            status.processing_step = 3  # è¯¦ç»†æ¨¡å¼ï¼šè·³åˆ°æ–‡æ¡£å¤„ç†æ­¥éª¤
        
        if language == "English":
            status.add_status_message(f"ğŸ“‚ Using cached data: {len(cached_documents)} documents")
            status.current_status_label = f"ğŸ“‚ Using cached conversation data ({len(cached_documents)} documents)"
        else:
            status.add_status_message(f"ğŸ“‚ ä½¿ç”¨ç¼“å­˜æ•°æ®: {len(cached_documents)} ä¸ªæ–‡æ¡£")
            status.current_status_label = f"ğŸ“‚ ä½¿ç”¨ç¼“å­˜çš„å¯¹è¯æ•°æ® ({len(cached_documents)} ä¸ªæ–‡æ¡£)"
        
        analyzer.session_manager.update_processing_status(status)
        logger.info(f"ğŸš€ [CACHE] è·³è¿‡æ•°æ®è·å–ï¼Œç›´æ¥å¼€å§‹å¤„ç†ç¼“å­˜æ–‡æ¡£")
    else:
        logger.info(f"ğŸ” [CACHE] æœªæ‰¾åˆ°ç¼“å­˜æ•°æ®ï¼Œå°†é‡æ–°è·å–: {conversation_cache_key}")
    
    # æ£€æŸ¥æ˜¯å¦å·²è¯·æ±‚åœæ­¢
    if status.stop_requested:
        return
    
    try:
        # æ­¥éª¤1ï¼šåˆ†æé—®é¢˜
        if status.processing_step == 1:
            if language == "English":
                status.current_status_label = "ğŸ§  Analyzing your question..."
                status.add_status_message("Started analyzing user question")
                analyzer.session_manager.update_processing_status(status)
                
                status.current_status_label = "ğŸ” Parsing question content..."
                status.add_status_message("ğŸ” Parsing question content...")
                analyzer.session_manager.update_processing_status(status)
                
                status.current_status_label = "ğŸ¤– Calling AI model for analysis..."
                status.add_status_message("ğŸ¤– Calling AI model for analysis...")
                analyzer.session_manager.update_processing_status(status)
            else:
                status.current_status_label = "ğŸ§  æ­£åœ¨åˆ†ææ‚¨çš„é—®é¢˜..."
                status.add_status_message("é–‹å§‹åˆ†æç”¨æˆ¶å•é¡Œ")
                analyzer.session_manager.update_processing_status(status)
                
                status.current_status_label = "ğŸ” è§£æå•é¡Œå…§å®¹..."
                status.add_status_message("ğŸ” è§£æå•é¡Œå…§å®¹...")
                analyzer.session_manager.update_processing_status(status)
                
                status.current_status_label = "ğŸ¤– AI is analyzing..."
                status.add_status_message("ğŸ¤– AI is analyzing...")
                analyzer.session_manager.update_processing_status(status)
            
            # åˆ›å»ºAIåˆ†æçŠ¶æ€æ˜¾ç¤º
            ai_analysis_placeholder = st.empty()
            with ai_analysis_placeholder.status("ğŸ¤– AI is analyzing...", expanded=True) as ai_analysis_status:
                if language == "English":
                    ai_analysis_status.write("ğŸ” Parsing question intent...")
                    ai_analysis_status.write(f"ğŸ“ Question: {status.user_question}")
                    ai_analysis_status.write(f"ğŸ“Š Stock: {ticker}")
                    ai_analysis_status.write("ğŸ§  Calling AI model to generate analysis prompts...")
                    ai_analysis_status.write("â³ Waiting for AI response...")
                else:
                    ai_analysis_status.write("ğŸ” Parsing question intent...")
                    ai_analysis_status.write(f"ğŸ“ Question: {status.user_question}")
                    ai_analysis_status.write(f"ğŸ“Š Stock: {ticker}")
                    ai_analysis_status.write("ğŸ§  Calling AI model to generate analysis prompts...")
                    ai_analysis_status.write("â³ Waiting for AI response...")
                
                # æ ¹æ®åˆ†ææ¨¡å¼é€‰æ‹©ä¸åŒçš„å¤„ç†é€»è¾‘
                if analysis_mode == "fast_mode":
                    # å¿«é€Ÿæ¨¡å¼ï¼šç”Ÿæˆå¿«é€Ÿæ¨¡å¼æç¤ºè¯
                    fast_mode_prompt = analyzer.generate_fast_mode_prompt(status.user_question, ticker, model_type)
                    status.processing_prompt = fast_mode_prompt
                    status.integration_prompt = ""  # å¿«é€Ÿæ¨¡å¼ä¸éœ€è¦integration prompt
                else:
                    # è¯¦ç»†æ¨¡å¼ï¼šç”Ÿæˆè¯¦ç»†åˆ†ææç¤ºè¯
                    processing_prompt, integration_prompt = analyzer.analyze_question(status.user_question, ticker, model_type)
                    status.processing_prompt = processing_prompt
                    status.integration_prompt = integration_prompt
                
                ai_analysis_status.write("âœ… AIåˆ†æå®Œæˆï¼")
                ai_analysis_status.update(label="âœ… é—®é¢˜åˆ†æå®Œæˆ", state="complete")
            
            # æ¸…é™¤AIåˆ†æçŠ¶æ€æ˜¾ç¤º
            ai_analysis_placeholder.empty()
            
            success_msg = "âœ… User question analysis completed" if language == "English" else "âœ… ç”¨æˆ¶å•é¡Œåˆ†æå®Œæˆ"
            status.add_status_message(success_msg)
            status.processing_step = 2
            analyzer.session_manager.update_processing_status(status)
            time.sleep(0.1) # çŸ­æš‚åœç•™ï¼Œè®©ç”¨æˆ·çœ‹åˆ°æ¶ˆæ¯
            st.rerun()
        
        # æ­¥éª¤2ï¼šè·å–å’Œç­›é€‰æ–‡æ¡£
        elif status.processing_step == 2:
            if status.stop_requested:
                return
                
            if language == "English":
                status.current_status_label = "ğŸ“‚ Retrieving and filtering documents..."
                status.add_status_message("ğŸ” Started document retrieval")
                analyzer.session_manager.update_processing_status(status)
                
                all_docs = []

                # å®šä¹‰è¡¨å•ç»„
                REPORTS_FORMS = ['10-K', '10-Q', '20-F', '6-K', '424B4']
                OTHER_FORMS = ['8-K', 'S-8', 'DEF 14A', 'F-3']
                
                status.add_status_message("ğŸ“‹ Preparing document type filtering...")
                analyzer.session_manager.update_processing_status(status)
                
                selected_forms = []
                if use_sec_reports:
                    selected_forms.extend(REPORTS_FORMS)
                if use_sec_others:
                    selected_forms.extend(OTHER_FORMS)

                # è·å–æ–‡ä»¶ - æ ¹æ®è‚¡ç¥¨ä»£ç ç±»å‹é€‰æ‹©ä¸åŒçš„æœåŠ¡
                logger.info(f"ğŸ” [DEBUG-EN] è‚¡ç¥¨ç±»å‹åˆ¤æ–­: ticker={ticker}, is_hk={is_hk_stock(ticker)}, is_saudi={is_saudi_stock(ticker)}")
                logger.info(f"ğŸ” [DEBUG-EN] selected_forms: {selected_forms}")
                if selected_forms:
                    if is_hk_stock(ticker):
                        # æ¸¯è‚¡æ–‡ä»¶
                        status.current_status_label = "ğŸ¢ Connecting to Hong Kong Stock Exchange..."
                        status.add_status_message("ğŸ¢ Connecting to Hong Kong Stock Exchange...")
                        analyzer.session_manager.update_processing_status(status)
                        
                        status.current_status_label = "ğŸ“„ Retrieving Hong Kong stock filings list..."
                        status.add_status_message("ğŸ“„ Retrieving Hong Kong stock filings list...")
                        analyzer.session_manager.update_processing_status(status)
                        
                        # å°†è¡¨å•ç±»å‹è½¬æ¢ä¸ºæ¸¯è‚¡åˆ†ç±»
                        hk_forms = []
                        if any(form in REPORTS_FORMS for form in selected_forms):
                            hk_forms.append('quarterly_annual')
                        if any(form in OTHER_FORMS for form in selected_forms):
                            hk_forms.append('others')
                        
                        def hk_status_callback(msg):
                            status.add_status_message(msg)
                            analyzer.session_manager.update_processing_status(status)
                        
                        hk_filings = analyzer.hk_service.get_hk_filings(ticker, years, forms_to_include=hk_forms, status_callback=hk_status_callback)
                        all_docs.extend(hk_filings)
                        status.add_status_message(f"âœ… Successfully retrieved {len(hk_filings)} Hong Kong stock filings")
                    elif is_saudi_stock(ticker):
                        # æ²™ç‰¹äº¤æ˜“æ‰€æ–‡ä»¶
                        logger.info(f"ğŸ” [DEBUG-EN] æ£€æµ‹åˆ°æ²™ç‰¹è‚¡ç¥¨: {ticker}")
                        status.current_status_label = "ğŸ‡¸ğŸ‡¦ Connecting to Saudi Exchange..."
                        status.add_status_message("ğŸ‡¸ğŸ‡¦ Connecting to Saudi Exchange...")
                        analyzer.session_manager.update_processing_status(status)
                        
                        status.current_status_label = "ğŸ“„ Retrieving Saudi Exchange announcements..."
                        status.add_status_message("ğŸ“„ Retrieving Saudi Exchange announcements...")
                        analyzer.session_manager.update_processing_status(status)
                        
                        def saudi_status_callback(msg):
                            status.add_status_message(msg)
                            analyzer.session_manager.update_processing_status(status)
                        
                        saudi_filings = analyzer.saudi_service.get_saudi_filings(ticker, years, status_callback=saudi_status_callback)
                        logger.info(f"ğŸ” [DEBUG-EN] saudi_filings è¿”å›ç»“æœ: ç±»å‹={type(saudi_filings)}, é•¿åº¦={len(saudi_filings) if saudi_filings else 'None'}")
                        if saudi_filings:
                            for i, doc in enumerate(saudi_filings[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
                                logger.info(f"ğŸ” [DEBUG-EN] saudi_filings[{i}]: {doc.title}, ç±»å‹={doc.type}, æ—¥æœŸ={doc.date}")
                        all_docs.extend(saudi_filings)
                        logger.info(f"ğŸ” [DEBUG-EN] all_docs.extendå: all_docsé•¿åº¦={len(all_docs)}")
                        status.add_status_message(f"âœ… Successfully retrieved {len(saudi_filings)} Saudi Exchange announcements")
                    else:
                        # ç¾è‚¡SECæ–‡ä»¶
                        status.current_status_label = "ğŸ‡ºğŸ‡¸ Connecting to SEC database..."
                        status.add_status_message("ğŸ‡ºğŸ‡¸ Connecting to SEC database...")
                        analyzer.session_manager.update_processing_status(status)
                        
                        status.current_status_label = "ğŸ“„ Retrieving SEC filings list..."
                        status.add_status_message("ğŸ“„ Retrieving SEC filings list...")
                        analyzer.session_manager.update_processing_status(status)
                        
                        def sec_status_callback(msg):
                            status.add_status_message(msg)
                            analyzer.session_manager.update_processing_status(status)
                        
                        sec_filings = analyzer.sec_service.get_filings(ticker, years, forms_to_include=selected_forms, status_callback=sec_status_callback)
                        all_docs.extend(sec_filings)
                        status.add_status_message(f"âœ… Successfully retrieved {len(sec_filings)} SEC filings")
                
                # è·å–è´¢æŠ¥è®°å½• - æ”¯æŒç¾è‚¡å’Œæ¸¯è‚¡
                if use_earnings:
                    status.current_status_label = "ğŸ™ï¸ Connecting to earnings call transcript service..."
                    status.add_status_message("ğŸ™ï¸ Connecting to earnings call transcript service...")
                    analyzer.session_manager.update_processing_status(status)
                    
                    status.current_status_label = "ğŸ“‹ Retrieving available earnings call list..."
                    status.add_status_message("ğŸ“‹ Retrieving available earnings call list...")
                    analyzer.session_manager.update_processing_status(status)
                    
                    all_earnings_urls = analyzer.earnings_service.get_available_quarters(ticker)
                    
                    # ä¿®æ­£å¹´ä»½è®¡ç®—é€»è¾‘ï¼šä¸SECä¿æŒä¸€è‡´
                    current_year = datetime.now().year
                    cutoff_date = datetime(current_year - years + 1, 1, 1).date()  # å¾€å‰æ¨yearså¹´
                    status.add_status_message(f"â° Started retrieving earnings calls and filtering by cutoff date ({cutoff_date})...")
                    analyzer.session_manager.update_processing_status(status)

                    filtered_earnings_docs = []
                    
                    # ä½¿ç”¨å¹¶è¡Œå¤„ç†æ¥æå‡é€Ÿåº¦
                    status.add_status_message("ğŸ“„ Starting parallel processing of earnings calls...")
                    analyzer.session_manager.update_processing_status(status)
                    
                    # åˆ›å»ºearningsè·å–çŠ¶æ€æ˜¾ç¤º
                    earnings_status_placeholder = st.empty()
                    with earnings_status_placeholder.status("ğŸ™ï¸ Retrieving earnings call transcripts...", expanded=True) as earnings_status:
                        earnings_status.write(f"ğŸ“‹ Found {len(all_earnings_urls)} available earnings calls")
                        earnings_status.write(f"ğŸ“… Filtering by cutoff date: {cutoff_date}")
                        earnings_status.write("ğŸ”„ Starting batch processing...")
                        
                        # åˆ†æ‰¹å¤„ç†ä»¥é¿å…è¿‡å¤šå¹¶å‘è¯·æ±‚
                        batch_size = 1  # æ¯æ‰¹å¤„ç†1ä¸ª
                        for batch_start in range(0, len(all_earnings_urls), batch_size):
                            if status.stop_requested:
                                break
                                
                            batch_end = min(batch_start + batch_size, len(all_earnings_urls))
                            batch_urls = all_earnings_urls[batch_start:batch_end]
                            
                            status.add_status_message(f"ğŸ“„ Processing batch {batch_start//batch_size + 1}/{(len(all_earnings_urls) + batch_size - 1)//batch_size} ({batch_start + 1}-{batch_end}/{len(all_earnings_urls)})")
                            analyzer.session_manager.update_processing_status(status)
                            
                            # æ˜¾ç¤ºå½“å‰æ‰¹æ¬¡æ­£åœ¨å¤„ç†çš„earnings
                            earnings_status.write(f"ğŸ“¦ Processing batch {batch_start//batch_size + 1}/{(len(all_earnings_urls) + batch_size - 1)//batch_size}")
                            
                            # æ˜¾ç¤ºå½“å‰æ‰¹æ¬¡çš„å…·ä½“é¡¹ç›®
                            for url_path in batch_urls:
                                parsed_info = analyzer.earnings_service.parse_transcript_url(url_path)
                                if parsed_info:
                                    _ticker, year, quarter = parsed_info
                                    earnings_status.write(f"â³ å¼€å§‹è·å–: {_ticker} {year} Q{quarter}")
                            
                            # é¡ºåºå¤„ç†å½“å‰æ‰¹æ¬¡
                            batch_results = analyzer.earnings_service.get_earnings_transcript_batch(batch_urls, max_workers=1)
                            
                            # å¤„ç†æ‰¹æ¬¡ç»“æœ
                            for i, (url_path, transcript_info) in enumerate(zip(batch_urls, batch_results)):
                                if status.stop_requested:
                                    break
                                    
                                parsed_info = analyzer.earnings_service.parse_transcript_url(url_path)
                                if parsed_info:
                                    _ticker, year, quarter = parsed_info
                                    
                                    if transcript_info and transcript_info.get('parsed_successfully'):
                                        real_date = transcript_info.get('date')
                                        if real_date:
                                            if real_date >= cutoff_date:
                                                doc = Document(
                                                    type='Earnings Call',
                                                    title=f"{transcript_info['ticker']} {transcript_info['year']} Q{transcript_info['quarter']} Earnings Call",
                                                    date=real_date, url=url_path, content=transcript_info.get('content'),
                                                    year=transcript_info.get('year'), quarter=transcript_info.get('quarter')
                                                )
                                                filtered_earnings_docs.append(doc)
                                                earnings_status.write(f"âœ… æˆåŠŸè·å–: {_ticker} {year} Q{quarter} ({real_date})")
                                            else:
                                                earnings_status.write(f"â¹ï¸ æ—¥æœŸè¿‡æ—©ï¼Œåœæ­¢è·å–: {_ticker} {year} Q{quarter} ({real_date})")
                                                status.add_status_message(f"Earnings call date {real_date} is earlier than cutoff date, stopping retrieval")
                                                time.sleep(0.1)
                                                break
                                    else:
                                        earnings_status.write(f"âš ï¸ è·å–å¤±è´¥: {_ticker} {year} Q{quarter}")
                                        logger.warning(f"Failed to retrieve or parse earnings call, skipping: {_ticker} {year} Q{quarter}")
                            
                            # å¦‚æœå‘ç°æ—¥æœŸè¿‡æ—©ï¼Œåœæ­¢å¤„ç†
                            if batch_results and any(
                                result and result.get('parsed_successfully') and 
                                result.get('date') and result.get('date') < cutoff_date 
                                for result in batch_results
                            ):
                                break
                        
                        earnings_status.write(f"âœ… å®Œæˆï¼å…±è·å– {len(filtered_earnings_docs)} ä¸ªæœ‰æ•ˆçš„è´¢æŠ¥è®°å½•")
                        earnings_status.update(label="âœ… Earnings call retrieval completed", state="complete")
                    
                    # æ¸…é™¤earningsçŠ¶æ€æ˜¾ç¤º
                    earnings_status_placeholder.empty()
                    
                    all_docs.extend(filtered_earnings_docs)
                    status.add_status_message(f"âœ… Successfully filtered {len(filtered_earnings_docs)} relevant earnings calls")
                    analyzer.session_manager.update_processing_status(status)

                status.add_status_message("ğŸ“Š Organizing document list...")
                analyzer.session_manager.update_processing_status(status)
                
                all_docs.sort(key=lambda x: x.date, reverse=True)
                
                # æ²™ç‰¹äº¤æ˜“æ‰€æ–‡æ¡£å¹¶å‘ä¸‹è½½å†…å®¹
                saudi_docs = [doc for doc in all_docs if doc.type == 'Saudi Exchange Filing']
                if saudi_docs:
                    status.add_status_message(f"ğŸš€ Pre-downloading {len(saudi_docs)} Saudi Exchange documents...")
                    analyzer.session_manager.update_processing_status(status)
                    logger.info(f"ğŸš€ [SAUDI-BATCH] å¼€å§‹é¢„ä¸‹è½½ {len(saudi_docs)} ä¸ªæ²™ç‰¹æ–‡æ¡£")
                    
                    # åˆ›å»ºæ²™ç‰¹ä¸‹è½½çŠ¶æ€æ˜¾ç¤º
                    saudi_download_placeholder = st.empty()
                    
                    def saudi_status_callback(message, completed, total):
                        with saudi_download_placeholder.container():
                            st.info(f"ğŸ‡¸ğŸ‡¦ **Saudi Exchange Download Status**")
                            st.progress(completed / total if total > 0 else 0)
                            st.write(f"{message}")
                            st.write(f"Progress: {completed}/{total}")
                    
                    # ä½¿ç”¨æ‰¹é‡ä¸‹è½½
                    all_docs = analyzer.saudi_service.download_saudi_filings_batch(
                        all_docs, 
                        max_workers=5, 
                        status_callback=saudi_status_callback
                    )
                    
                    # æ¸…é™¤ä¸‹è½½çŠ¶æ€æ˜¾ç¤º
                    saudi_download_placeholder.empty()
                    
                    status.add_status_message(f"âœ… Saudi Exchange documents pre-downloaded")
                    analyzer.session_manager.update_processing_status(status)
                
                status.documents = all_docs
                status.update_progress(0, len(all_docs), "Document list ready")
                status.add_status_message(f"âœ… Document list ready, total {len(all_docs)} documents")
                
                # ç¼“å­˜å¯¹è¯æ•°æ® - ä¾›åç»­å¯¹è¯å¤ç”¨
                analyzer.cache_manager.set(conversation_cache_key, all_docs)
                logger.info(f"ğŸ’¾ [CACHE] å·²ç¼“å­˜å¯¹è¯æ•°æ®: {conversation_cache_key}, å…±{len(all_docs)}ä¸ªæ–‡æ¡£")
                
                # è°ƒè¯•æ—¥å¿—ï¼šæ˜¾ç¤ºæ–‡æ¡£ç±»å‹åˆ†å¸ƒ
                doc_types = {}
                for doc in all_docs:
                    doc_type = doc.type
                    doc_types[doc_type] = doc_types.get(doc_type, 0) + 1
                logger.info(f"ğŸ“Š [DEBUG] æ–‡æ¡£ç±»å‹åˆ†å¸ƒ: {doc_types}")
                logger.info(f"ğŸ“Š [DEBUG] è®¾ç½®status.documentsï¼Œæ€»æ•°: {len(all_docs)}")
                
                # æ ¹æ®åˆ†ææ¨¡å¼é€‰æ‹©ä¸åŒçš„å¤„ç†æ­¥éª¤
                if analysis_mode == "fast_mode":
                    status.processing_step = 5  # è·³è½¬åˆ°å¿«é€Ÿæ¨¡å¼å¤„ç†
                else:
                    status.processing_step = 3  # è¯¦ç»†æ¨¡å¼ï¼šé€ä¸ªå¤„ç†æ–‡æ¡£
                analyzer.session_manager.update_processing_status(status)
            
            else:
                # ä¸­æ–‡ç‰ˆæœ¬çš„æ¶ˆæ¯
                status.current_status_label = "ğŸ“‚ æ­£åœ¨æ£€ç´¢å’Œç­›é€‰æ–‡æ¡£..."
                status.add_status_message("ğŸ” é–‹å§‹æª¢ç´¢æ–‡æª”")
                analyzer.session_manager.update_processing_status(status)
                
                all_docs = []

                # å®šä¹‰è¡¨å•ç»„
                REPORTS_FORMS = ['10-K', '10-Q', '20-F', '6-K', '424B4']
                OTHER_FORMS = ['8-K', 'S-8', 'DEF 14A', 'F-3']
                
                status.add_status_message("ğŸ“‹ æº–å‚™æ–‡æª”é¡å‹ç¯©é¸...")
                analyzer.session_manager.update_processing_status(status)
                
                selected_forms = []
                if use_sec_reports:
                    selected_forms.extend(REPORTS_FORMS)
                if use_sec_others:
                    selected_forms.extend(OTHER_FORMS)

                # è·å–æ–‡ä»¶ - æ ¹æ®è‚¡ç¥¨ä»£ç ç±»å‹é€‰æ‹©ä¸åŒçš„æœåŠ¡
                if selected_forms:
                    if is_hk_stock(ticker):
                        # æ¸¯è‚¡æ–‡ä»¶
                        status.current_status_label = "ğŸ¢ æ­£åœ¨é€£æ¥æ¸¯è‚¡äº¤æ˜“æ‰€..."
                        status.add_status_message("ğŸ¢ æ­£åœ¨é€£æ¥æ¸¯è‚¡äº¤æ˜“æ‰€...")
                        analyzer.session_manager.update_processing_status(status)
                        
                        status.current_status_label = "ğŸ“„ æ­£åœ¨è·å–æ¸¯è‚¡æ–‡ä»¶åˆ—è¡¨..."
                        status.add_status_message("ğŸ“„ æ­£åœ¨è·å–æ¸¯è‚¡æ–‡ä»¶åˆ—è¡¨...")
                        analyzer.session_manager.update_processing_status(status)
                        
                        # å°†è¡¨å•ç±»å‹è½¬æ¢ä¸ºæ¸¯è‚¡åˆ†ç±»
                        hk_forms = []
                        if any(form in REPORTS_FORMS for form in selected_forms):
                            hk_forms.append('quarterly_annual')
                        if any(form in OTHER_FORMS for form in selected_forms):
                            hk_forms.append('others')
                        
                        def hk_status_callback(msg):
                            status.add_status_message(msg)
                            analyzer.session_manager.update_processing_status(status)
                        
                        hk_filings = analyzer.hk_service.get_hk_filings(ticker, years, forms_to_include=hk_forms, status_callback=hk_status_callback)
                        all_docs.extend(hk_filings)
                        status.add_status_message(f"âœ… æˆåŠŸè·å– {len(hk_filings)} ä»½æ¸¯è‚¡æ–‡ä»¶")
                    elif is_saudi_stock(ticker):
                        # æ²™ç‰¹äº¤æ˜“æ‰€æ–‡ä»¶
                        status.current_status_label = "ğŸ‡¸ğŸ‡¦ æ­£åœ¨é€£æ¥æ²™ç‰¹äº¤æ˜“æ‰€..."
                        status.add_status_message("ğŸ‡¸ğŸ‡¦ æ­£åœ¨é€£æ¥æ²™ç‰¹äº¤æ˜“æ‰€...")
                        analyzer.session_manager.update_processing_status(status)
                        
                        status.current_status_label = "ğŸ“„ æ­£åœ¨è·å–æ²™ç‰¹äº¤æ˜“æ‰€å…¬å‘Š..."
                        status.add_status_message("ğŸ“„ æ­£åœ¨è·å–æ²™ç‰¹äº¤æ˜“æ‰€å…¬å‘Š...")
                        analyzer.session_manager.update_processing_status(status)
                        
                        def saudi_status_callback(msg):
                            status.add_status_message(msg)
                            analyzer.session_manager.update_processing_status(status)
                        
                        saudi_filings = analyzer.saudi_service.get_saudi_filings(ticker, years, status_callback=saudi_status_callback)
                        all_docs.extend(saudi_filings)
                        status.add_status_message(f"âœ… æˆåŠŸè·å– {len(saudi_filings)} ä»½æ²™ç‰¹äº¤æ˜“æ‰€å…¬å‘Š")
                    else:
                        # ç¾è‚¡SECæ–‡ä»¶
                        status.current_status_label = "ğŸ‡ºğŸ‡¸ æ­£åœ¨é€£æ¥SECæ•¸æ“šåº«..."
                        status.add_status_message("ğŸ‡ºğŸ‡¸ æ­£åœ¨é€£æ¥SECæ•¸æ“šåº«...")
                        analyzer.session_manager.update_processing_status(status)
                        
                        status.current_status_label = "ğŸ“„ æ­£åœ¨è·å–SECæ–‡ä»¶åˆ—è¡¨..."
                        status.add_status_message("ğŸ“„ æ­£åœ¨è·å–SECæ–‡ä»¶åˆ—è¡¨...")
                        analyzer.session_manager.update_processing_status(status)
                        
                        def sec_status_callback(msg):
                            status.add_status_message(msg)
                            analyzer.session_manager.update_processing_status(status)
                        
                        sec_filings = analyzer.sec_service.get_filings(ticker, years, forms_to_include=selected_forms, status_callback=sec_status_callback)
                        all_docs.extend(sec_filings)
                        status.add_status_message(f"âœ… æˆåŠŸè·å– {len(sec_filings)} ä»½SECæ–‡ä»¶")
                
                # è·å–è´¢æŠ¥è®°å½• - æ”¯æŒç¾è‚¡å’Œæ¸¯è‚¡
                if use_earnings:
                    status.current_status_label = "ğŸ™ï¸ æ­£åœ¨é€£æ¥è²¡å ±æœƒè­°è¨˜éŒ„æœå‹™..."
                    status.add_status_message("ğŸ™ï¸ æ­£åœ¨é€£æ¥è²¡å ±æœƒè­°è¨˜éŒ„æœå‹™...")
                    analyzer.session_manager.update_processing_status(status)
                    
                    status.current_status_label = "ğŸ“‹ æ­£åœ¨è·å–å¯ç”¨è´¢æŠ¥åˆ—è¡¨..."
                    status.add_status_message("ğŸ“‹ æ­£åœ¨è·å–å¯ç”¨è´¢æŠ¥åˆ—è¡¨...")
                    analyzer.session_manager.update_processing_status(status)
                    
                    all_earnings_urls = analyzer.earnings_service.get_available_quarters(ticker)
                    
                    # ä¿®æ­£å¹´ä»½è®¡ç®—é€»è¾‘ï¼šä¸SECä¿æŒä¸€è‡´
                    current_year = datetime.now().year
                    cutoff_date = datetime(current_year - years + 1, 1, 1).date()  # å¾€å‰æ¨yearså¹´
                    status.add_status_message(f"â° é–‹å§‹é€ä¸€è·å–è´¢æŠ¥ä¸¦æŒ‰æˆªæ­¢æ—¥æœŸ ({cutoff_date}) ç¯©é¸...")
                    analyzer.session_manager.update_processing_status(status)

                    filtered_earnings_docs = []
                    
                    # ä½¿ç”¨å¹¶è¡Œå¤„ç†æ¥æå‡é€Ÿåº¦
                    status.add_status_message("ğŸ“„ å¼€å§‹å¹¶è¡Œå¤„ç†è´¢æŠ¥è®°å½•...")
                    analyzer.session_manager.update_processing_status(status)
                    
                    # åˆ›å»ºearningsè·å–çŠ¶æ€æ˜¾ç¤º
                    earnings_status_placeholder = st.empty()
                    with earnings_status_placeholder.status("ğŸ™ï¸ æ­£åœ¨è·å–è´¢æŠ¥ä¼šè®®è®°å½•...", expanded=True) as earnings_status:
                        earnings_status.write(f"ğŸ“‹ æ‰¾åˆ° {len(all_earnings_urls)} ä¸ªå¯ç”¨çš„è´¢æŠ¥è®°å½•")
                        earnings_status.write(f"ğŸ“… æŒ‰æˆªæ­¢æ—¥æœŸç­›é€‰: {cutoff_date}")
                        earnings_status.write("ğŸ”„ å¼€å§‹æ‰¹é‡å¤„ç†...")
                        
                        # åˆ†æ‰¹å¤„ç†ä»¥é¿å…è¿‡å¤šå¹¶å‘è¯·æ±‚
                        batch_size = 2  # æ¯æ‰¹å¤„ç†2ä¸ª
                        for batch_start in range(0, len(all_earnings_urls), batch_size):
                            if status.stop_requested:
                                break
                                
                            batch_end = min(batch_start + batch_size, len(all_earnings_urls))
                            batch_urls = all_earnings_urls[batch_start:batch_end]
                            
                            status.add_status_message(f"ğŸ“„ å¤„ç†æ‰¹æ¬¡ {batch_start//batch_size + 1}/{(len(all_earnings_urls) + batch_size - 1)//batch_size} ({batch_start + 1}-{batch_end}/{len(all_earnings_urls)})")
                            analyzer.session_manager.update_processing_status(status)
                            
                            # æ˜¾ç¤ºå½“å‰æ‰¹æ¬¡æ­£åœ¨å¤„ç†çš„earnings
                            earnings_status.write(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_start//batch_size + 1}/{(len(all_earnings_urls) + batch_size - 1)//batch_size}")
                            
                            # æ˜¾ç¤ºå½“å‰æ‰¹æ¬¡çš„å…·ä½“é¡¹ç›®
                            for url_path in batch_urls:
                                parsed_info = analyzer.earnings_service.parse_transcript_url(url_path)
                                if parsed_info:
                                    _ticker, year, quarter = parsed_info
                                    earnings_status.write(f"â³ å¼€å§‹è·å–: {_ticker} {year} Q{quarter}")
                            
                            # é¡ºåºå¤„ç†å½“å‰æ‰¹æ¬¡
                            batch_results = analyzer.earnings_service.get_earnings_transcript_batch(batch_urls, max_workers=1)
                            
                            # å¤„ç†æ‰¹æ¬¡ç»“æœ
                            for i, (url_path, transcript_info) in enumerate(zip(batch_urls, batch_results)):
                                if status.stop_requested:
                                    break
                                    
                                parsed_info = analyzer.earnings_service.parse_transcript_url(url_path)
                                if parsed_info:
                                    _ticker, year, quarter = parsed_info
                                    
                                    if transcript_info and transcript_info.get('parsed_successfully'):
                                        real_date = transcript_info.get('date')
                                        if real_date:
                                            if real_date >= cutoff_date:
                                                doc = Document(
                                                    type='Earnings Call',
                                                    title=f"{transcript_info['ticker']} {transcript_info['year']} Q{transcript_info['quarter']} Earnings Call",
                                                    date=real_date, url=url_path, content=transcript_info.get('content'),
                                                    year=transcript_info.get('year'), quarter=transcript_info.get('quarter')
                                                )
                                                filtered_earnings_docs.append(doc)
                                                earnings_status.write(f"âœ… æˆåŠŸè·å–: {_ticker} {year} Q{quarter} ({real_date})")
                                            else:
                                                earnings_status.write(f"â¹ï¸ æ—¥æœŸè¿‡æ—©ï¼Œåœæ­¢è·å–: {_ticker} {year} Q{quarter} ({real_date})")
                                                status.add_status_message(f"è´¢æŠ¥æ—¥æœŸ {real_date} æ—©äºæˆªæ­¢æ—¥æœŸï¼Œåœæ­¢è·å–")
                                                time.sleep(0.1)
                                                break
                                    else:
                                        earnings_status.write(f"âš ï¸ è·å–å¤±è´¥: {_ticker} {year} Q{quarter}")
                                        logger.warning(f"è·å–æˆ–è§£æè´¢æŠ¥å¤±è´¥ï¼Œè·³è¿‡: {_ticker} {year} Q{quarter}")
                            
                            # å¦‚æœå‘ç°æ—¥æœŸè¿‡æ—©ï¼Œåœæ­¢å¤„ç†
                            if batch_results and any(
                                result and result.get('parsed_successfully') and 
                                result.get('date') and result.get('date') < cutoff_date 
                                for result in batch_results
                            ):
                                break
                        
                        earnings_status.write(f"âœ… å®Œæˆï¼å…±è·å– {len(filtered_earnings_docs)} ä¸ªæœ‰æ•ˆçš„è´¢æŠ¥è®°å½•")
                        earnings_status.update(label="âœ… è´¢æŠ¥è®°å½•è·å–å®Œæˆ", state="complete")
                    
                    # æ¸…é™¤earningsçŠ¶æ€æ˜¾ç¤º
                    earnings_status_placeholder.empty()
                    
                    all_docs.extend(filtered_earnings_docs)
                    status.add_status_message(f"âœ… æˆåŠŸç­›é€‰å‡º {len(filtered_earnings_docs)} ä»½ç›¸å…³è´¢æŠ¥")
                    analyzer.session_manager.update_processing_status(status)

                status.add_status_message("ğŸ“Š æ­£åœ¨æ•´ç†æ–‡æª”åˆ—è¡¨...")
                analyzer.session_manager.update_processing_status(status)
                
                all_docs.sort(key=lambda x: x.date, reverse=True)
                
                # æ²™ç‰¹äº¤æ˜“æ‰€æ–‡æ¡£å¹¶å‘ä¸‹è½½å†…å®¹
                saudi_docs = [doc for doc in all_docs if doc.type == 'Saudi Exchange Filing']
                if saudi_docs:
                    status.add_status_message(f"ğŸš€ é¢„ä¸‹è½½ {len(saudi_docs)} ä¸ªæ²™ç‰¹äº¤æ˜“æ‰€æ–‡æ¡£...")
                    analyzer.session_manager.update_processing_status(status)
                    logger.info(f"ğŸš€ [SAUDI-BATCH] å¼€å§‹é¢„ä¸‹è½½ {len(saudi_docs)} ä¸ªæ²™ç‰¹æ–‡æ¡£")
                    
                    # åˆ›å»ºæ²™ç‰¹ä¸‹è½½çŠ¶æ€æ˜¾ç¤º
                    saudi_download_placeholder = st.empty()
                    
                    def saudi_status_callback_cn(message, completed, total):
                        current_language = st.session_state.get("selected_language", "English")
                        with saudi_download_placeholder.container():
                            if current_language == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©":
                                st.info(f"ğŸ‡¸ğŸ‡¦ **Ø­Ø§Ù„Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø³ÙˆÙ‚ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©**")
                                st.progress(completed / total if total > 0 else 0)
                                st.write(f"{message}")
                                st.write(f"Ø§Ù„ØªÙ‚Ø¯Ù…: {completed}/{total}")
                            else:
                                st.info(f"ğŸ‡¸ğŸ‡¦ **Saudi Exchange Download Status**")
                                st.progress(completed / total if total > 0 else 0)
                                st.write(f"{message}")
                                st.write(f"Progress: {completed}/{total}")
                    
                    # ä½¿ç”¨æ‰¹é‡ä¸‹è½½
                    all_docs = analyzer.saudi_service.download_saudi_filings_batch(
                        all_docs, 
                        max_workers=5, 
                        status_callback=saudi_status_callback_cn
                    )
                    
                    # æ¸…é™¤ä¸‹è½½çŠ¶æ€æ˜¾ç¤º
                    saudi_download_placeholder.empty()
                    
                    status.add_status_message(f"âœ… æ²™ç‰¹äº¤æ˜“æ‰€æ–‡æ¡£é¢„ä¸‹è½½å®Œæˆ")
                    analyzer.session_manager.update_processing_status(status)
                
                status.documents = all_docs
                status.update_progress(0, len(all_docs), "AI is thinking...")
                status.add_status_message(f"âœ… æ–‡æ¡£åˆ—è¡¨å‡†å¤‡å°±ç»ªï¼Œå…± {len(all_docs)} ä»½")
                
                # ç¼“å­˜å¯¹è¯æ•°æ® - ä¾›åç»­å¯¹è¯å¤ç”¨
                analyzer.cache_manager.set(conversation_cache_key, all_docs)
                logger.info(f"ğŸ’¾ [CACHE] å·²ç¼“å­˜å¯¹è¯æ•°æ®: {conversation_cache_key}, å…±{len(all_docs)}ä¸ªæ–‡æ¡£")
                
                # è°ƒè¯•æ—¥å¿—ï¼šæ˜¾ç¤ºæ–‡æ¡£ç±»å‹åˆ†å¸ƒ
                doc_types = {}
                for doc in all_docs:
                    doc_type = doc.type
                    doc_types[doc_type] = doc_types.get(doc_type, 0) + 1
                logger.info(f"ğŸ“Š [DEBUG-CN] æ–‡æ¡£ç±»å‹åˆ†å¸ƒ: {doc_types}")
                logger.info(f"ğŸ“Š [DEBUG-CN] è®¾ç½®status.documentsï¼Œæ€»æ•°: {len(all_docs)}")
                
                # æ ¹æ®åˆ†ææ¨¡å¼é€‰æ‹©ä¸åŒçš„å¤„ç†æ­¥éª¤
                if analysis_mode == "fast_mode":
                    status.processing_step = 5  # è·³è½¬åˆ°å¿«é€Ÿæ¨¡å¼å¤„ç†
                else:
                    status.processing_step = 3  # è¯¦ç»†æ¨¡å¼ï¼šé€ä¸ªå¤„ç†æ–‡æ¡£
                analyzer.session_manager.update_processing_status(status)

            st.rerun()

        # æ­¥éª¤3ï¼šæŒ‰æ—¥æœŸé¡ºåºå¤„ç†æ–‡æ¡£
        elif status.processing_step == 3:
            if status.stop_requested:
                return
                
            docs_to_process = status.documents
            
            # åˆå§‹åŒ–å¤„ç†çŠ¶æ€
            if status.completed_documents == 0:
                status.document_results = []
            
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ–‡æ¡£éœ€è¦å¤„ç†
            if status.completed_documents < len(docs_to_process):
                current_doc = docs_to_process[status.completed_documents]
                
                # æ›´æ–°çŠ¶æ€
                analyzing_msg = f"æ­£åœ¨åˆ†æ: {current_doc.title}" if language == "ä¸­æ–‡" else f"Analyzing: {current_doc.title}"
                status.add_status_message(analyzing_msg)
                
                progress_label = f"ğŸ“– åˆ†ææ–‡æ¡£ä¸­... {status.completed_documents + 1}/{len(docs_to_process)}" if language == "ä¸­æ–‡" else f"ğŸ“– Analyzing document {status.completed_documents + 1}/{len(docs_to_process)}"
                status.update_progress(status.completed_documents, len(docs_to_process), progress_label)
                analyzer.session_manager.update_processing_status(status)
                
                try:
                    # ç‰¹æ®Šå¤„ç†6-Kæ–‡ä»¶
                    if hasattr(current_doc, 'form_type') and current_doc.form_type == '6-K':
                        sixk_msg = f"æ£€æµ‹åˆ°6-Kæ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†é™„ä»¶" if language == "ä¸­æ–‡" else f"Detected 6-K file, starting to process attachments"
                        status.add_status_message(sixk_msg)
                        
                        # åˆå§‹åŒ–6-Kå¤„ç†å™¨
                        analyzer.sec_service._init_sixk_processor(analyzer.document_manager.temp_dir)
                        
                        # ä»URLä¸­æå–tickerå’Œcik
                        ticker = st.session_state.analyzer_ticker
                        
                        # è·å–CIK
                        ticker_map = analyzer.sec_service.get_cik_map()
                        cik = ticker_map.get(ticker.upper(), '')
                        
                        logger.info(f"ğŸ” [6K-MAIN] å‡†å¤‡å¤„ç†6-Kæ–‡ä»¶: {current_doc.title}")
                        logger.info(f"ğŸ” [6K-MAIN] Ticker: {ticker}, CIK: {cik}")
                        logger.info(f"ğŸ” [6K-MAIN] 6-K URL: {current_doc.url}")
                        
                        downloading_msg = f"æ­£åœ¨ä¸‹è½½å’Œå¤„ç†6-Ké™„ä»¶..." if language == "ä¸­æ–‡" else f"Downloading and processing 6-K attachments..."
                        status.add_status_message(downloading_msg)
                        analyzer.session_manager.update_processing_status(status)
                        
                        # å¤„ç†6-Kæ–‡ä»¶
                        logger.info(f"ğŸ” [6K-MAIN] å¼€å§‹è°ƒç”¨SixKProcessor.process_6k_filing...")
                        processed_docs = analyzer.sec_service.sixk_processor.process_6k_filing(
                            ticker, cik, current_doc.url, current_doc
                        )
                        logger.info(f"ğŸ” [6K-MAIN] SixKProcessor.process_6k_filingè¿”å›ç»“æœ: {len(processed_docs)} ä¸ªæ–‡æ¡£")
                        
                        if processed_docs:
                            for i, doc in enumerate(processed_docs):
                                logger.info(f"ğŸ” [6K-MAIN] è¿”å›æ–‡æ¡£{i+1}: {doc.title}, å†…å®¹é•¿åº¦: {len(doc.content) if doc.content else 0}")
                        else:
                            logger.info(f"â„¹ï¸ [6K-MAIN] SixKProcessoræ²¡æœ‰è¿”å›ä»»ä½•æ–‡æ¡£ï¼Œè¿™æ„å‘³ç€æ²¡æœ‰æ‰¾åˆ°ex99é™„ä»¶")
                        
                        completed_msg = f"6-Kå¤„ç†å®Œæˆï¼Œç”Ÿæˆäº† {len(processed_docs)} ä¸ªåˆ†ææ–‡æ¡£" if language == "ä¸­æ–‡" else f"6-K processing completed, generated {len(processed_docs)} analysis documents"
                        status.add_status_message(completed_msg)
                        
                        # æ£€æŸ¥æ˜¯å¦éœ€è¦å¯¹6-Kæ–‡ä»¶è¿›è¡Œåˆ†ç±»è¿‡æ»¤
                        should_filter_6k = (st.session_state.analyzer_use_sec_reports and 
                                           not st.session_state.analyzer_use_sec_others)
                        
                        # å¤„ç†æ‰€æœ‰6-Kç›¸å…³æ–‡æ¡£
                        for i, doc in enumerate(processed_docs):
                            if status.stop_requested:
                                break
                                
                            analyzing_6k_msg = f"æ­£åœ¨åˆ†æç¬¬ {i+1}/{len(processed_docs)} ä¸ª6-Kæ–‡æ¡£: {doc.title}" if language == "ä¸­æ–‡" else f"Analyzing {i+1}/{len(processed_docs)} 6-K document: {doc.title}"
                            status.add_status_message(analyzing_6k_msg)
                            analyzer.session_manager.update_processing_status(status)
                            
                            # å¦‚æœéœ€è¦è¿‡æ»¤6-Kæ–‡ä»¶ï¼Œå…ˆç”¨ä¾¿å®œæ¨¡å‹è¿›è¡Œåˆ†ç±»
                            if should_filter_6k:
                                classifying_msg = f"æ­£åœ¨åˆ†ç±»6-Kæ–‡æ¡£..." if language == "ä¸­æ–‡" else f"Classifying 6-K document..."
                                status.add_status_message(classifying_msg)
                                
                                # ç¡®ä¿æ–‡æ¡£æœ‰å†…å®¹
                                if not doc.content:
                                    if doc.type == 'SEC Filing':
                                        doc.content = analyzer.sec_service.download_filing(doc.url)
                                
                                # ä½¿ç”¨ä¾¿å®œæ¨¡å‹è¿›è¡Œåˆ†ç±»
                                is_quarterly_annual_ipo = analyzer.gemini_service.classify_6k_document(doc.content)
                                
                                if not is_quarterly_annual_ipo:
                                    # å¦‚æœä¸æ˜¯å­£æŠ¥/å¹´æŠ¥/IPOï¼Œè·³è¿‡è¿™ä¸ªæ–‡æ¡£
                                    skip_msg = f"è·³è¿‡éå­£æŠ¥/å¹´æŠ¥/IPOçš„6-Kæ–‡æ¡£: {doc.title}" if language == "ä¸­æ–‡" else f"Skipping non-quarterly/annual/IPO 6-K document: {doc.title}"
                                    status.add_status_message(skip_msg)
                                    continue
                                else:
                                    # å¦‚æœæ˜¯å­£æŠ¥/å¹´æŠ¥/IPOï¼Œç»§ç»­å¤„ç†
                                    continue_msg = f"æ£€æµ‹åˆ°å­£æŠ¥/å¹´æŠ¥/IPOæ–‡æ¡£ï¼Œç»§ç»­åˆ†æ: {doc.title}" if language == "ä¸­æ–‡" else f"Detected quarterly/annual/IPO document, continuing analysis: {doc.title}"
                                    status.add_status_message(continue_msg)
                            
                            # åˆ›å»ºAIåˆ†æçŠ¶æ€æ˜¾ç¤º
                            ai_status_placeholder = st.empty()
                            with ai_status_placeholder.status(f"ğŸ¤– AIæ­£åœ¨åˆ†æ6-Kæ–‡æ¡£ {i+1}/{len(processed_docs)}...", expanded=True) as ai_status:
                                ai_status.write(f"ğŸ“„ æ­£åœ¨åˆ†æ: {doc.title}")
                                ai_status.write("ğŸ“ æ­£åœ¨æ„å»ºåˆ†ææç¤ºè¯...")
                                ai_status.write("ğŸ§  æ­£åœ¨è°ƒç”¨AIæ¨¡å‹è¿›è¡Œæ·±åº¦åˆ†æ...")
                                ai_status.write("â³ å¼€å§‹æµå¼å“åº”...")
                                
                                # æ‰§è¡Œå®é™…çš„AIåˆ†æ - ä½¿ç”¨æµå¼å“åº”
                                stream_generator = analyzer.process_document_stream(doc, status.processing_prompt, model_type)
                                
                                ai_status.write("âœ… AIåˆ†æå¼€å§‹ï¼")
                                ai_status.update(label=f"âœ… 6-Kæ–‡æ¡£ {i+1}/{len(processed_docs)} åˆ†æå¼€å§‹", state="complete")
                            
                            # æ¸…é™¤AIçŠ¶æ€æ˜¾ç¤º
                            ai_status_placeholder.empty()
                            
                            # æ˜¾ç¤ºæ–‡æ¡£æ ‡é¢˜
                            st.markdown(f"### ğŸ“… {doc.date}")
                            st.markdown(f"### {doc.title}")
                            
                            # ä½¿ç”¨æµå¼å“åº”æ˜¾ç¤ºç»“æœ
                            analysis_result = st.write_stream(stream_generator)
                            
                            # æ ¹æ®æ–‡æ¡£ç±»å‹è®¾ç½®å¤´åƒ
                            avatar = "ğŸ“„"
                            
                            # ä¿å­˜æ–‡æ¡£å†…å®¹åˆ°ä¸´æ—¶æ–‡ä»¶
                            temp_file_path = analyzer.document_manager.save_document_content(doc)
                            
                            # å°†åˆ†æç»“æœæ·»åŠ åˆ°èŠå¤©å†å²ä¸­ï¼Œè¿™æ ·rerunæ—¶ä¸ä¼šä¸¢å¤±
                            message_content = f"### ğŸ“… {doc.date}\n### {doc.title}\n\n{analysis_result}"
                            st.session_state.analyzer_messages.append({
                                "role": "assistant",
                                "content": message_content,
                                "avatar": avatar,
                                "temp_file_path": temp_file_path,
                                "document_title": doc.title
                            })
                            
                            # ä¿å­˜ç»“æœ
                            status.document_results.append({
                                "title": doc.title,
                                "date": doc.date.isoformat(),
                                "analysis": analysis_result
                            })
                            
                            completed_6k_msg = f"å®Œæˆç¬¬ {i+1} ä¸ª6-Kæ–‡æ¡£åˆ†æ" if language == "ä¸­æ–‡" else f"Completed {i+1} 6-K document analysis"
                            status.add_status_message(completed_6k_msg)
                    else:
                        # æ™®é€šæ–‡æ¡£å¤„ç†
                        # åˆ›å»ºAIåˆ†æçŠ¶æ€æ˜¾ç¤º
                        ai_status_placeholder = st.empty()
                        with ai_status_placeholder.status("ğŸ¤– AIæ­£åœ¨åˆ†ææ–‡æ¡£å†…å®¹...", expanded=True) as ai_status:
                            # æ˜¾ç¤ºè¯¦ç»†çš„AIåˆ†ææ­¥éª¤
                            ai_status.write("ğŸ“„ æ­£åœ¨å‡†å¤‡æ–‡æ¡£å†…å®¹...")
                            
                            # æ£€æŸ¥æ–‡æ¡£å†…å®¹æ˜¯å¦éœ€è¦ä¸‹è½½
                            if not current_doc.content:
                                ai_status.write("ğŸ“¥ æ­£åœ¨ä¸‹è½½æ–‡æ¡£å†…å®¹...")
                                if current_doc.type == 'SEC Filing':
                                    if hasattr(current_doc, 'form_type') and current_doc.form_type == '6-K':
                                        ai_status.write("âš ï¸ 6-Kæ–‡ä»¶å†…å®¹å¤„ç†å¤±è´¥")
                                    else:
                                        ai_status.write("ğŸ”— æ­£åœ¨ä»SEC EDGARä¸‹è½½æ–‡æ¡£...")
                                elif current_doc.type == 'HK Stock Filing':
                                    ai_status.write("ğŸ”— æ­£åœ¨ä»æ¸¯äº¤æ‰€ä¸‹è½½æ–‡æ¡£...")
                                elif current_doc.type == 'Earnings Call':
                                    ai_status.write("ğŸ”— æ­£åœ¨è·å–è´¢æŠ¥ä¼šè®®è®°å½•...")
                            
                            ai_status.write("ğŸ“ æ­£åœ¨æ„å»ºåˆ†ææç¤ºè¯...")
                            ai_status.write("ğŸ§  æ­£åœ¨è°ƒç”¨AIæ¨¡å‹è¿›è¡Œæ·±åº¦åˆ†æ...")
                            ai_status.write("â³ å¼€å§‹æµå¼å“åº”...")
                            
                            # æ‰§è¡Œå®é™…çš„AIåˆ†æ - ä½¿ç”¨æµå¼å“åº”
                            stream_generator = analyzer.process_document_stream(current_doc, status.processing_prompt, model_type)
                            
                            ai_status.write("âœ… AIåˆ†æå¼€å§‹ï¼")
                            ai_status.update(label="âœ… AIåˆ†æå¼€å§‹", state="complete")
                        
                        # æ¸…é™¤AIçŠ¶æ€æ˜¾ç¤º
                        ai_status_placeholder.empty()
                        
                        # æ˜¾ç¤ºæ–‡æ¡£æ ‡é¢˜
                        st.markdown(f"### ğŸ“… {current_doc.date}")
                        st.markdown(f"### {current_doc.title}")
                        
                        # ä½¿ç”¨æµå¼å“åº”æ˜¾ç¤ºç»“æœ
                        analysis_result = st.write_stream(stream_generator)
                        
                        # æ ¹æ®æ–‡æ¡£ç±»å‹è®¾ç½®å¤´åƒ
                        if current_doc.type == 'SEC Filing':
                            avatar = "ğŸ“„"
                        elif current_doc.type == 'HK Stock Filing':
                            avatar = "ğŸ¢"
                        elif current_doc.type == 'Earnings Call':
                            avatar = "ğŸ™ï¸"
                        else:
                            avatar = "ğŸ“„"
                        
                        # ä¿å­˜æ–‡æ¡£å†…å®¹åˆ°ä¸´æ—¶æ–‡ä»¶
                        temp_file_path = analyzer.document_manager.save_document_content(current_doc)
                        
                        # å°†åˆ†æç»“æœæ·»åŠ åˆ°èŠå¤©å†å²ä¸­ï¼Œè¿™æ ·rerunæ—¶ä¸ä¼šä¸¢å¤±
                        message_content = f"### ğŸ“… {current_doc.date}\n### {current_doc.title}\n\n{analysis_result}"
                        st.session_state.analyzer_messages.append({
                            "role": "assistant",
                            "content": message_content,
                            "avatar": avatar,
                            "temp_file_path": temp_file_path,
                            "document_title": current_doc.title
                        })
                        
                        # ä¿å­˜ç»“æœ
                        status.document_results.append({
                            "title": current_doc.title,
                            "date": current_doc.date.isoformat(),
                            "analysis": analysis_result
                        })
                    
                    status.completed_documents += 1
                    
                    # æ›´æ–°çŠ¶æ€
                    analyzer.session_manager.update_processing_status(status)
                    
                    # å¦‚æœè¿˜æœ‰æ›´å¤šæ–‡æ¡£éœ€è¦å¤„ç†ï¼Œç»§ç»­ä¸‹ä¸€ä¸ª
                    if status.completed_documents < len(docs_to_process) and not status.stop_requested:
                        st.rerun()
                    else:
                        # æ‰€æœ‰æ–‡æ¡£å¤„ç†å®Œæˆï¼Œè¿›å…¥ä¸‹ä¸€æ­¥
                        status.processing_step = 4
                        analyzer.session_manager.update_processing_status(status)
                        st.rerun()
                    
                except Exception as exc:
                    failed_msg = f"åˆ†æå¤±è´¥: {current_doc.title} - {exc}" if language == "ä¸­æ–‡" else f"Analysis failed: {current_doc.title} - {exc}"
                    status.add_status_message(failed_msg)
                    logger.error(f"æ–‡æ¡£åˆ†æå¤±è´¥: {current_doc.title} - {exc}")
                    
                    # ä¹Ÿå°†é”™è¯¯ä¿¡æ¯æ·»åŠ åˆ°èŠå¤©å†å²ä¸­
                    error_prefix = f"**âš ï¸ {current_doc.title} åˆ†æå¤±è´¥:**" if language == "ä¸­æ–‡" else f"**âš ï¸ {current_doc.title} Analysis Failed:**"
                    error_message = f"{error_prefix}\n\n{exc}"
                    st.session_state.analyzer_messages.append({
                        "role": "assistant", 
                        "content": error_message,
                        "avatar": "âš ï¸"
                    })
                    
                    # è·³è¿‡å¤±è´¥çš„æ–‡æ¡£ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª
                    status.completed_documents += 1
                    analyzer.session_manager.update_processing_status(status)
                    st.rerun()
            else:
                # æ‰€æœ‰æ–‡æ¡£å¤„ç†å®Œæˆ
                all_completed_msg = "âœ… æ‰€æœ‰æ–‡æ¡£åˆ†æå®Œæˆ" if language == "ä¸­æ–‡" else "âœ… All document analysis completed"
                status.current_status_label = all_completed_msg
                status.processing_step = 4
                analyzer.session_manager.update_processing_status(status)
                st.rerun()
        
        # æ­¥éª¤4ï¼šæ•´åˆç»“æœ
        elif status.processing_step == 4:
            if status.stop_requested:
                return
                
            generating_msg = "ğŸ“Š æ­£åœ¨ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š..." if language == "ä¸­æ–‡" else "ğŸ“Š Generating final report..."
            status.current_status_label = generating_msg
            
            integrating_msg = "æ•´åˆæ‰€æœ‰åˆ†æç»“æœ..." if language == "ä¸­æ–‡" else "Integrating all analysis results..."
            status.add_status_message(integrating_msg)
            analyzer.session_manager.update_processing_status(status)
            
            # è¿‡æ»¤æ‰å¤±è´¥çš„ç»“æœ
            successful_results = [res for res in status.document_results if res is not None]
            
            # æ˜¾ç¤ºç»¼åˆæŠ¥å‘Šæ ‡é¢˜
            st.markdown("### ğŸ“Š Summary")
            
            # ä½¿ç”¨æµå¼å“åº”æ˜¾ç¤ºæœ€ç»ˆæŠ¥å‘Š
            final_report_stream = analyzer.integrate_results_stream(
                successful_results, status.integration_prompt, status.user_question, ticker, model_type
            )
            final_report = st.write_stream(final_report_stream)
            
            # å°†ç»¼åˆæŠ¥å‘Šæ·»åŠ åˆ°èŠå¤©å†å²ä¸­
            summary_content = f"### ğŸ“Š Summary\n\n{final_report}"
            st.session_state.analyzer_messages.append({
                "role": "assistant",
                "content": summary_content,
                "avatar": "ğŸ“Š"
            })
            
            report_completed_msg = "ç»¼åˆæŠ¥å‘Šç”Ÿæˆå®Œæ¯•ï¼" if language == "ä¸­æ–‡" else "Comprehensive report generated!"
            status.add_status_message(report_completed_msg)
            
            processing_completed_msg = "âœ… å¤„ç†å®Œæˆï¼" if language == "ä¸­æ–‡" else "âœ… Processing completed!"
            status.current_status_label = processing_completed_msg
            status.progress_percentage = 100.0
            analyzer.session_manager.update_processing_status(status)
            
            # çŸ­æš‚æ˜¾ç¤ºå®ŒæˆçŠ¶æ€
            time.sleep(0.1)
            
            # é‡ç½®çŠ¶æ€
            status = ProcessingStatus()
            analyzer.session_manager.update_processing_status(status)

            st.rerun()
        
        # æ­¥éª¤5ï¼šå¿«é€Ÿæ¨¡å¼å¤„ç†
        elif status.processing_step == 5:
            if status.stop_requested:
                return
                
            generating_msg = "âš¡ æ­£åœ¨å¿«é€Ÿåˆ†ææ‰€æœ‰æ–‡æ¡£..." if language == "ä¸­æ–‡" else "âš¡ Fast analyzing all documents..."
            status.current_status_label = generating_msg
            status.add_status_message(generating_msg)
            analyzer.session_manager.update_processing_status(status)
            
            # ç”Ÿæˆå¿«é€Ÿæ¨¡å¼æç¤ºè¯
            fast_mode_prompt = analyzer.generate_fast_mode_prompt(status.user_question, ticker, model_type)
            
            # åˆ›å»ºAIåˆ†æçŠ¶æ€æ˜¾ç¤º
            ai_status_placeholder = st.empty()
            with ai_status_placeholder.status("âš¡ å¿«é€Ÿæ¨¡å¼AIåˆ†æä¸­...", expanded=True) as ai_status:
                ai_status.write(f"ğŸ“Š æ­£åœ¨åŒæ—¶åˆ†æ {len(status.documents)} ä¸ªæ–‡æ¡£")
                ai_status.write("ğŸ§  æ­£åœ¨è°ƒç”¨AIæ¨¡å‹è¿›è¡Œç»¼åˆåˆ†æ...")
                
                # æ‰§è¡Œå¿«é€Ÿæ¨¡å¼åˆ†æ
                fast_stream = analyzer.process_all_documents_fast(status.documents, fast_mode_prompt, model_type)
                
                ai_status.write("âœ… å¿«é€Ÿåˆ†æå‡†å¤‡å®Œæˆï¼")
                ai_status.update(label="âœ… å¿«é€Ÿåˆ†æå‡†å¤‡å®Œæˆ", state="complete")
            
            # æ¸…é™¤AIçŠ¶æ€æ˜¾ç¤º
            ai_status_placeholder.empty()
            
            # æ˜¾ç¤ºå¿«é€Ÿåˆ†æç»“æœ
            st.markdown("### âš¡ å¿«é€Ÿåˆ†æç»“æœ" if language == "ä¸­æ–‡" else "### âš¡ Fast Analysis Results")
            
            # æ·»åŠ AIæ€è€ƒçŠ¶æ€
            thinking_placeholder = st.empty()
            if language == "English":
                thinking_msg = "ğŸ¤– AI is thinking now..."
            elif language == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©":
                thinking_msg = "ğŸ¤– Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙŠÙÙƒØ± Ø§Ù„Ø¢Ù†..."
            else:
                thinking_msg = "ğŸ¤– AIæ­£åœ¨æ€è€ƒä¸­..."
            thinking_placeholder.info(thinking_msg)
            
            # ä½¿ç”¨æµå¼å“åº”æ˜¾ç¤ºç»“æœ
            final_result = st.write_stream(fast_stream)
            
            # æ¸…é™¤æ€è€ƒçŠ¶æ€
            thinking_placeholder.empty()
            
            # å°†ç»“æœæ·»åŠ åˆ°èŠå¤©å†å²ä¸­
            result_content = f"### âš¡ {'å¿«é€Ÿåˆ†æç»“æœ' if language == 'ä¸­æ–‡' else 'Fast Analysis Results'}\n\n{final_result}"
            st.session_state.analyzer_messages.append({
                "role": "assistant",
                "content": result_content,
                "avatar": "âš¡"
            })
            
            # å®Œæˆå¤„ç†
            completed_msg = "å¿«é€Ÿåˆ†æå®Œæˆï¼" if language == "ä¸­æ–‡" else "Fast analysis completed!"
            status.add_status_message(completed_msg)
            
            processing_completed_msg = "âœ… å¤„ç†å®Œæˆï¼" if language == "ä¸­æ–‡" else "âœ… Processing completed!"
            status.current_status_label = processing_completed_msg
            status.progress_percentage = 100.0
            analyzer.session_manager.update_processing_status(status)
            
            # çŸ­æš‚æ˜¾ç¤ºå®ŒæˆçŠ¶æ€
            time.sleep(0.1)
            
            # é‡ç½®çŠ¶æ€
            status = ProcessingStatus()
            analyzer.session_manager.update_processing_status(status)
            
            st.rerun()

    except Exception as e:
        logger.error(f"å¤„ç†æµç¨‹å‡ºé”™: {e}", exc_info=True)
        error_msg = f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°ä¸¥é‡é”™è¯¯: {e}" if language == "ä¸­æ–‡" else f"A serious error occurred during processing: {e}"
        st.error(error_msg)
        # é‡ç½®çŠ¶æ€
        status = ProcessingStatus()
        analyzer.session_manager.update_processing_status(status)
        st.rerun()


def process_user_question(analyzer: SECEarningsAnalyzer, ticker: str, years: int, use_sec: bool, use_earnings: bool, model_type: str):
    """å¤„ç†ç”¨æˆ·é—®é¢˜çš„å®Œæ•´æµç¨‹"""
    # DEPRECATED: use process_user_question_new instead
    pass

if __name__ == "__main__":
    main() 