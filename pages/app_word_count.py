"""
SEC & è´¢æŠ¥ä¼šè®®è®°å½•åˆ†æå¸ˆ - Productionçº§åˆ«
ä¸“ä¸šçš„è´¢åŠ¡æ•°æ®ç»¼åˆåˆ†æå¹³å°

Features:
- SECæ–‡ä»¶è·å–å’Œåˆ†æ
- è´¢æŠ¥ä¼šè®®è®°å½•è·å–å’Œåˆ†æ
- AIé©±åŠ¨çš„æ–‡æ¡£å¤„ç†
- å®æ—¶è¿›åº¦è·Ÿè¸ª
- é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
- ç¼“å­˜ä¼˜åŒ–
"""

import streamlit as st
import os
import json
import requests
import warnings
import logging
import time
import hashlib
import tempfile
import uuid
import fitz  # PyMuPDF for PDF processing
import shutil
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass, field, fields
from functools import lru_cache
from contextlib import contextmanager
import re
import html
from urllib.parse import urljoin
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import pandas as pd

# é…ç½®é€‰é¡¹ï¼šæ˜¯å¦ä¿å­˜transcriptæ–‡ä»¶åˆ°ç£ç›˜
SAVE_TRANSCRIPT_FILES = os.getenv("SAVE_TRANSCRIPT_FILES", "false").lower() == "true"

# ç¬¬ä¸‰æ–¹åº“
from sec_edgar_api import EdgarClient
from bs4 import BeautifulSoup, XMLParsedAsHTMLWarning
import httpx
from google import genai
from google.genai import types
from itertools import cycle

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="SEC & è´¢æŠ¥ä¼šè®®è®°å½•åˆ†æå¸ˆ",
    page_icon="ğŸ“Š",
    layout="wide"
)

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# è¯­è¨€é…ç½®
LANGUAGE_CONFIG = {
    "English": {
        "title": "ğŸ“Š Financial Disclosure & Earnings Insights",
        "sidebar_header": "ğŸ“‹ Configuration",
        "ticker_label": "Ticker",
        "ticker_placeholder": "e.g., AAPL, 1024 HK",
        "years_label": "Years of Data",
        "data_type_header": "ğŸ“„ Data Type",
        "sec_reports_us": "Quarterly & Annual (10-K, 10-Q, 20-F, 6-K, 424B4)",
        "sec_others_us": "Others (8-K, S-8, DEF 14A, F-3)",
        "sec_reports_hk": "Quarterly & Annual Results",
        "sec_others_hk": "Other Announcements",
        "earnings_label": "Earnings Call Transcripts",
        "earnings_caption": "Earnings call transcripts",
        "model_header": "ğŸ¤– AI Model",
        "model_label": "Select Model",
        "api_header": "ğŸ’³ API Configuration",
        "access_code_label": "Enter Access Code",
        "access_code_placeholder": "Enter access code to enable premium API",
        "premium_enabled": "âœ… Premium API Service Enabled",
        "free_api": "â„¹ï¸ Using Free API Service",
        "access_code_error": "âŒ Invalid Access Code",
        "premium_success": "ğŸ‰ Premium API Service Enabled!",
        "language_header": "ğŸŒ Language",
        "language_label": "Select Language",
        "hk_stock_info": "ğŸ¢ Hong Kong Stock - Standardized to: {}",
        "us_stock_info": "ğŸ‡ºğŸ‡¸ US Stock",
        "chat_placeholder": "Please enter your question...",
        "status_header": "ğŸ“‹ STATUS",
        "stop_button": "â¹ï¸ Stop Processing",
        "progress_text": "Progress: {}/{} documents",
        "stop_success": "â¹ï¸ Processing stopped by user",
        "processing_stopped": "Processing has been stopped by user request."
    },
    "ä¸­æ–‡": {
        "title": "ğŸ“Š Financial Disclosure & Earnings Insights",
        "sidebar_header": "ğŸ“‹ Configuration",
        "ticker_label": "Ticker",
        "ticker_placeholder": "e.g., AAPL, 1024 HK",
        "years_label": "Years of Data",
        "data_type_header": "ğŸ“„ Data Type",
        "sec_reports_us": "Quarterly & Annual (10-K, 10-Q, 20-F, 6-K, 424B4)",
        "sec_others_us": "Others (8-K, S-8, DEF 14A, F-3)",
        "sec_reports_hk": "Quarterly & Annual Results",
        "sec_others_hk": "Other Announcements",
        "earnings_label": "Earnings Call Transcripts",
        "earnings_caption": "Earnings call transcripts",
        "model_header": "ğŸ¤– AI Model",
        "model_label": "Select Model",
        "api_header": "ğŸ’³ API Configuration",
        "access_code_label": "Enter Access Code",
        "access_code_placeholder": "Enter access code to enable premium API",
        "premium_enabled": "âœ… Premium API Service Enabled",
        "free_api": "â„¹ï¸ Using Free API Service",
        "access_code_error": "âŒ Invalid Access Code",
        "premium_success": "ğŸ‰ Premium API Service Enabled!",
        "language_header": "ğŸŒ Language",
        "language_label": "Select Language",
        "hk_stock_info": "ğŸ¢ æ¸¯è‚¡ - å·²æ ‡å‡†åŒ–ä¸º: {}",
        "us_stock_info": "ğŸ‡ºğŸ‡¸ ç¾è‚¡",
        "chat_placeholder": "è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...",
        "status_header": "ğŸ“‹ STATUS",
        "stop_button": "â¹ï¸ åœæ­¢å¤„ç†",
        "progress_text": "è¿›åº¦: {}/{} ä¸ªæ–‡æ¡£",
        "stop_success": "â¹ï¸ ç”¨æˆ·å·²åœæ­¢å¤„ç†",
        "processing_stopped": "å¤„ç†å·²è¢«ç”¨æˆ·åœæ­¢ã€‚"
    }
}

# å¸¸é‡é…ç½®
@dataclass
class Config:
    """åº”ç”¨é…ç½®"""
    # æ¨¡å‹é…ç½®
    MODELS: Dict[str, str] = field(default_factory=lambda: {
        "gemini-2.5-flash": "Gemini 2.5 Flash",
        "gemini-2.5-pro": "Gemini 2.5 Pro"
    })
    
    # SECé…ç½®
    SEC_USER_AGENT: str = "SEC Earnings Analyzer <analysis@example.com>"
    SEC_FORMS: List[str] = field(default_factory=lambda: [
        '10-K', '10-Q', '8-K', '20-F', '6-K', '424B4', 'DEF 14A', 'S-8'
    ])
    
    # è¯·æ±‚é…ç½®
    REQUEST_TIMEOUT: int = 30
    MAX_RETRIES: int = 3
    RETRY_DELAY: float = 1.0
    
    # å†…å®¹é™åˆ¶
    MAX_CONTENT_LENGTH: int = 900000
    
    # ç¼“å­˜é…ç½®
    CACHE_TTL: int = 3600  # 1å°æ—¶
    
    # æ—¥æœŸè§£ææ ¼å¼
    DATE_FORMATS: List[str] = field(default_factory=lambda: [
        '%B %d, %Y',    # January 1, 2023
        '%b %d, %Y',    # Jan 1, 2023
        '%Y-%m-%d',     # 2023-01-01
        '%m/%d/%Y',     # 01/01/2023
    ])
    
    # æ—¥æœŸæ¨¡å¼
    DATE_PATTERNS: List[str] = field(default_factory=lambda: [
        r'\b\w+ \d{1,2}, \d{4}\b',  # January 1, 2023
        r'\b\d{4}-\d{1,2}-\d{1,2}\b',  # 2023-01-01
        r'\b\d{1,2}/\d{1,2}/\d{4}\b',  # 01/01/2023
    ])

config = Config()

# æ•°æ®ç±»å®šä¹‰
@dataclass
class Document:
    """æ–‡æ¡£æ•°æ®ç±»"""
    type: str
    title: str
    date: datetime.date
    url: str
    content: Optional[str] = None
    form_type: Optional[str] = None
    year: Optional[int] = None
    quarter: Optional[int] = None
    temp_file_path: Optional[str] = None  # æ·»åŠ ä¸´æ—¶æ–‡ä»¶è·¯å¾„å­—æ®µ

@dataclass
class ProcessingStatus:
    """å¤„ç†çŠ¶æ€æ•°æ®ç±»"""
    is_processing: bool = False
    completed_documents: int = 0
    total_documents: int = 0
    document_results: List[Dict] = field(default_factory=list)
    processing_step: int = 0
    documents: List[Document] = field(default_factory=list)
    error_message: Optional[str] = None
    status_messages: List[str] = field(default_factory=list)
    current_status_label: str = ""
    progress_percentage: float = 0.0
    stop_requested: bool = False

    def __post_init__(self):
        # Ensure fields are initialized if not provided
        if self.processing_step is None:
            self.processing_step = 0
        if self.completed_documents is None:
            self.completed_documents = 0
        if self.total_documents is None:
            self.total_documents = 0
        if self.document_results is None:
            self.document_results = []
        if self.documents is None:
            self.documents = []
        if self.error_message is None:
            self.error_message = None
        if self.status_messages is None:
            self.status_messages = []
        if self.current_status_label is None:
            self.current_status_label = ""
        if self.progress_percentage is None:
            self.progress_percentage = 0.0
        if self.stop_requested is None:
            self.stop_requested = False
    
    def add_status_message(self, message: str):
        """æ·»åŠ çŠ¶æ€æ¶ˆæ¯"""
        self.status_messages.append(f"â±ï¸ {datetime.now().strftime('%H:%M:%S')} - {message}")
        # åªä¿ç•™æœ€è¿‘çš„20æ¡æ¶ˆæ¯
        if len(self.status_messages) > 20:
            self.status_messages = self.status_messages[-20:]
    
    def update_progress(self, completed: int, total: int, label: str = ""):
        """æ›´æ–°è¿›åº¦"""
        self.completed_documents = completed
        self.total_documents = total
        self.progress_percentage = (completed / total) * 100 if total > 0 else 0
        self.current_status_label = label

# å¼‚å¸¸ç±»å®šä¹‰
class SECAnalyzerError(Exception):
    """SECåˆ†æå™¨åŸºç¡€å¼‚å¸¸"""
    pass

class APIError(SECAnalyzerError):
    """APIè°ƒç”¨å¼‚å¸¸"""
    pass

class DataRetrievalError(SECAnalyzerError):
    """æ•°æ®è·å–å¼‚å¸¸"""
    pass

class DateParsingError(SECAnalyzerError):
    """æ—¥æœŸè§£æå¼‚å¸¸"""
    pass

# å·¥å…·ç±»
def is_hk_stock(ticker: str) -> bool:
    """æ£€æµ‹æ˜¯å¦ä¸ºæ¸¯è‚¡ä»£ç """
    if not ticker:
        return False
    
    ticker_upper = ticker.upper().strip()
    
    # æ£€æŸ¥æ˜¯å¦ä»¥.HKç»“å°¾
    if ticker_upper.endswith('.HK'):
        return True
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯ "æ•°å­— HK" æ ¼å¼
    if ' HK' in ticker_upper:
        return True
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯çº¯æ•°å­—ï¼ˆæ¸¯è‚¡ä»£ç é€šå¸¸æ˜¯æ•°å­—ï¼‰
    ticker_clean = ticker.strip()
    if ticker_clean.isdigit():
        return True
    
    return False

def normalize_hk_ticker(ticker: str) -> str:
    """æ ‡å‡†åŒ–æ¸¯è‚¡ä»£ç ä¸º XXXX.HK æ ¼å¼ï¼Œè‡ªå‹•è£œ0æˆå››ä½æ•¸"""
    if not ticker:
        return ticker
    
    ticker_clean = ticker.strip()
    
    # å¦‚æœå·²ç»æ˜¯æ ‡å‡†æ ¼å¼ï¼Œæå–æ•¸å­—éƒ¨åˆ†è™•ç†
    if ticker_clean.upper().endswith('.HK'):
        number_part = ticker_clean.upper().replace('.HK', '').strip()
        if number_part.isdigit():
            # è£œ0æˆå››ä½æ•¸
            padded_number = number_part.zfill(4)
            return f"{padded_number}.HK"
        return ticker_clean.upper()
    
    # å¤„ç† "æ•°å­— HK" æ ¼å¼
    if ' HK' in ticker_clean.upper():
        number_part = ticker_clean.upper().replace(' HK', '').strip()
        if number_part.isdigit():
            # è£œ0æˆå››ä½æ•¸
            padded_number = number_part.zfill(4)
            return f"{padded_number}.HK"
    
    # å¤„ç†çº¯æ•°å­—
    if ticker_clean.isdigit():
        # è£œ0æˆå››ä½æ•¸
        padded_number = ticker_clean.zfill(4)
        return f"{padded_number}.HK"
    
    # å…¶ä»–æƒ…å†µè¿”å›åŸå€¼
    return ticker_clean.upper()

def clean_hk_ticker(ticker: str) -> str:
    """æ¸…ç†æ¸¯è‚¡ä»£ç ï¼Œç§»é™¤.HKåç¼€ï¼Œè¿”å›çº¯æ•°å­—"""
    normalized = normalize_hk_ticker(ticker)
    return normalized.replace('.HK', '').replace('.hk', '')

class RateLimiter:
    """APIè¯·æ±‚é™æµå™¨"""
    def __init__(self, max_calls: int = 10, window: int = 60):
        self.max_calls = max_calls
        self.window = window
        self.calls = []
    
    def wait_if_needed(self):
        """å¦‚æœéœ€è¦ï¼Œç­‰å¾…ç›´åˆ°å¯ä»¥å‘å‡ºè¯·æ±‚"""
        now = time.time()
        self.calls = [call_time for call_time in self.calls if now - call_time < self.window]
        
        if len(self.calls) >= self.max_calls:
            wait_time = self.window - (now - self.calls[0])
            if wait_time > 0:
                time.sleep(wait_time)
                self.calls = []
        
        self.calls.append(now)

class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨"""
    def __init__(self):
        # Session state is now initialized by SessionManager
        pass
    
    def get_cache_key(self, *args) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_string = "|".join(str(arg) for arg in args)
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def get(self, key: str, default=None):
        """è·å–ç¼“å­˜å€¼"""
        cache_data = st.session_state.cache.get(key)
        if cache_data:
            timestamp, value = cache_data
            if time.time() - timestamp < config.CACHE_TTL:
                return value
            else:
                # ç¼“å­˜è¿‡æœŸï¼Œåˆ é™¤
                del st.session_state.cache[key]
        return default
    
    def set(self, key: str, value: Any):
        """è®¾ç½®ç¼“å­˜å€¼"""
        st.session_state.cache[key] = (time.time(), value)
    
    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        st.session_state.cache.clear()

class DocumentManager:
    """æ–‡æ¡£ç®¡ç†å™¨ï¼Œè´Ÿè´£ä¿å­˜å’Œç®¡ç†ä¸´æ—¶æ–‡ä»¶"""
    
    def __init__(self):
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        self.temp_dir = tempfile.mkdtemp(prefix="sec_analyzer_")
        logger.info(f"åˆ›å»ºä¸´æ—¶ç›®å½•: {self.temp_dir}")
    
    def save_document_content(self, document: Document) -> str:
        """ä¿å­˜æ–‡æ¡£å†…å®¹åˆ°ä¸´æ—¶æ–‡ä»¶å¹¶è¿”å›æ–‡ä»¶è·¯å¾„"""
        try:
            # ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶å
            file_id = str(uuid.uuid4())[:8]
            safe_title = re.sub(r'[^\w\-_\.]', '_', document.title)
            filename = f"{file_id}_{safe_title}.txt"
            file_path = os.path.join(self.temp_dir, filename)
            
            # ä¿å­˜å†…å®¹
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(f"æ–‡æ¡£æ ‡é¢˜: {document.title}\n")
                f.write(f"æ–‡æ¡£ç±»å‹: {document.type}\n")
                f.write(f"æ–‡æ¡£æ—¥æœŸ: {document.date}\n")
                f.write(f"åŸå§‹URL: {document.url}\n")
                f.write("="*80 + "\n\n")
                f.write(document.content or "å†…å®¹ä¸ºç©º")
            
            document.temp_file_path = file_path
            logger.info(f"æ–‡æ¡£å·²ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶: {filename}")
            return file_path
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ–‡æ¡£å†…å®¹å¤±è´¥: {e}")
            return None
    
    def get_download_content(self, file_path: str) -> Optional[bytes]:
        """è·å–æ–‡ä»¶å†…å®¹ç”¨äºä¸‹è½½"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read().encode('utf-8')
        except Exception as e:
            logger.error(f"è¯»å–æ–‡ä»¶å†…å®¹å¤±è´¥: {e}")
            return None
    
    def cleanup(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        try:
            import shutil
            shutil.rmtree(self.temp_dir)
            logger.info(f"å·²æ¸…ç†ä¸´æ—¶ç›®å½•: {self.temp_dir}")
        except Exception as e:
            logger.error(f"æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")

class SixKProcessor:
    """6-Kæ–‡ä»¶å¤„ç†å™¨"""
    
    def __init__(self, temp_dir: str):
        self.temp_dir = temp_dir
        self.headers = {"User-Agent": config.SEC_USER_AGENT}
    
    def process_6k_filing(self, ticker: str, cik: str, filing_url: str, document: Document) -> List[Document]:
        """å¤„ç†6-Kæ–‡ä»¶ï¼Œä¸‹è½½é™„ä»¶å¹¶æå–ex99æ–‡ä»¶"""
        try:
            # ä»URLä¸­æå–accession numberï¼ˆæ— ç ´æŠ˜å·æ ¼å¼ï¼‰
            # URLæ ¼å¼ï¼šhttps://www.sec.gov/Archives/edgar/data/{cik}/{accession_no_no_dashes}/{primary_doc}
            accession_match = re.search(r'/(\d{18})/', filing_url)
            if not accession_match:
                logger.error(f"æ— æ³•ä»URLä¸­æå–accession number: {filing_url}")
                return [document]  # è¿”å›åŸæ–‡æ¡£
            
            accession_no_no_dashes = accession_match.group(1)
            # é‡æ–°æ„é€ å¸¦ç ´æŠ˜å·çš„æ ¼å¼ç”¨äºæ˜¾ç¤º
            accession_no = f"{accession_no_no_dashes[:10]}-{accession_no_no_dashes[10:12]}-{accession_no_no_dashes[12:]}"
            
            # åˆ›å»º6-Kæ–‡ä»¶ä¸“ç”¨ç›®å½•
            filing_dir = os.path.join(self.temp_dir, f"6K_{ticker}_{accession_no}")
            os.makedirs(filing_dir, exist_ok=True)
            
            logger.info(f"å¼€å§‹å¤„ç†6-Kæ–‡ä»¶: {ticker} - {accession_no}")
            
            # ä¸‹è½½æ‰€æœ‰é™„ä»¶
            attachments = self._download_6k_attachments(cik, accession_no_no_dashes, filing_dir)
            
            if not attachments:
                logger.warning(f"æœªæ‰¾åˆ°6-Ké™„ä»¶: {ticker} - {accession_no}")
                return [document]
            
            # å¤„ç†ex99æ–‡ä»¶
            ex99_documents = self._process_ex99_files(attachments, filing_dir, document, ticker)
            
            if not ex99_documents:
                logger.info(f"æœªæ‰¾åˆ°ex99æ–‡ä»¶: {ticker} - {accession_no}")
                return [document]
            
            logger.info(f"æˆåŠŸå¤„ç†6-Kæ–‡ä»¶ï¼Œç”Ÿæˆ {len(ex99_documents)} ä¸ªæ–‡æ¡£")
            return ex99_documents
            
        except Exception as e:
            logger.error(f"å¤„ç†6-Kæ–‡ä»¶å¤±è´¥: {e}")
            return [document]  # è¿”å›åŸæ–‡æ¡£
    
    def _download_6k_attachments(self, cik: str, accession_no_no_dashes: str, filing_dir: str) -> List[str]:
        """ä¸‹è½½6-Kæ–‡ä»¶çš„æ‰€æœ‰é™„ä»¶ï¼Œåªä¸‹è½½pdf/htm/htmlæ–‡ä»¶"""
        base_url = f"https://www.sec.gov/Archives/edgar/data/{cik}/{accession_no_no_dashes}/"
        index_url = base_url + "index.json"
        
        try:
            response = httpx.get(index_url, headers=self.headers, timeout=config.REQUEST_TIMEOUT)
            response.raise_for_status()
            
            index_data = response.json()
            directory = index_data.get('directory', {})
            items = directory.get('item', [])
            
            if not items:
                logger.warning(f"6-Kæ–‡ä»¶æ— é™„ä»¶åˆ—è¡¨: {index_url}")
                return []
            
            # ç­›é€‰åªéœ€è¦çš„æ–‡ä»¶ç±»å‹
            target_files = []
            for item in items:
                file_name = item.get('name', '')
                if not file_name:
                    continue
                
                # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
                ext = os.path.splitext(file_name)[1].lower()
                if ext in ['.pdf', '.htm', '.html']:
                    target_files.append(item)
                else:
                    logger.info(f"è·³è¿‡éç›®æ ‡æ–‡ä»¶ç±»å‹: {file_name}")
            
            if not target_files:
                logger.warning(f"æœªæ‰¾åˆ°pdf/htm/htmlæ–‡ä»¶")
                return []
            
            downloaded_files = []
            logger.info(f"æ‰¾åˆ° {len(target_files)} ä¸ªç›®æ ‡æ–‡ä»¶ï¼Œå¼€å§‹ä¸‹è½½...")
            
            for item in target_files:
                file_name = item.get('name', '')
                file_url = base_url + file_name
                file_path = os.path.join(filing_dir, file_name)
                
                try:
                    file_response = httpx.get(file_url, headers=self.headers, timeout=config.REQUEST_TIMEOUT)
                    file_response.raise_for_status()
                    
                    # åˆ¤æ–­æ–‡ä»¶ç±»å‹å¹¶ä¿å­˜
                    content_type = file_response.headers.get('content-type', '').lower()
                    if 'text' in content_type or 'html' in content_type or 'xml' in content_type:
                        with open(file_path, 'w', encoding='utf-8') as f:
                            f.write(file_response.text)
                    else:
                        with open(file_path, 'wb') as f:
                            f.write(file_response.content)
                    
                    downloaded_files.append(file_path)
                    logger.info(f"å·²ä¸‹è½½6-Ké™„ä»¶: {file_name}")
                    
                except Exception as e:
                    logger.warning(f"ä¸‹è½½6-Ké™„ä»¶å¤±è´¥ {file_name}: {e}")
            
            logger.info(f"æˆåŠŸä¸‹è½½ {len(downloaded_files)} ä¸ª6-Kç›®æ ‡é™„ä»¶")
            return downloaded_files
            
        except Exception as e:
            logger.error(f"è·å–6-Ké™„ä»¶åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def _process_ex99_files(self, attachments: List[str], filing_dir: str, original_doc: Document, ticker: str) -> List[Document]:
        """å¤„ç†ex99æ–‡ä»¶ï¼ŒæŒ‰ç…§è¦æ±‚åˆ†ç±»å¤„ç†HTMLå’ŒPDF"""
        ex99_files = []
        
        # æ‰¾åˆ°æ‰€æœ‰åŒ…å«_ex99çš„æ–‡ä»¶
        for file_path in attachments:
            file_name = os.path.basename(file_path).lower()
            if 'ex99' in file_name:
                ex99_files.append(file_path)
        
        if not ex99_files:
            logger.info("æœªæ‰¾åˆ°åŒ…å«_ex99çš„æ–‡ä»¶")
            return []
        
        logger.info(f"æ‰¾åˆ° {len(ex99_files)} ä¸ªex99æ–‡ä»¶: {[os.path.basename(f) for f in ex99_files]}")
        
        # åˆ†ç±»å¤„ç†ä¸åŒç±»å‹çš„æ–‡ä»¶
        html_files = []
        pdf_files = []
        
        for file_path in ex99_files:
            ext = os.path.splitext(file_path)[1].lower()
            if ext in ['.html', '.htm']:
                html_files.append(file_path)
            elif ext == '.pdf':
                pdf_files.append(file_path)
            else:
                logger.info(f"è·³è¿‡ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_path}")
        
        documents = []
        
        # å¤„ç†HTML/HTMæ–‡ä»¶ - è½¬æ¢ä¸ºmarkdownåæ£€æŸ¥æ˜¯å¦å¯ä»¥åˆå¹¶
        if html_files:
            logger.info(f"å¤„ç† {len(html_files)} ä¸ªHTMLæ–‡ä»¶")
            
            # å…ˆè½¬æ¢æ‰€æœ‰HTMLæ–‡ä»¶ä¸ºmarkdown
            html_contents = []
            for file_path in html_files:
                content = self._convert_html_to_markdown(file_path)
                if content:
                    file_name = os.path.basename(file_path)
                    html_contents.append({
                        'file_name': file_name,
                        'content': content
                    })
            
            if html_contents:
                # è®¡ç®—æ€»å­—ç¬¦æ•°
                total_chars = sum(len(item['content']) for item in html_contents)
                
                if total_chars <= config.MAX_CONTENT_LENGTH:
                    # åˆå¹¶æ‰€æœ‰HTMLå†…å®¹
                    combined_content = ""
                    for item in html_contents:
                        combined_content += f"\n\n=== {item['file_name']} ===\n\n{item['content']}"
                    
                    html_doc = Document(
                        type=original_doc.type,
                        title=f"{ticker} 6-K ex99 files",
                        date=original_doc.date,
                        url=original_doc.url,
                        content=combined_content.strip(),
                        form_type="6-K-Ex99-HTML-Combined"
                    )
                    documents.append(html_doc)
                    logger.info(f"HTMLæ–‡ä»¶å·²åˆå¹¶å¤„ç†ï¼Œæ€»å­—ç¬¦æ•°: {total_chars}")
                else:
                    # åˆ†åˆ«å¤„ç†æ¯ä¸ªHTMLæ–‡ä»¶
                    for i, item in enumerate(html_contents):
                        html_doc = Document(
                            type=original_doc.type,
                            title=f"{ticker} 6-K ex99-{i+1}",
                            date=original_doc.date,
                            url=original_doc.url,
                            content=item['content'],
                            form_type="6-K-Ex99-HTML"
                        )
                        documents.append(html_doc)
                    logger.info(f"HTMLæ–‡ä»¶åˆ†åˆ«å¤„ç†ï¼Œæ€»å­—ç¬¦æ•°è¶…è¿‡é™åˆ¶: {total_chars}")
        
        # å¤„ç†PDFæ–‡ä»¶ - å¿…é¡»åˆ†å¼€å¤„ç†
        if pdf_files:
            logger.info(f"å¤„ç† {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")
            
            for i, file_path in enumerate(pdf_files):
                content = self._convert_pdf_to_text(file_path)
                if content:
                    file_name = os.path.basename(file_path)
                    pdf_doc = Document(
                        type=original_doc.type,
                        title=f"{ticker} 6-K ex99 PDF-{i+1}",
                        date=original_doc.date,
                        url=original_doc.url,
                        content=content,
                        form_type="6-K-Ex99-PDF"
                    )
                    documents.append(pdf_doc)
                    logger.info(f"PDFæ–‡ä»¶å·²å¤„ç†: {file_name}")
                else:
                    logger.warning(f"PDFæ–‡ä»¶è½¬æ¢å¤±è´¥: {file_path}")
        
        logger.info(f"Ex99æ–‡ä»¶å¤„ç†å®Œæˆï¼Œç”Ÿæˆ {len(documents)} ä¸ªæ–‡æ¡£")
        return documents
    
    def _convert_html_to_markdown(self, file_path: str) -> str:
        """å°†HTMLè½¬æ¢ä¸ºmarkdownæ ¼å¼çš„æ–‡æœ¬"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
            
            # ä½¿ç”¨BeautifulSoupè§£æHTML
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾
            for script in soup(["script", "style"]):
                script.decompose()
            
            # å¤„ç†è¡¨æ ¼ - ä¿æŒç»“æ„
            for table in soup.find_all('table'):
                # ç®€å•çš„è¡¨æ ¼è½¬æ¢
                rows = table.find_all('tr')
                table_text = "\n"
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    if cells:
                        cell_texts = [cell.get_text(strip=True) for cell in cells]
                        table_text += "| " + " | ".join(cell_texts) + " |\n"
                table_text += "\n"
                table.replace_with(table_text)
            
            # å¤„ç†æ ‡é¢˜
            for i in range(1, 7):
                for heading in soup.find_all(f'h{i}'):
                    heading_text = heading.get_text(strip=True)
                    heading.replace_with(f"\n{'#' * i} {heading_text}\n")
            
            # å¤„ç†æ®µè½
            for p in soup.find_all('p'):
                p_text = p.get_text(strip=True)
                if p_text:
                    p.replace_with(f"\n{p_text}\n")
            
            # å¤„ç†åˆ—è¡¨
            for ul in soup.find_all('ul'):
                items = ul.find_all('li')
                list_text = "\n"
                for item in items:
                    item_text = item.get_text(strip=True)
                    if item_text:
                        list_text += f"- {item_text}\n"
                list_text += "\n"
                ul.replace_with(list_text)
            
            for ol in soup.find_all('ol'):
                items = ol.find_all('li')
                list_text = "\n"
                for i, item in enumerate(items, 1):
                    item_text = item.get_text(strip=True)
                    if item_text:
                        list_text += f"{i}. {item_text}\n"
                list_text += "\n"
                ol.replace_with(list_text)
            
            # è·å–æœ€ç»ˆæ–‡æœ¬
            text = soup.get_text()
            
            # æ¸…ç†æ–‡æœ¬
            lines = text.split('\n')
            cleaned_lines = []
            for line in lines:
                line = line.strip()
                if line:
                    cleaned_lines.append(line)
            
            # åˆå¹¶è¿ç»­çš„ç©ºè¡Œ
            final_text = '\n'.join(cleaned_lines)
            
            # å»é™¤å¤šä½™çš„ç©ºè¡Œ
            final_text = re.sub(r'\n{3,}', '\n\n', final_text)
            
            return final_text.strip()
            
        except Exception as e:
            logger.error(f"è½¬æ¢HTMLæ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return ""
    
    def _convert_pdf_to_text(self, file_path: str) -> str:
        """å°†PDFè½¬æ¢ä¸ºæ–‡æœ¬"""
        try:
            doc = fitz.open(file_path)
            text = ""
            
            for page_num in range(doc.page_count):
                page = doc[page_num]
                page_text = page.get_text()
                if page_text.strip():
                    text += f"\n--- ç¬¬ {page_num + 1} é¡µ ---\n"
                    text += page_text
                    text += "\n"
            
            doc.close()
            
            # æ¸…ç†æ–‡æœ¬
            text = re.sub(r'\n{3,}', '\n\n', text)
            return text.strip()
            
        except Exception as e:
            logger.error(f"è½¬æ¢PDFæ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return ""

# æ¸¯è‚¡æ–‡ä»¶ä¸‹è½½å™¨
class HKStockFilingsDownloader:
    """æ¸¯è‚¡å…¬å‘Šä¸‹è½½å™¨"""
    def __init__(self):
        self.base_url = "https://www1.hkexnews.hk"
        self.prefix_url = f"{self.base_url}/search/prefix.do"
        self.search_url = f"{self.base_url}/search/titlesearch.xhtml"
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,zh-TW;q=0.8,zh;q=0.7,zh-CN;q=0.6,ja;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'same-origin',
            'Sec-Fetch-User': '?1',
        }
        
        # æ–‡ä»¶ç±»å‹æ˜ å°„
        self.filing_types = {
            'Annual Report': 'å¹´æŠ¥',
            'Quarterly Results': 'å­£æŠ¥',
            'Interim Results': 'ä¸­æœŸæŠ¥å‘Š',
            'Final Results': 'å…¨å¹´ä¸šç»©',
            'Financial Statements': 'è´¢åŠ¡æŠ¥è¡¨'
        }
        
    def get_stock_id(self, ticker):
        """æ ¹æ®è‚¡ç¥¨ä»£ç è·å–è‚¡ç¥¨ID"""
        # æ¸…ç†è‚¡ç¥¨ä»£ç ï¼Œç§»é™¤.HKåç¼€
        clean_ticker = clean_hk_ticker(ticker)
        
        # æ„å»ºè¯·æ±‚URL
        timestamp = int(time.time() * 1000)
        params = {
            'callback': 'callback',
            'lang': 'EN',
            'type': 'A',
            'name': clean_ticker,
            'market': 'SEHK',
            '_': timestamp
        }
        
        # è®¾ç½®AJAXè¯·æ±‚å¤´
        ajax_headers = self.headers.copy()
        ajax_headers.update({
            'Accept': 'text/javascript, application/javascript, application/ecmascript, application/x-ecmascript, */*; q=0.01',
            'X-Requested-With': 'XMLHttpRequest',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Referer': f'{self.base_url}/search/titlesearch.xhtml?lang=en'
        })
        
        try:
            with httpx.Client(headers=ajax_headers, timeout=30) as client:
                response = client.get(self.prefix_url, params=params)
                response.raise_for_status()
                
                # è§£æJSONPå“åº”
                content = response.text
                logger.debug(f"æ¸¯è‚¡APIåŸå§‹å“åº”: {content}")
                
                # ç§»é™¤JSONPåŒ…è£…
                data = None
                if content.startswith('callback(') and content.endswith(');'):
                    json_str = content[9:-2]
                    data = json.loads(json_str)
                elif content.startswith('callback(') and content.endswith('});'):
                    json_str = content[9:-3]
                    data = json.loads(json_str)
                else:
                    # å°è¯•ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–JSONéƒ¨åˆ†
                    match = re.search(r'callback\((.*)\);?\s*$', content)
                    if match:
                        json_str = match.group(1)
                        data = json.loads(json_str)
                    else:
                        logger.error(f"æ— æ³•è§£æJSONPæ ¼å¼: {content}")
                        return None, None, None
                
                if 'stockInfo' in data and data['stockInfo']:
                    stock_info = data['stockInfo'][0]
                    stock_id = stock_info['stockId']
                    stock_code = stock_info['code']
                    stock_name = stock_info['name']
                    
                    logger.info(f"æ‰¾åˆ°æ¸¯è‚¡: {stock_code} - {stock_name} (ID: {stock_id})")
                    return stock_id, stock_code, stock_name
                else:
                    logger.warning(f"æœªæ‰¾åˆ°æ¸¯è‚¡ä»£ç  {ticker} çš„ä¿¡æ¯")
                    return None, None, None
                    
        except Exception as e:
            logger.error(f"è·å–æ¸¯è‚¡IDæ—¶å‡ºé”™: {str(e)}")
            return None, None, None
    
    def get_filings_list(self, stock_id, from_date=None, to_date=None, cutoff_date=None, status_callback=None):
        """è·å–æŒ‡å®šè‚¡ç¥¨çš„æ‰€æœ‰å…¬å‘Šåˆ—è¡¨ï¼Œæ”¯æŒè‡ªåŠ¨ç¿»é¡µ"""
        if not from_date:
            from_date = "19990401"  # é»˜è®¤ä»1999å¹´å¼€å§‹
        if not to_date:
            to_date = datetime.now().strftime("%Y%m%d")
        
        # è§£ææˆªæ­¢æ—¥æœŸ
        cutoff_datetime = None
        if cutoff_date:
            cutoff_datetime = datetime.strptime(cutoff_date, "%Y%m%d")
        
        all_filings = []
        row_range = 0  # ä»ç¬¬ä¸€é¡µå¼€å§‹
        page_size = 100  # æ¯é¡µ100æ¡è®°å½•
        total_record_count = None  # æ€»è®°å½•æ•°
        
        while True:
            # æ„å»ºGETè¯·æ±‚URLï¼ˆç¬¬ä¸€é¡µç”¨POSTï¼Œåç»­é¡µé¢ç”¨GETï¼‰
            if row_range == 0:
                # ç¬¬ä¸€é¡µä½¿ç”¨POSTè¯·æ±‚
                post_data = {
                    'lang': 'EN',
                    'category': '0',
                    'market': 'SEHK',
                    'searchType': '0',
                    'documentType': '-1',
                    't1code': '-2',
                    't2Gcode': '-2',
                    't2code': '-2',
                    'stockId': str(stock_id),
                    'from': from_date,
                    'to': to_date,
                    'MB-Daterange': '0',
                    'title': ''
                }
                
                # è®¾ç½®POSTè¯·æ±‚å¤´
                post_headers = self.headers.copy()
                post_headers.update({
                    'Content-Type': 'application/x-www-form-urlencoded',
                    'Cache-Control': 'max-age=0',
                    'Referer': f'{self.base_url}/search/titlesearch.xhtml?lang=en'
                })
                
                try:
                    with httpx.Client(headers=post_headers, timeout=30) as client:
                        response = client.post(self.search_url, data=post_data)
                        response.raise_for_status()
                        
                        page_filings = self.parse_filings_html(response.text)
                        
                except Exception as e:
                    logger.error(f"è·å–æ¸¯è‚¡å…¬å‘Šåˆ—è¡¨ç¬¬1é¡µæ—¶å‡ºé”™: {str(e)}")
                    break
            else:
                # åç»­é¡µé¢ä½¿ç”¨GETè¯·æ±‚
                get_url = f"{self.base_url}/search/titleSearchServlet.do"
                params = {
                    'sortDir': '0',
                    'sortByOptions': 'DateTime',
                    'category': '0',
                    'market': 'SEHK',
                    'stockId': str(stock_id),
                    'documentType': '-1',
                    'fromDate': from_date,
                    'toDate': to_date,
                    'title': '',
                    'searchType': '0',
                    't1code': '-2',
                    't2Gcode': '-2',
                    't2code': '-2',
                    'rowRange': str(row_range),
                    'lang': 'E'
                }
                
                # è®¾ç½®GETè¯·æ±‚å¤´
                get_headers = self.headers.copy()
                get_headers.update({
                    'accept': '*/*',
                    'x-requested-with': 'XMLHttpRequest',
                    'Referer': f'{self.base_url}/search/titlesearch.xhtml?lang=en'
                })
                
                try:
                    with httpx.Client(headers=get_headers, timeout=30) as client:
                        response = client.get(get_url, params=params)
                        response.raise_for_status()
                        
                        # è§£æJSONå“åº”
                        json_data = response.json()
                        page_filings = self.parse_filings_json(json_data.get('result', '[]'))
                        
                        # ä»JSONå“åº”ä¸­è·å–æ€»è®°å½•æ•°
                        if total_record_count is None:
                            try:
                                import json
                                result_data = json.loads(json_data.get('result', '[]'))
                                if result_data and len(result_data) > 0:
                                    total_record_count = int(result_data[0].get('TOTAL_COUNT', 0))
                                    logger.info(f"æ¸¯è‚¡å…¬å‘Šæ€»è®°å½•æ•°: {total_record_count}")
                            except (json.JSONDecodeError, ValueError, KeyError) as e:
                                logger.warning(f"æ— æ³•è§£ææ€»è®°å½•æ•°: {e}")
                        
                except Exception as e:
                    logger.error(f"è·å–æ¸¯è‚¡å…¬å‘Šåˆ—è¡¨ç¬¬{row_range//page_size + 1}é¡µæ—¶å‡ºé”™: {str(e)}")
                    break
            
            if not page_filings:
                logger.info(f"æ¸¯è‚¡å…¬å‘Šåˆ—è¡¨ç¬¬{row_range//page_size + 1}é¡µæ— æ•°æ®ï¼Œåœæ­¢ç¿»é¡µ")
                break
            
            # å…ˆå°†æœ¬é¡µæ‰€æœ‰å…¬å‘Šæ·»åŠ åˆ°ç»“æœä¸­
            all_filings.extend(page_filings)
            
            # æ£€æŸ¥æœ¬é¡µæœ€åä¸€ä¸ªå…¬å‘Šçš„æ—¥æœŸæ˜¯å¦æ—©äºæˆªæ­¢æ—¥æœŸ
            if cutoff_datetime and page_filings:
                last_filing = page_filings[-1]  # è·å–æœ€åä¸€ä¸ªå…¬å‘Š
                last_filing_date = self.parse_filing_date(last_filing.get('release_time', ''))
                if last_filing_date and last_filing_date < cutoff_datetime:
                    logger.info(f"æ¸¯è‚¡å…¬å‘Šæœ€åä¸€ä¸ªæ—¥æœŸ {last_filing_date} æ—©äºæˆªæ­¢æ—¥æœŸ {cutoff_datetime}ï¼Œåœæ­¢ç¿»é¡µ")
                    break
            
            # å¦‚æœè¿™ä¸€é¡µçš„è®°å½•æ•°å°‘äºé¡µé¢å¤§å°ï¼Œè¯´æ˜å·²ç»æ˜¯æœ€åä¸€é¡µ
            if len(page_filings) < page_size:
                logger.info(f"æ¸¯è‚¡å…¬å‘Šåˆ—è¡¨ç¬¬{row_range//page_size + 1}é¡µè®°å½•æ•° {len(page_filings)} å°‘äºé¡µé¢å¤§å°ï¼Œåœæ­¢ç¿»é¡µ")
                break
            
            # å‡†å¤‡ä¸‹ä¸€é¡µ
            row_range += page_size
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»è¶…è¿‡æ€»è®°å½•æ•°
            if total_record_count is not None and row_range >= total_record_count:
                logger.info(f"æ¸¯è‚¡å…¬å‘Šåˆ—è¡¨rowRange({row_range}) å·²è¾¾åˆ°æ€»è®°å½•æ•°({total_record_count})ï¼Œåœæ­¢ç¿»é¡µ")
                break
            
            logger.info(f"æ¸¯è‚¡å…¬å‘Šåˆ—è¡¨å‡†å¤‡è·å–ç¬¬{row_range//page_size + 1}é¡µ (rowRange={row_range})")
            if status_callback:
                status_callback(f"æ­£åœ¨è·å–ç¬¬ {row_range//page_size + 1} é¡µæ¸¯è‚¡å…¬å‘Š...")
        
        logger.info(f"æ¸¯è‚¡å…¬å‘Šåˆ—è¡¨å…±è·å– {len(all_filings)} æ¡è®°å½•")
        return all_filings
    
    def parse_filings_html(self, html_content):
        """è§£æHTMLå“åº”ï¼Œæå–å…¬å‘Šé“¾æ¥"""
        soup = BeautifulSoup(html_content, 'html.parser')
        filings = []
        
        # æŸ¥æ‰¾æ‰€æœ‰è¡¨æ ¼è¡Œ
        table_rows = soup.find_all('tr')
        
        for row in table_rows:
            # æŸ¥æ‰¾åŒ…å«PDFé“¾æ¥çš„å•å…ƒæ ¼
            doc_link_cell = row.find('div', class_='doc-link')
            if doc_link_cell:
                # è·å–PDFé“¾æ¥
                pdf_link = doc_link_cell.find('a')
                if pdf_link and pdf_link.get('href'):
                    href = pdf_link.get('href')
                    
                    # è·å–å‘å¸ƒæ—¶é—´
                    time_cell = row.find('td', class_='release-time')
                    release_time = time_cell.get_text(strip=True).replace('Release Time: ', '') if time_cell else 'N/A'
                    
                    # è·å–è‚¡ç¥¨ä»£ç 
                    code_cell = row.find('td', class_='stock-short-code')
                    stock_code = code_cell.get_text(strip=True).replace('Stock Code: ', '').split('\n')[0] if code_cell else 'N/A'
                    
                    # è·å–è‚¡ç¥¨åç§°
                    name_cell = row.find('td', class_='stock-short-name')
                    stock_name = name_cell.get_text(strip=True).replace('Stock Short Name: ', '').split('\n')[0] if name_cell else 'N/A'
                    
                    # è·å–æ–‡æ¡£æ ‡é¢˜
                    doc_title = pdf_link.get_text(strip=True)
                    
                    # è·å–æ–‡æ¡£ç±»å‹ï¼ˆä»headline divè·å–ï¼‰
                    headline_div = row.find('div', class_='headline')
                    doc_type = headline_div.get_text(strip=True) if headline_div else 'Unknown'
                    
                    # è·å–æ–‡ä»¶å¤§å°
                    filesize_span = doc_link_cell.find('span', class_='attachment_filesize')
                    filesize = filesize_span.get_text(strip=True) if filesize_span else 'N/A'
                    
                    # æ„å»ºå®Œæ•´URL
                    full_url = urljoin(self.base_url, href)
                    
                    filing_info = {
                        'release_time': release_time,
                        'stock_code': stock_code,
                        'stock_name': stock_name,
                        'doc_type': doc_type,
                        'doc_title': doc_title,
                        'file_size': filesize,
                        'url': full_url,
                        'href': href
                    }
                    
                    filings.append(filing_info)
        
        return filings
    
    def parse_filings_json(self, json_string):
        """è§£æJSONæ ¼å¼çš„å…¬å‘Šå“åº”"""
        try:
            import json
            filings_data = json.loads(json_string)
            filings = []
            
            for item in filings_data:
                filing_info = {
                    'release_time': item.get('DATE_TIME', ''),
                    'stock_code': item.get('STOCK_CODE', '').replace('<br/>', '/'),
                    'stock_name': item.get('STOCK_NAME', '').replace('<br/>', '/'),
                    'doc_type': item.get('LONG_TEXT', ''),
                    'doc_title': item.get('TITLE', ''),
                    'file_size': item.get('FILE_INFO', ''),
                    'url': f"{self.base_url}{item.get('FILE_LINK', '')}",
                    'href': item.get('FILE_LINK', '')
                }
                filings.append(filing_info)
            
            return filings
            
        except Exception as e:
            logger.error(f"è§£æJSONå…¬å‘Šæ•°æ®å¤±è´¥: {e}")
            return []
    
    def parse_filing_date(self, date_str):
        """è§£æå…¬å‘Šæ—¥æœŸå­—ç¬¦ä¸²ä¸ºdatetimeå¯¹è±¡"""
        try:
            # æ¸¯è‚¡æ—¥æœŸæ ¼å¼ï¼šDD/MM/YYYY HH:MM
            if '/' in date_str and ' ' in date_str:
                date_part = date_str.split(' ')[0]  # å–æ—¥æœŸéƒ¨åˆ†
                parts = date_part.split('/')
                if len(parts) == 3:
                    day, month, year = parts
                    return datetime(int(year), int(month), int(day))
            return None
        except Exception as e:
            logger.warning(f"è§£ææ¸¯è‚¡æ—¥æœŸå¤±è´¥: {date_str} - {e}")
            return None
    
    def categorize_filings(self, filings):
        """å°†å…¬å‘Šåˆ†ä¸ºä¸¤ç»„ï¼šå­£æŠ¥å¹´æŠ¥ç»„å’Œå…¶ä»–ç»„"""
        # å­£æŠ¥å¹´æŠ¥å…³é”®è¯ï¼ˆåŸºäºheadline divå†…å®¹ï¼‰
        quarterly_annual_keywords = [
            'Final Results',
            'Quarterly Results', 
            'Interim Results',
            'Offer for Subscription'
        ]
        
        quarterly_annual_filings = []
        other_filings = []
        
        for filing in filings:
            doc_type = filing['doc_type']
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å­£æŠ¥å¹´æŠ¥å…³é”®è¯
            is_quarterly_annual = any(keyword in doc_type for keyword in quarterly_annual_keywords)
            
            if is_quarterly_annual:
                quarterly_annual_filings.append(filing)
            else:
                other_filings.append(filing)
        
        return quarterly_annual_filings, other_filings
    
    def download_filing_content(self, filing_info):
        """ä¸‹è½½å•ä¸ªå…¬å‘Šæ–‡ä»¶å†…å®¹"""
        try:
            with httpx.Client(headers=self.headers, timeout=60) as client:
                response = client.get(filing_info['url'])
                response.raise_for_status()
                
                return response.content
                
        except Exception as e:
            logger.error(f"ä¸‹è½½æ¸¯è‚¡æ–‡ä»¶æ—¶å‡ºé”™ {filing_info['url']}: {str(e)}")
            return None

# è£…é¥°å™¨
def retry_on_failure(max_retries: int = config.MAX_RETRIES, delay: float = config.RETRY_DELAY):
    """é‡è¯•è£…é¥°å™¨"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            last_exception = None
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    logger.warning(f"å‡½æ•° {func.__name__} ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {e}")
                    if attempt < max_retries - 1:
                        time.sleep(delay * (2 ** attempt))  # æŒ‡æ•°é€€é¿
            
            logger.error(f"å‡½æ•° {func.__name__} åœ¨ {max_retries} æ¬¡å°è¯•åä»ç„¶å¤±è´¥")
            raise last_exception
        return wrapper
    return decorator

@contextmanager
def error_handler(operation_name: str):
    """é”™è¯¯å¤„ç†ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    try:
        yield
    except Exception as e:
        logger.error(f"{operation_name} å¤±è´¥: {e}")
        st.error(f"{operation_name} å¤±è´¥: {e}")
        raise

# Session State ç®¡ç†
class SessionManager:
    """SessionçŠ¶æ€ç®¡ç†å™¨"""
    
    @staticmethod
    def init_session_state():
        """åˆå§‹åŒ–æ‰€æœ‰å¿…è¦çš„session stateå˜é‡"""
        defaults = {
            "analyzer_messages": [],
            "analyzer_ticker": "",
            "analyzer_years": 2,
            "analyzer_use_sec_reports": True,
            "analyzer_use_sec_others": False,
            "analyzer_use_earnings": True,
            "analyzer_model": "gemini-2.5-flash",
            "api_key_cycle": cycle(st.secrets["GOOGLE_API_KEYS"]),
            "processing_status": ProcessingStatus().__dict__,
            "cache": {},
            "use_premium_api": False,
            "premium_access_code": "",
            "selected_language": "English"
        }
        
        for key, default_value in defaults.items():
            if key not in st.session_state:
                st.session_state[key] = default_value
    
    @staticmethod
    def get_processing_status() -> ProcessingStatus:
        """è·å–å¤„ç†çŠ¶æ€å¯¹è±¡"""
        # Get the field names from the dataclass
        known_fields = {f.name for f in fields(ProcessingStatus)}
        # Filter the session state dict to only include known fields
        current_status_dict = st.session_state.get("processing_status", {})
        filtered_args = {k: v for k, v in current_status_dict.items() if k in known_fields}
        return ProcessingStatus(**filtered_args)
    
    @staticmethod
    def update_processing_status(status: ProcessingStatus):
        """æ›´æ–°å¤„ç†çŠ¶æ€"""
        st.session_state.processing_status = status.__dict__

# AI æœåŠ¡
class GeminiService:
    """Gemini AIæœåŠ¡"""
    
    def __init__(self):
        self.rate_limiter = RateLimiter(max_calls=20, window=60)
    
    def get_next_api_key(self) -> str:
        """è·å–ä¸‹ä¸€ä¸ªAPIå¯†é’¥"""
        # æª¢æŸ¥æ˜¯å¦ä½¿ç”¨ä»˜è²»API
        if st.session_state.get("use_premium_api", False):
            return st.secrets["PREMIUM_API_KEY"]
        
        # ä½¿ç”¨ä¸€èˆ¬çš„è¼ªæ›API
        if hasattr(st.session_state, 'api_key_cycle'):
            return next(st.session_state.api_key_cycle)
        else:
            # å¦‚æœsession stateæœªåˆå§‹åŒ–ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
            if not hasattr(self, '_api_key_cycle'):
                api_keys = st.secrets["GOOGLE_API_KEYS"]
                self._api_key_cycle = cycle(api_keys)
            return next(self._api_key_cycle)
    
    def init_client(self) -> genai.Client:
        """åˆå§‹åŒ–Geminiå®¢æˆ·ç«¯"""
        return genai.Client(api_key=self.get_next_api_key())
    
    @retry_on_failure(max_retries=3)
    def call_api(self, prompt: str, model_type: str = "gemini-2.5-flash") -> str:
        """è°ƒç”¨Gemini API"""
        self.rate_limiter.wait_if_needed()
        
        try:
            client = self.init_client()
            
            contents = [
                types.Content(
                    role="user",
                    parts=[types.Part.from_text(text=prompt)],
                ),
            ]

            response = client.models.generate_content(
                model=model_type,
                contents=contents,
            )

            return response.candidates[0].content.parts[0].text
            
        except Exception as e:
            logger.error(f"Gemini APIè°ƒç”¨å¤±è´¥: {e}")
            raise APIError(f"Gemini APIè°ƒç”¨å¤±è´¥: {e}")
    
    @retry_on_failure(max_retries=3)
    def call_api_stream(self, prompt: str, model_type: str = "gemini-2.5-flash"):
        """è°ƒç”¨Gemini API æµå¼å“åº”"""
        self.rate_limiter.wait_if_needed()
        
        try:
            client = self.init_client()
            
            contents = [
                types.Content(
                    role="user",
                    parts=[types.Part.from_text(text=prompt)],
                ),
            ]

            # ä½¿ç”¨æµå¼å“åº”
            response_stream = client.models.generate_content_stream(
                model=model_type,
                contents=contents,
            )
            
            # ç”Ÿæˆå™¨å‡½æ•°ï¼Œé€æ­¥è¿”å›æ–‡æœ¬ç‰‡æ®µ
            for chunk in response_stream:
                if chunk.candidates and chunk.candidates[0].content.parts:
                    yield chunk.candidates[0].content.parts[0].text
            
        except Exception as e:
            logger.error(f"Gemini APIæµå¼è°ƒç”¨å¤±è´¥: {e}")
            raise APIError(f"Gemini APIæµå¼è°ƒç”¨å¤±è´¥: {e}")
    
    @retry_on_failure(max_retries=3)
    def classify_6k_document(self, document_content: str) -> bool:
        """ä½¿ç”¨ä¾¿å®œæ¨¡å‹åˆ¤æ–­6-Kæ–‡ä»¶æ˜¯å¦ä¸ºå­£æŠ¥/å¹´æŠ¥/IPOæŠ¥å‘Š"""
        try:
            # è·å–å½“å‰è¯­è¨€è®¾ç½®
            language = st.session_state.get("selected_language", "English")
            
            if language == "English":
                prompt = f"""
                You are a financial document classifier. Please analyze the following 6-K filing content and determine if it is a Quarterly Report, Annual Report, or IPO Report.

                Classification criteria:
                - Quarterly Report: Contains quarterly financial results, earnings data, quarterly business updates
                - Annual Report: Contains annual financial results, yearly business summary, annual shareholder information
                - IPO Report: Contains initial public offering information, prospectus data, listing announcements

                Please respond with ONLY a JSON object in this exact format:
                {{
                    "is_quarterly_annual_ipo": true/false,
                    "document_type": "quarterly/annual/ipo/other",
                    "confidence": "high/medium/low"
                }}

                Document content (first 5000 characters):
                {document_content[:5000]}
                """
            else:
                prompt = f"""
                ä½ æ˜¯ä¸€ä¸ªé‡‘èæ–‡æ¡£åˆ†ç±»å™¨ã€‚è¯·åˆ†æä»¥ä¸‹6-Kæ–‡ä»¶å†…å®¹ï¼Œåˆ¤æ–­å®ƒæ˜¯å¦æ˜¯å­£æŠ¥ã€å¹´æŠ¥æˆ–IPOæŠ¥å‘Šã€‚

                åˆ†ç±»æ ‡å‡†ï¼š
                - å­£æŠ¥ï¼šåŒ…å«å­£åº¦è´¢åŠ¡ç»“æœã€ç›ˆåˆ©æ•°æ®ã€å­£åº¦ä¸šåŠ¡æ›´æ–°
                - å¹´æŠ¥ï¼šåŒ…å«å¹´åº¦è´¢åŠ¡ç»“æœã€å¹´åº¦ä¸šåŠ¡æ€»ç»“ã€å¹´åº¦è‚¡ä¸œä¿¡æ¯
                - IPOæŠ¥å‘Šï¼šåŒ…å«é¦–æ¬¡å…¬å¼€å‘è¡Œä¿¡æ¯ã€æ‹›è‚¡è¯´æ˜ä¹¦æ•°æ®ã€ä¸Šå¸‚å…¬å‘Š

                è¯·åªå›ç­”JSONæ ¼å¼ï¼š
                {{
                    "is_quarterly_annual_ipo": true/false,
                    "document_type": "quarterly/annual/ipo/other",
                    "confidence": "high/medium/low"
                }}

                æ–‡æ¡£å†…å®¹ï¼ˆå‰5000å­—ç¬¦ï¼‰ï¼š
                {document_content[:5000]}
                """
            
            result = self.call_api(prompt, "gemini-2.5-flash-lite-preview-06-17")
            
            # å°è¯•è§£æJSON
            try:
                # å°è¯•ä»Markdownä»£ç å—ä¸­æå–JSON
                import re
                match = re.search(r"```json\s*(\{.*?\})\s*```", result, re.DOTALL)
                if match:
                    json_str = match.group(1)
                else:
                    json_str = result
                
                import json
                classification = json.loads(json_str)
                return classification.get("is_quarterly_annual_ipo", False)
                
            except (json.JSONDecodeError, ValueError) as e:
                logger.warning(f"è§£æ6-Kåˆ†ç±»JSONå¤±è´¥: {e}. æ¨¡å‹è¿”å›: {result}")
                # å¦‚æœè§£æå¤±è´¥ï¼Œä¿å®ˆå¤„ç†ï¼Œè¿”å›Trueç»§ç»­åˆ†æ
                return True
                
        except Exception as e:
            logger.error(f"6-Kæ–‡æ¡£åˆ†ç±»å¤±è´¥: {e}")
            # å¦‚æœåˆ†ç±»å¤±è´¥ï¼Œä¿å®ˆå¤„ç†ï¼Œè¿”å›Trueç»§ç»­åˆ†æ
            return True

    @retry_on_failure(max_retries=1)
    def count_tokens(self, text: str, model_type: str = "gemini-2.5-flash") -> int:
        """è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡"""
        try:
            client = self.init_client()
            response = client.models.count_tokens(
                model=model_type,
                contents=[text]
            )
            return response.total_tokens
        except Exception as e:
            logger.error(f"è®¡ç®—Tokenæ•°é‡å¤±è´¥: {e}")
            # å¦‚æœAPIè°ƒç”¨å¤±è´¥ï¼Œå¯ä»¥è¿”å›ä¸€ä¸ªä¼°ç®—å€¼æˆ–é”™è¯¯æ ‡è¯†
            # åŸºäºä¸€èˆ¬è§„åˆ™ï¼Œä¸€ä¸ªtokençº¦ç­‰äº4ä¸ªè‹±æ–‡å­—ç¬¦æˆ–0.5-1.5ä¸ªä¸­æ–‡å­—ç¬¦
            return len(text) // 2

# SEC æœåŠ¡
class SECService:
    """SECæ–‡ä»¶æœåŠ¡"""
    
    def __init__(self, cache_manager: CacheManager):
        self.rate_limiter = RateLimiter(max_calls=10, window=60)
        warnings.filterwarnings("ignore", category=XMLParsedAsHTMLWarning)
        self.cache_manager = cache_manager
        self.sixk_processor = None  # å°†åœ¨éœ€è¦æ—¶åˆå§‹åŒ–
    
    def _init_sixk_processor(self, temp_dir: str):
        """åˆå§‹åŒ–6-Kå¤„ç†å™¨"""
        if self.sixk_processor is None:
            self.sixk_processor = SixKProcessor(temp_dir)

    @lru_cache(maxsize=100)
    def get_cik_map(self) -> Dict[str, str]:
        """è·å–tickeråˆ°CIKçš„æ˜ å°„ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        cache_key = "cik_map"
        cached_result = self.cache_manager.get(cache_key)
        if cached_result:
            return cached_result
        
        try:
            headers = {'User-Agent': config.SEC_USER_AGENT}
            response = httpx.get(
                "https://www.sec.gov/files/company_tickers.json", 
                headers=headers,
                timeout=config.REQUEST_TIMEOUT
            )
            response.raise_for_status()
            
            all_companies = response.json()
            cik_map = {company['ticker']: company['cik_str'] for company in all_companies.values()}
            
            self.cache_manager.set(cache_key, cik_map)
            return cik_map
            
        except Exception as e:
            logger.error(f"è·å–CIKæ˜ å°„å¤±è´¥: {e}")
            raise DataRetrievalError(f"è·å–CIKæ˜ å°„å¤±è´¥: {e}")
    
    @retry_on_failure(max_retries=3)
    def get_filings(self, ticker: str, years: int = 3, forms_to_include: Optional[List[str]] = None, status_callback=None) -> List[Document]:
        """è·å–SECæ–‡ä»¶åˆ—è¡¨"""
        self.rate_limiter.wait_if_needed()
        
        # å¦‚æœæœªæä¾›è¦åŒ…å«çš„è¡¨å•ï¼Œåˆ™ä½¿ç”¨é…ç½®ä¸­çš„æ‰€æœ‰è¡¨å•
        if forms_to_include is None:
            forms_to_include = config.SEC_FORMS
        
        cache_key = self.cache_manager.get_cache_key("sec_filings", ticker, years, tuple(sorted(forms_to_include)))
        cached_result = self.cache_manager.get(cache_key)
        if cached_result:
            return cached_result
        
        try:
            edgar = EdgarClient(user_agent=config.SEC_USER_AGENT)
            
            # ä¿®æ­£å¹´ä»½è®¡ç®—é€»è¾‘ï¼šå¦‚æœæ˜¯2å¹´ï¼Œå°±æ˜¯2024/1/1åˆ°2025/1/1
            current_year = datetime.now().year
            end_date = datetime(current_year + 1, 1, 1)  # å»¶ä¼¸åˆ°ä¸‹ä¸€å¹´çš„1æœˆ1å·
            start_date = datetime(current_year - years + 1, 1, 1)  # å¾€å‰æ¨yearså¹´
            
            logger.info(f"SECæ–‡ä»¶æ—¥æœŸèŒƒå›´: {start_date.date()} åˆ° {end_date.date()}")
            
            ticker_map = self.get_cik_map()
            cik = ticker_map.get(ticker.upper())
            
            if not cik:
                logger.warning(f"æœªæ‰¾åˆ°ticker {ticker} çš„CIK")
                return []
            
            submissions = edgar.get_submissions(cik=str(cik).zfill(10))
            
            # è·å–æ‰€æœ‰æ–‡ä»¶æ•°æ®ï¼ˆåŒ…æ‹¬recentå’Œfilesï¼‰
            all_forms = []
            all_accession_numbers = []
            all_filing_dates = []
            all_primary_documents = []
            
            # å¤„ç†recentæ•°æ®
            recent = submissions.get('filings', {}).get('recent', {})
            if recent and 'form' in recent:
                all_forms.extend(recent.get('form', []))
                all_accession_numbers.extend(recent.get('accessionNumber', []))
                all_filing_dates.extend(recent.get('filingDate', []))
                all_primary_documents.extend(recent.get('primaryDocument', []))
                logger.info(f"SEC recentæ–‡ä»¶æ•°: {len(recent.get('form', []))}")
            
            # å¤„ç†filesæ•°æ®ï¼ˆå†å²æ–‡ä»¶ï¼‰
            files = submissions.get('filings', {}).get('files', [])
            if files:
                logger.info(f"SECéœ€è¦å¤„ç† {len(files)} ä¸ªå†å²æ–‡ä»¶æ‰¹æ¬¡")
                max_batches = 100  # æœ€å¤šå¤„ç†10ä¸ªæ‰¹æ¬¡ï¼Œé¿å…è¿‡å¤šAPIè°ƒç”¨
                processed_batches = 0
                
                for idx, file_info in enumerate(files):
                    if processed_batches >= max_batches:
                        logger.info(f"SECå·²å¤„ç† {max_batches} ä¸ªæ‰¹æ¬¡ï¼Œåœæ­¢å¤„ç†æ›´å¤šå†å²æ–‡ä»¶")
                        break
                        
                    file_name = file_info.get('name', '')
                    if file_name:
                        try:
                            if status_callback:
                                status_callback(f"æ­£åœ¨è·å–ç¬¬ {idx + 1}/{min(len(files), max_batches)} æ‰¹SECå†å²æ–‡ä»¶...")
                            # è·å–å†å²æ–‡ä»¶æ•°æ®
                            historical_data = edgar.get_submissions(cik=str(cik).zfill(10), file_name=file_name)
                            if historical_data and 'form' in historical_data:
                                batch_size = len(historical_data.get('form', []))
                                logger.info(f"SECå†å²æ–‡ä»¶æ‰¹æ¬¡ {file_name}: {batch_size} ä¸ªæ–‡ä»¶")
                                
                                # æ£€æŸ¥æ‰¹æ¬¡ä¸­æœ€æ—©çš„æ—¥æœŸï¼Œå¦‚æœå¤ªæ—©åˆ™åœæ­¢
                                batch_dates = historical_data.get('filingDate', [])
                                if batch_dates:
                                    earliest_date = min(batch_dates)
                                    earliest_datetime = datetime.strptime(earliest_date, '%Y-%m-%d').date()
                                    if earliest_datetime < start_date.date():
                                        logger.info(f"SECå†å²æ–‡ä»¶æ‰¹æ¬¡ {file_name} æœ€æ—©æ—¥æœŸ {earliest_datetime} æ—©äºæˆªæ­¢æ—¥æœŸ {start_date.date()}ï¼Œåœæ­¢å¤„ç†")
                                        break
                                
                                all_forms.extend(historical_data.get('form', []))
                                all_accession_numbers.extend(historical_data.get('accessionNumber', []))
                                all_filing_dates.extend(historical_data.get('filingDate', []))
                                all_primary_documents.extend(historical_data.get('primaryDocument', []))
                                processed_batches += 1
                        except Exception as e:
                            logger.warning(f"è·å–SECå†å²æ–‡ä»¶æ‰¹æ¬¡ {file_name} å¤±è´¥: {e}")
                            continue
            
            if not all_forms:
                logger.warning(f"æœªæ‰¾åˆ°SECæ–‡ä»¶æ•°æ®")
                return []
            
            logger.info(f"SECæ€»æ–‡ä»¶æ•°: {len(all_forms)}")
            
            documents = []
            cutoff_reached = False
            
            for i in range(len(all_forms)):
                form_type = all_forms[i]
                if form_type in forms_to_include:
                    filing_date = datetime.strptime(all_filing_dates[i], '%Y-%m-%d').date()
                    
                    # æ£€æŸ¥æ˜¯å¦æ—©äºæˆªæ­¢æ—¥æœŸ
                    if filing_date < start_date.date():
                        logger.info(f"SECæ–‡ä»¶æ—¥æœŸ {filing_date} æ—©äºæˆªæ­¢æ—¥æœŸ {start_date.date()}ï¼Œåœæ­¢å¤„ç†")
                        cutoff_reached = True
                        break
                    
                    if start_date.date() <= filing_date < end_date.date():  # ä¸åŒ…å«ç»“æŸæ—¥æœŸ
                        accession_no_no_dashes = all_accession_numbers[i].replace('-', '')
                        filing_url = f"https://www.sec.gov/Archives/edgar/data/{cik}/{accession_no_no_dashes}/{all_primary_documents[i]}"
                        
                        documents.append(Document(
                            type='SEC Filing',
                            title=f"{ticker} {form_type}",
                            date=filing_date,
                            url=filing_url,
                            form_type=form_type
                        ))
            
            # æŒ‰æ—¥æœŸæ’åºï¼ˆæ–°åˆ°æ—§ï¼‰
            documents.sort(key=lambda x: x.date, reverse=True)
            
            logger.info(f"æ‰¾åˆ° {len(documents)} ä¸ªSECæ–‡ä»¶")
            self.cache_manager.set(cache_key, documents)
            return documents
            
        except Exception as e:
            logger.error(f"è·å–SECæ–‡ä»¶å¤±è´¥: {e}")
            raise DataRetrievalError(f"è·å–SECæ–‡ä»¶å¤±è´¥: {e}")
    
    @retry_on_failure(max_retries=3)
    def download_filing(self, filing_url: str) -> str:
        """ä¸‹è½½SECæ–‡ä»¶å†…å®¹"""
        self.rate_limiter.wait_if_needed()
        
        try:
            response = httpx.get(
                filing_url, 
                headers={"User-Agent": config.SEC_USER_AGENT},
                timeout=config.REQUEST_TIMEOUT
            )
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'lxml')
            document_tag = soup.find('document')
            
            content = document_tag.get_text(separator='\n', strip=True) if document_tag else soup.get_text(separator='\n', strip=True)
            
            # é™åˆ¶å†…å®¹é•¿åº¦
            if len(content) > config.MAX_CONTENT_LENGTH:
                content = content[:config.MAX_CONTENT_LENGTH] + "\n[å†…å®¹å·²æˆªæ–­]"
            
            return content
            
        except Exception as e:
            logger.error(f"ä¸‹è½½SECæ–‡ä»¶å¤±è´¥: {e}")
            return f"ä¸‹è½½æ–‡ä»¶æ—¶å‡ºé”™: {e}"

# æ¸¯è‚¡æœåŠ¡
class HKStockService:
    """æ¸¯è‚¡æ–‡ä»¶æœåŠ¡"""
    
    def __init__(self, cache_manager: CacheManager):
        self.rate_limiter = RateLimiter(max_calls=10, window=60)
        self.cache_manager = cache_manager
        self.downloader = HKStockFilingsDownloader()
    
    def parse_hk_date(self, date_str: str) -> Optional[datetime.date]:
        """è§£ææ¸¯è‚¡æ—¥æœŸæ ¼å¼"""
        try:
            # æ¸¯è‚¡æ—¥æœŸæ ¼å¼ï¼šDD/MM/YYYY HH:MM æˆ– Release Time:DD/MM/YYYY HH:MM
            date_part = date_str
            if 'Release Time:' in date_str:
                date_part = date_str.replace('Release Time:', '').strip()
            
            if '/' in date_part and ' ' in date_part:
                date_only = date_part.split(' ')[0]  # å–æ—¥æœŸéƒ¨åˆ†
                parts = date_only.split('/')
                if len(parts) == 3:
                    day, month, year = parts
                    return datetime(int(year), int(month), int(day)).date()
            return None
        except Exception as e:
            logger.warning(f"è§£ææ¸¯è‚¡æ—¥æœŸå¤±è´¥: {date_str} - {e}")
            return None
    
    @retry_on_failure(max_retries=3)
    def get_hk_filings(self, ticker: str, years: int = 3, forms_to_include: Optional[List[str]] = None, status_callback=None) -> List[Document]:
        """è·å–æ¸¯è‚¡æ–‡ä»¶åˆ—è¡¨"""
        self.rate_limiter.wait_if_needed()
        
        # å¦‚æœæœªæä¾›è¦åŒ…å«çš„è¡¨å•ï¼Œåˆ™ä½¿ç”¨é»˜è®¤çš„å­£æŠ¥å¹´æŠ¥
        if forms_to_include is None:
            forms_to_include = ['quarterly_annual']
        
        cache_key = self.cache_manager.get_cache_key("hk_filings", ticker, years, tuple(sorted(forms_to_include)))
        cached_result = self.cache_manager.get(cache_key)
        if cached_result:
            return cached_result
        
        try:
            # è·å–è‚¡ç¥¨ID
            if status_callback:
                status_callback("æ­£åœ¨è·å–æ¸¯è‚¡ID...")
            stock_id, stock_code, stock_name = self.downloader.get_stock_id(ticker)
            if not stock_id:
                logger.warning(f"æœªæ‰¾åˆ°æ¸¯è‚¡ticker {ticker} çš„ä¿¡æ¯")
                return []
            
            # è®¡ç®—æ—¥æœŸèŒƒå›´
            current_year = datetime.now().year
            end_date = datetime(current_year + 1, 1, 1)
            start_date = datetime(current_year - years + 1, 1, 1)
            
            # è½¬æ¢ä¸ºæ¸¯è‚¡APIæ ¼å¼
            from_date = start_date.strftime("%Y%m%d")
            to_date = end_date.strftime("%Y%m%d")
            
            logger.info(f"æ¸¯è‚¡æ–‡ä»¶æ—¥æœŸèŒƒå›´: {start_date.date()} åˆ° {end_date.date()}")
            
            # è·å–å…¬å‘Šåˆ—è¡¨
            if status_callback:
                status_callback("æ­£åœ¨è·å–æ¸¯è‚¡å…¬å‘Šåˆ—è¡¨...")
            all_filings = self.downloader.get_filings_list(stock_id, from_date, to_date, from_date, status_callback)
            
            if not all_filings:
                logger.warning(f"æœªæ‰¾åˆ°æ¸¯è‚¡ {ticker} çš„å…¬å‘Š")
                return []
            
            # åˆ†ç±»å…¬å‘Š
            quarterly_annual_filings, other_filings = self.downloader.categorize_filings(all_filings)
            
            # æ ¹æ®ç”¨æˆ·é€‰æ‹©å†³å®šä½¿ç”¨å“ªäº›å…¬å‘Š
            selected_filings = []
            if 'quarterly_annual' in forms_to_include:
                selected_filings.extend(quarterly_annual_filings)
            if 'others' in forms_to_include:
                selected_filings.extend(other_filings)
            
            # è½¬æ¢ä¸ºDocumentå¯¹è±¡
            documents = []
            for filing in selected_filings:
                # è§£ææ—¥æœŸ
                filing_date = self.parse_hk_date(filing['release_time'])
                if filing_date and start_date.date() <= filing_date < end_date.date():
                    documents.append(Document(
                        type='HK Stock Filing',
                        title=f"{stock_code} {filing['doc_type']} - {filing['doc_title']}",
                        date=filing_date,
                        url=filing['url'],
                        form_type=filing['doc_type'],
                        content=None  # å†…å®¹å°†åœ¨éœ€è¦æ—¶ä¸‹è½½
                    ))
            
            # æŒ‰æ—¥æœŸæ’åºï¼ˆæ–°åˆ°æ—§ï¼‰
            documents.sort(key=lambda x: x.date, reverse=True)
            
            logger.info(f"æ‰¾åˆ° {len(documents)} ä¸ªæ¸¯è‚¡æ–‡ä»¶")
            self.cache_manager.set(cache_key, documents)
            return documents
            
        except Exception as e:
            logger.error(f"è·å–æ¸¯è‚¡æ–‡ä»¶å¤±è´¥: {e}")
            raise DataRetrievalError(f"è·å–æ¸¯è‚¡æ–‡ä»¶å¤±è´¥: {e}")
    
    @retry_on_failure(max_retries=3)
    def download_hk_filing(self, filing_url: str) -> str:
        """ä¸‹è½½æ¸¯è‚¡æ–‡ä»¶å†…å®¹"""
        self.rate_limiter.wait_if_needed()
        
        try:
            # æ„å»ºfiling_infoå¯¹è±¡
            filing_info = {'url': filing_url}
            
            # ä¸‹è½½PDFå†…å®¹
            pdf_content = self.downloader.download_filing_content(filing_info)
            
            if not pdf_content:
                return "ä¸‹è½½æ¸¯è‚¡æ–‡ä»¶å¤±è´¥"
            
            # ä½¿ç”¨PyMuPDFå¤„ç†PDF
            with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as temp_file:
                temp_file.write(pdf_content)
                temp_file_path = temp_file.name
            
            try:
                # ä½¿ç”¨PyMuPDFæå–æ–‡æœ¬
                doc = fitz.open(temp_file_path)
                text = ""
                
                for page_num in range(doc.page_count):
                    page = doc[page_num]
                    page_text = page.get_text()
                    if page_text.strip():
                        text += f"\n--- ç¬¬ {page_num + 1} é¡µ ---\n"
                        text += page_text
                        text += "\n"
                
                doc.close()
                
                # æ¸…ç†æ–‡æœ¬
                text = re.sub(r'\n{3,}', '\n\n', text)
                
                # é™åˆ¶å†…å®¹é•¿åº¦
                if len(text) > config.MAX_CONTENT_LENGTH:
                    text = text[:config.MAX_CONTENT_LENGTH] + "\n[å†…å®¹å·²æˆªæ–­]"
                
                return text.strip()
                
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                os.unlink(temp_file_path)
            
        except Exception as e:
            logger.error(f"ä¸‹è½½æ¸¯è‚¡æ–‡ä»¶å¤±è´¥: {e}")
            return f"ä¸‹è½½æ¸¯è‚¡æ–‡ä»¶æ—¶å‡ºé”™: {e}"

# è´¢æŠ¥ä¼šè®®è®°å½•æœåŠ¡
class EarningsService:
    """è´¢æŠ¥ä¼šè®®è®°å½•æœåŠ¡"""
    
    def __init__(self, cache_manager: CacheManager):
        self.rate_limiter = RateLimiter(max_calls=10, window=60)
        self.cache_manager = cache_manager
        self.session = requests.Session() # ä½¿ç”¨æŒä¹…åŒ–ä¼šè¯å¤„ç†cookies
        self._lock = threading.Lock()  # æ·»åŠ çº¿ç¨‹é”ç”¨äºå¹¶è¡Œå¤„ç†

    @staticmethod
    def parse_transcript_url(url_path: str) -> Optional[Tuple[str, int, str]]:
        """
        ä»URLè·¯å¾„ä¸­è§£æè‚¡ç¥¨ä»£ç ã€å¹´ä»½å’Œå­£åº¦ä¿¡æ¯
        
        Args:
            url_path (str): transcript URLè·¯å¾„, ä¾‹å¦‚ '/company/BABA/transcripts/2025/4/'
        
        Returns:
            tuple: (ticker, year, quarter) æˆ– None
        """
        # å…è®¸tickerä¸­åŒ…å«ç‚¹(.)å’Œæ•°å­—
        pattern = r'/company/([A-Z0-9\.]+)/transcripts/(\d{4})/(\d+)/'
        match = re.match(pattern, url_path)
        
        if match:
            ticker, year, quarter = match.groups()
            return ticker, int(year), str(quarter)
        
        return None

    def get_earnings_transcript_batch(self, url_paths: List[str], max_workers: int = 1) -> List[Optional[Dict]]:
        """
        å¹¶è¡Œè·å–å¤šä¸ªè´¢æŠ¥ä¼šè®®è®°å½•
        
        Args:
            url_paths: URLè·¯å¾„åˆ—è¡¨
            max_workers: æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°
        
        Returns:
            List[Optional[Dict]]: è´¢æŠ¥è®°å½•ä¿¡æ¯åˆ—è¡¨ï¼Œä¸è¾“å…¥é¡ºåºä¸€è‡´
        """
        results = [None] * len(url_paths)  # é¢„åˆ†é…ç»“æœåˆ—è¡¨
        
        def process_single_transcript(index_url_pair):
            """å¤„ç†å•ä¸ªè´¢æŠ¥è®°å½•çš„å†…éƒ¨å‡½æ•°"""
            index, url_path = index_url_pair
            try:
                with self._lock:
                    # ä½¿ç”¨é”æ¥é™åˆ¶å¹¶å‘è¯·æ±‚
                    self.rate_limiter.wait_if_needed()
                
                result = self.get_earnings_transcript(url_path)
                return index, result
            except Exception as e:
                logger.error(f"å¹¶è¡Œå¤„ç†è´¢æŠ¥è®°å½•å¤±è´¥ {url_path}: {e}")
                return index, None
        
        # ä½¿ç”¨ThreadPoolExecutorè¿›è¡Œå¹¶è¡Œå¤„ç†
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_index = {
                executor.submit(process_single_transcript, (i, url_path)): i 
                for i, url_path in enumerate(url_paths)
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_index):
                try:
                    index, result = future.result()
                    results[index] = result
                except Exception as e:
                    index = future_to_index[future]
                    logger.error(f"å¹¶è¡Œå¤„ç†ä»»åŠ¡å¤±è´¥ index {index}: {e}")
                    results[index] = None
        
        return results

    @retry_on_failure(max_retries=3)
    def get_available_quarters(self, ticker: str) -> List[str]:
        """è·å–æŒ‡å®šå…¬å¸æ‰€æœ‰å¯ç”¨çš„å­£åº¦URLåˆ—è¡¨ (ä¸æµ‹è¯•è„šæœ¬å¯¹é½)"""
        self.rate_limiter.wait_if_needed()
        
        cache_key = self.cache_manager.get_cache_key("earnings_quarters_urls", ticker)
        cached_result = self.cache_manager.get(cache_key)
        if cached_result:
            logger.info(f"ä»ç¼“å­˜åŠ è½½ {ticker} çš„å¯ç”¨å­£åº¦åˆ—è¡¨")
            return cached_result
        
        try:
            ticker_upper = ticker.upper()
            logger.info(f"è·å– {ticker_upper} çš„æ‰€æœ‰å¯ç”¨å­£åº¦...")
            url = f"https://discountingcashflows.com/company/{ticker_upper}/transcripts/?org.htmx.cache-buster=baseContent"
            headers = {
                "accept": "*/*",
                "accept-language": "en-US,en;q=0.9,zh-TW;q=0.8,zh;q=0.7,zh-CN;q=0.6,ja;q=0.5",
                "hx-boosted": "true",
                "hx-current-url": f"https://discountingcashflows.com/company/{ticker_upper}/chart/",
                "hx-request": "true",
                "hx-target": "baseContent",
                "priority": "u=1, i",
                "sec-ch-ua": '"Not)A;Brand";v="8", "Chromium";v="138", "Google Chrome";v="138"',
                "sec-ch-ua-mobile": "?0",
                "sec-ch-ua-platform": '"macOS"',
                "sec-fetch-dest": "empty",
                "sec-fetch-mode": "cors",
                "sec-fetch-site": "same-origin",
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36"
            }
            
            response = self.session.get(url, headers=headers, timeout=config.REQUEST_TIMEOUT)
            logger.info(f"è·å–å¯ç”¨å­£åº¦åˆ—è¡¨ å“åº”çŠ¶æ€ç : {response.status_code}")
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            transcript_links = []
            
            pattern = re.compile(rf'/company/{ticker_upper}/transcripts/(\d{{4}})/(\d+)/')
            
            for link in soup.find_all('a', href=pattern):
                href = link.get('href')
                if href and href != f'/company/{ticker_upper}/transcripts/':
                    transcript_links.append(href)
            
            transcript_links = sorted(list(set(transcript_links)), reverse=True)
            logger.info(f"æ‰¾åˆ° {len(transcript_links)} ä¸ªå¯ç”¨å­£åº¦ for {ticker_upper}")
            
            self.cache_manager.set(cache_key, transcript_links)
            return transcript_links
            
        except Exception as e:
            logger.error(f"è·å–è´¢æŠ¥ä¼šè®®è®°å½•å­£åº¦åˆ—è¡¨å¤±è´¥: {e}")
            raise DataRetrievalError(f"è·å–è´¢æŠ¥ä¼šè®®è®°å½•å­£åº¦åˆ—è¡¨å¤±è´¥: {e}")
    
    def parse_date_text(self, date_text: str) -> Optional[datetime.date]:
        """è§£ææ—¥æœŸæ–‡æœ¬"""
        if not date_text:
            return None
        
        for fmt in config.DATE_FORMATS:
            try:
                return datetime.strptime(date_text, fmt).date()
            except ValueError:
                continue
        
        logger.warning(f"æ— æ³•è§£ææ—¥æœŸæ ¼å¼: {date_text}")
        return None
    
    @retry_on_failure(max_retries=3)
    def get_earnings_transcript(self, url_path: str) -> Optional[Dict]:
        """
        è·å–å•ä¸ªè´¢æŠ¥ä¼šè®®è®°å½•ï¼ŒåŒ…å«æ—¥æœŸè§£æå’Œæ–‡ä»¶ä¿å­˜ï¼Œä¸æµ‹è¯•è„šæœ¬é€»è¾‘å®Œå…¨å¯¹é½ã€‚
        """
        self.rate_limiter.wait_if_needed()

        parsed_info = self.parse_transcript_url(url_path)
        if not parsed_info:
            logger.error(f"æ— æ³•ä»URLè·¯å¾„è§£æä¿¡æ¯: {url_path}")
            return None
        ticker, year, quarter_num = parsed_info
        logger.info(f"--- å¼€å§‹è·å–: {ticker} Q{quarter_num} {year} ---")
        logger.info(f"é€‰æ‹©çš„URL: {url_path}")

        try:
            base_url = f"https://discountingcashflows.com{url_path}"
            cache_buster_url = f"{base_url}?org.htmx.cache-buster=transcriptsContent"
            
            headers = {
                "accept": "*/*",
                "accept-language": "en-US,en;q=0.9,zh-TW;q=0.8,zh;q=0.7,zh-CN;q=0.6,ja;q=0.5",
                "hx-boosted": "true",
                "hx-current-url": base_url,
                "hx-request": "true",
                "hx-target": "transcriptsContent",
                "priority": "u=1, i",
                "sec-ch-ua": '"Google Chrome";v="137", "Chromium";v="137", "Not/A)Brand";v="24"',
                "sec-ch-ua-mobile": "?0",
                "sec-ch-ua-platform": '"macOS"',
                "sec-fetch-dest": "empty",
                "sec-fetch-mode": "cors",
                "sec-fetch-site": "same-origin",
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36"
            }

            logger.info(f"Getting main page: {base_url}")
            main_response = self.session.get(base_url, headers={"User-Agent": headers["User-Agent"]})
            logger.info(f"Main page response status: {main_response.status_code}")
            main_response.raise_for_status()

            csrf_match = re.search(r'csrftoken["\']?\s*:\s*["\']([^"\']+)["\']', main_response.text)
            if not csrf_match:
                csrf_match = re.search(r'name=["\']csrfmiddlewaretoken["\'] value=["\']([^"\']+)["\']', main_response.text)
            if csrf_match:
                csrf_token = csrf_match.group(1)
                headers["x-csrftoken"] = csrf_token
                self.session.cookies['csrftoken'] = csrf_token
                logger.info(f"Extracted CSRF token: {csrf_token[:20]}...")

            logger.info(f"Making transcript request: {cache_buster_url}")
            response = self.session.get(cache_buster_url, headers=headers, timeout=config.REQUEST_TIMEOUT)
            logger.info(f"Transcript response status code: {response.status_code}")
            response.raise_for_status()
            
            transcript_data, fiscal_info = self._parse_html_transcript(response.text)
            
            if transcript_data:
                # è§£ææ—¥æœŸ
                parsed_date = self.parse_date_text(fiscal_info.get('date'))

                # ä¿å­˜æ–‡ä»¶ (å¯é€‰)
                txt_filename = None
                html_filename = None
                if SAVE_TRANSCRIPT_FILES:
                    txt_filename = self._save_transcript_as_txt(
                        transcript_data,
                        fiscal_info,
                        f"transcript_{ticker}_FY{fiscal_info.get('fiscal_year', year)}_Q{quarter_num}.txt"
                    )
                    html_filename = self._save_raw_html(response.text, ticker, fiscal_info, quarter_num)
                
                # è¿”å›ç»“æ„åŒ–æ•°æ®
                return {
                    'ticker': ticker,
                    'year': year,
                    'quarter': quarter_num,
                    'date': parsed_date,
                    'content': transcript_data.get('content', ''),
                    'txt_filename': txt_filename,
                    'html_filename': html_filename,
                    'parsed_successfully': True
                }
            else:
                logger.error(f"æœªèƒ½ä»HTMLè§£æè´¢æŠ¥æ•°æ®: {url_path}")
                return None
            
        except Exception as e:
            logger.error(f"è·å–è´¢æŠ¥ä¼šè®®è®°å½•æ—¶å‡ºé”™: {e} from {url_path}")
            raise DataRetrievalError(f"è·å–è´¢æŠ¥ä¼šè®®è®°å½•æ—¶å‡ºé”™: {e}")

    def _parse_html_transcript(self, html_content: str) -> Tuple[Optional[Dict], Dict]:
        """ä»HTMLå†…å®¹ä¸­è§£æç»“æ„åŒ–æ•°æ®"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            fiscal_info = {}
            
            # æŸ¥æ‰¾è´¢å¹´ä¿¡æ¯
            fiscal_span = soup.find('span', string=re.compile(r'Fiscal Year.*Quarter'))
            if fiscal_span:
                fiscal_text = fiscal_span.get_text()
                fy_match = re.search(r'Fiscal Year \(FY\) (\d+), Quarter (\d+)', fiscal_text)
                if fy_match:
                    fiscal_info['fiscal_year'] = fy_match.group(1)
                    fiscal_info['quarter'] = fy_match.group(2)
            
            # æŸ¥æ‰¾æ—¥æœŸä¿¡æ¯
            date_span = soup.find('span', class_='text-xs')
            if date_span:
                date_text = date_span.get_text().strip()
                fiscal_info['date'] = date_text
            
            # ä»textareaä¸­æå–å†…å®¹
            textarea = soup.find('textarea', id='AIInsightsContent')
            if not textarea:
                return None, fiscal_info
            
            json_content = html.unescape(textarea.get_text())
            transcript_data = json.loads(json_content)
            return transcript_data, fiscal_info
            
        except Exception as e:
            logger.error(f"è§£æHTMLæ—¶å‡ºé”™: {e}")
            return None, {}

    def _extract_speaker_content(self, transcript_content: str) -> List[Dict]:
        """ä»è®°å½•ä¸­æå–å‘è¨€äººå’Œå†…å®¹"""
        speakers_content = []
        lines = transcript_content.split('\n')
        current_speaker = None
        current_content = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            speaker_match = re.match(r'^([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s*[-:](.*)$', line)
            if speaker_match:
                if current_speaker and current_content:
                    speakers_content.append({
                        'speaker': current_speaker,
                        'content': ' '.join(current_content).strip()
                    })
                current_speaker = speaker_match.group(1).strip()
                current_content = [speaker_match.group(2).strip()] if speaker_match.group(2).strip() else []
            else:
                if current_speaker:
                    current_content.append(line)
        
        if current_speaker and current_content:
            speakers_content.append({
                'speaker': current_speaker,
                'content': ' '.join(current_content).strip()
            })
        return speakers_content

    def _save_raw_html(self, html_content: str, ticker: str, fiscal_info: Dict, quarter_num: str) -> Optional[str]:
        """ä¿å­˜åŸå§‹HTMLæ–‡ä»¶"""
        try:
            folder_name = f"{ticker}_transcripts"
            os.makedirs(folder_name, exist_ok=True)
            html_filename = f"{folder_name}/transcript_{ticker}_FY{fiscal_info.get('fiscal_year', 'UNKNOWN')}_Q{quarter_num}.html"
            with open(html_filename, 'w', encoding='utf-8') as f:
                f.write(html_content)
            logger.info(f"åŸå§‹HTMLå·²ä¿å­˜åˆ°: {html_filename}")
            return html_filename
        except Exception as e:
            logger.error(f"ä¿å­˜HTMLæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return None

    def _save_transcript_as_txt(self, transcript_data: Dict, fiscal_info: Dict, filename: str) -> Optional[str]:
        """å°†è§£æåçš„è®°å½•ä¿å­˜ä¸ºTXTæ–‡ä»¶"""
        try:
            ticker = transcript_data.get('symbol', 'UNKNOWN')
            folder_name = f"{ticker}_transcripts"
            os.makedirs(folder_name, exist_ok=True)
            txt_filename = f"{folder_name}/{filename}"
            
            with open(txt_filename, 'w', encoding='utf-8') as f:
                f.write("="*60 + "\nEARNINGS CALL TRANSCRIPT\n" + "="*60 + "\n\n")
                if fiscal_info:
                    f.write(f"Company: {transcript_data.get('symbol', 'N/A')}\n")
                    f.write(f"Fiscal Year: {fiscal_info.get('fiscal_year', 'N/A')}\n")
                    f.write(f"Quarter: {fiscal_info.get('quarter', 'N/A')}\n")
                    f.write(f"Date: {fiscal_info.get('date', 'N/A')}\n")
                    f.write(f"Transcript Date: {transcript_data.get('date', 'N/A')}\n")
                    f.write("\n" + "="*60 + "\n\n")
                
                content = transcript_data.get('content', '')
                speakers_content = self._extract_speaker_content(content)
                
                if speakers_content:
                    for item in speakers_content:
                        f.write(f"[{item['speaker']}]\n{item['content']}\n\n" + "-"*40 + "\n\n")
                else:
                    f.write("RAW TRANSCRIPT CONTENT:\n" + "-"*40 + "\n" + content)
            
            logger.info(f"Transcript saved to: {txt_filename}")
            return txt_filename
        except Exception as e:
            logger.error(f"ä¿å­˜transcript TXTæ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return None

# ä¸»ä¸šåŠ¡é€»è¾‘
class SECEarningsAnalyzer:
    """SEC & è´¢æŠ¥åˆ†æå™¨ä¸»ç±»"""
    
    def __init__(self):
        self.gemini_service = GeminiService()
        self.cache_manager = CacheManager()
        self.sec_service = SECService(self.cache_manager)
        self.hk_service = HKStockService(self.cache_manager)
        self.earnings_service = EarningsService(self.cache_manager)
        self.session_manager = SessionManager()
        self.document_manager = DocumentManager()

    def analyze_question(self, question: str, ticker: str, model_type: str) -> Tuple[str, str]:
        """åˆ†æç”¨æˆ·é—®é¢˜å¹¶ç”Ÿæˆæç¤ºè¯"""
        # è·å–å½“å‰è¯­è¨€è®¾ç½®
        language = st.session_state.get("selected_language", "English")
        
        if language == "English":
            analysis_prompt = f"""
            You are a professional financial analyst assistant, specialized in analyzing user questions and breaking them down into two key processing steps.

            User Question: {question}
            Stock Ticker: {ticker}

            Your Task:
            1. Analyze the user's question and understand its core intent
            2. Generate two high-quality prompts:
               - Document Processing Prompt: Used to extract information related to the user's question from a single document
               - Integration Prompt: Used to integrate analysis results from multiple document processing responses

            Requirements:
            - The document processing prompt must be specific and professional, capable of extracting all relevant information from a single document
            - The integration prompt must be able to integrate results from multiple documents, providing complete analysis and insights
            - Both prompts should be concise and clear, highlighting key points
            - The generated prompts should produce professional, readable analysis results
            - The user's original question MUST appear in both the document processing prompt and integration prompt
            - Always answer in English
            - **Must return only JSON format, do not include any other text or explanations.**

            Please return directly in JSON format:
            ```json
            {{
                "processing_prompt": "Document processing prompt",
                "integration_prompt": "Integration prompt"
            }}
            ```
            """
        else:  # ä¸­æ–‡
            analysis_prompt = f"""
            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆåŠ©æ‰‹ï¼Œä¸“é—¨è´Ÿè´£åˆ†æç”¨æˆ·çš„é—®é¢˜å¹¶å°†å…¶åˆ†è§£ä¸ºä¸¤ä¸ªå…³é”®çš„å¤„ç†æ­¥éª¤ã€‚

            ç”¨æˆ·é—®é¢˜: {question}
            è‚¡ç¥¨ä»£ç : {ticker}

            ä½ çš„ä»»åŠ¡ï¼š
            1. åˆ†æç”¨æˆ·çš„é—®é¢˜ï¼Œç†è§£å…¶æ ¸å¿ƒæ„å›¾
            2. ç”Ÿæˆä¸¤ä¸ªé«˜è´¨é‡çš„æç¤ºè¯ï¼š
               - å¤„ç†èµ„æ–™promptï¼šç”¨äºä»å•ä¸ªæ–‡æ¡£ä¸­æå– èˆ‡ç”¨æˆ·é—®é¢˜ ç›¸å…³çš„ä¿¡æ¯
               - ç»Ÿæ•´promptï¼šç”¨äºæ•´åˆå¤šä¸ª å¤„ç†èµ„æ–™å›ç­” çš„åˆ†æç»“æœ

            è¦æ±‚ï¼š
            - å¤„ç†èµ„æ–™promptå¿…é¡»å…·ä½“ã€ä¸“ä¸šï¼Œèƒ½å¤Ÿä»å•ä¸ªæ–‡æ¡£ä¸­æå–æ‰€æœ‰ç›¸å…³ä¿¡æ¯
            - ç»Ÿæ•´promptå¿…é¡»èƒ½å¤Ÿæ•´åˆå¤šä¸ªæ–‡æ¡£çš„ç»“æœï¼Œæä¾›å®Œæ•´çš„åˆ†æå’Œæ´å¯Ÿ
            - ä¸¤ä¸ªpromptéƒ½è¦ç®€æ´æ˜äº†ï¼Œé‡ç‚¹çªå‡º
            - ç”Ÿæˆçš„promptåº”è¯¥èƒ½å¤Ÿäº§ç”Ÿä¸“ä¸šã€æ˜“è¯»çš„åˆ†æç»“æœ
            - ç”¨æˆ¶åŸå§‹å•é¡Œ å¿…é ˆåœ¨è™•ç†è³‡æ–™prompt å’Œ çµ±æ•´prompt ä¸­éƒ½å‡ºç¾
            - **å¿…é¡»åªè¿”å›JSONæ ¼å¼ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡æœ¬æˆ–è§£é‡Šã€‚**

            è¯·ç›´æ¥è¿”å›JSONæ ¼å¼ï¼š
            ```json
            {{
                "processing_prompt": "å¤„ç†èµ„æ–™prompt",
                "integration_prompt": "ç»Ÿæ•´prompt"
            }}
            ```
            """
        
        try:
            result = self.gemini_service.call_api(analysis_prompt, model_type)
            # å°è¯•ä»Markdownä»£ç å—ä¸­æå–JSON
            match = re.search(r"```json\s*(\{.*?\})\s*```", result, re.DOTALL)
            if match:
                json_str = match.group(1)
            else:
                json_str = result # å¦‚æœæ²¡æœ‰markdownï¼Œåˆ™å‡å®šæ•´ä¸ªå­—ç¬¦ä¸²æ˜¯JSON
            
            prompt_data = json.loads(json_str)
            
            processing_prompt = prompt_data.get("processing_prompt", "")
            integration_prompt = prompt_data.get("integration_prompt", "")
            
            if not processing_prompt or not integration_prompt:
                raise ValueError("ç”Ÿæˆçš„æç¤ºè¯ä¸ºç©º")
            
            return processing_prompt, integration_prompt
            
        except (json.JSONDecodeError, ValueError) as e:
            logger.error(f"è§£æprompt JSONå¤±è´¥: {e}. æ¨¡å‹è¿”å›: {result}")
            # ä½¿ç”¨é»˜è®¤æç¤ºè¯
            if language == "English":
                processing_prompt = f"Please extract information related to '{question}' from the following document and provide detailed analysis. Keep answers concise and to the point. Start with conclusions, use appropriate emojis and markdown format. If not found, briefly state 'Not mentioned in document'. Always answer in English."
                integration_prompt = f"Please integrate all the following analysis results to answer the user's question: '{question}', and provide a complete analysis report. Keep answers concise and to the point. Start with conclusions, use appropriate emojis and markdown format. If not found, briefly state 'Not mentioned in documents'. Always answer in English."
            else:
                processing_prompt = f"è¯·ä»ä»¥ä¸‹æ–‡æ¡£ä¸­æå–ä¸'{question}'ç›¸å…³çš„ä¿¡æ¯ï¼Œå¹¶è¿›è¡Œè¯¦ç»†åˆ†æï¼Œåªå›ç­”é‡é»å°±å¥½ï¼Œè¨˜å¾—ä¸å»¢è©±ã€‚å›ç­”è¦çµè«–å…ˆèªªï¼Œå¯ä»¥é©ç•¶ä½¿ç”¨emojiï¼Œmarkdownæ ¼å¼ï¼Œå¦‚æœæ²’æ‰¾åˆ°å°±ç°¡çŸ­å›ç­”ï¼ŒèªªæœªæåŠå°±å¥½ã€‚"
                integration_prompt = f"è¯·æ•´åˆä»¥ä¸‹æ‰€æœ‰åˆ†æç»“æœï¼Œå›ç­”ç”¨æˆ·é—®é¢˜ï¼š'{question}'ï¼Œå¹¶æä¾›å®Œæ•´çš„åˆ†ææŠ¥å‘Šï¼Œåªå›ç­”é‡é»å°±å¥½ï¼Œè¨˜å¾—ä¸å»¢è©±ã€‚å›ç­”è¦çµè«–å…ˆèªªï¼Œå¯ä»¥é©ç•¶ä½¿ç”¨emojiï¼Œmarkdownæ ¼å¼ï¼Œå¦‚æœæ²’æ‰¾åˆ°å°±ç°¡çŸ­å›ç­”ï¼ŒèªªæœªæåŠå°±å¥½ã€‚"
            
            return processing_prompt, integration_prompt
    
    def process_document(self, document: Document, processing_prompt: str, model_type: str) -> str:
        """å¤„ç†å•ä¸ªæ–‡æ¡£"""
        try:
            # è·å–å½“å‰è¯­è¨€è®¾ç½®
            language = st.session_state.get("selected_language", "English")
            
            # å¦‚æœæ–‡æ¡£å†…å®¹ä¸ºç©ºï¼Œåˆ™ä¸‹è½½
            if not document.content:
                if document.type == 'SEC Filing':
                    # æ£€æŸ¥æ˜¯å¦ä¸º6-Kæ–‡ä»¶
                    if hasattr(document, 'form_type') and document.form_type == '6-K':
                        # 6-Kæ–‡ä»¶åº”è¯¥å·²ç»åœ¨SixKProcessorä¸­å¤„ç†è¿‡äº†
                        logger.warning(f"6-Kæ–‡ä»¶å†…å®¹ä¸ºç©ºï¼Œè¿™ä¸åº”è¯¥å‘ç”Ÿ: {document.title}")
                        document.content = "6-Kæ–‡ä»¶å†…å®¹å¤„ç†å¤±è´¥" if language == "ä¸­æ–‡" else "6-K file content processing failed"
                    else:
                        # æ™®é€šSECæ–‡ä»¶å¤„ç†
                        document.content = self.sec_service.download_filing(document.url)
                elif document.type == 'HK Stock Filing':
                    # æ¸¯è‚¡æ–‡ä»¶å¤„ç†
                    document.content = self.hk_service.download_hk_filing(document.url)
                elif document.type == 'Earnings Call':
                    # åœ¨æ–°çš„æµç¨‹ä¸­ï¼Œå†…å®¹å·²é¢„å…ˆè·å–
                    logger.warning(f"å¤„ç†æ–‡æ¡£æ—¶å‘ç°è´¢æŠ¥è®°å½•å†…å®¹ä¸ºç©º: {document.title}")
                    document.content = "å†…å®¹æœªæ‰¾åˆ°" if language == "ä¸­æ–‡" else "Content not found"
            
            # å‡†å¤‡prompt - æ ¹æ®è¯­è¨€é€‰æ‹©
            if language == "English":
                prompt = f"""
                You are a professional document analyst, specialized in extracting and analyzing information from financial documents.

                Document Title: {document.title}
                Document Date: {document.date}
                Document Type: {document.type}
                
                Processing Requirements: {processing_prompt}
                Also answer similar requirements, don't miss anything
                
                Requirements:
                - Carefully read the provided document content
                - Extract relevant information according to the user's specific requirements
                - Provide accurate, professional analysis
                - Ensure answers come from document content, don't imagine
                - I don't have time to read, ensure answers are direct and to the point, no need for polite conversation
                - Always answer in English
                - when markdown output, Escape all dollar signs $ for currency as \$ to prevent Markdown from rendering them as math.
                
                Answer Requirements:
                - Start with ğŸ“ emoji, followed by what type of document this is and its purpose, 
                - second line Start with ğŸ’¡ on next new line row, directly state conclusions, answer conclusions related to my processing requirements, all in short sentences
                - Please provide structured analysis results, only answer key points, remember no nonsense.
                - First sentence should state key points without pleasantries. Don't say "According to the document content you provided..." such nonsense, directly state key points
                - Answer should start with conclusions, can use emojis to help users read, markdown format
                - If the document doesn't contain information related to my question, just say "Not mentioned in document" period, one sentence only, no nonsense, I don't have time to read

                Document Content:
                {document.content}
                """
            else:  # ä¸­æ–‡
                prompt = f"""
                ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ†æå¸ˆï¼Œä¸“é—¨è´Ÿè´£ä»è´¢åŠ¡æ–‡æ¡£ä¸­æå–å’Œåˆ†æä¿¡æ¯ã€‚

                æ–‡æ¡£æ ‡é¢˜: {document.title}
                æ–‡æ¡£æ—¥æœŸ: {document.date}
                æ–‡æ¡£ç±»å‹: {document.type}
                
                å¤„ç†è¦æ±‚: {processing_prompt}
                æœ‰èˆ‡ä»¥ä¸Šè¦æ±‚é¡ä¼¼çš„ä¹Ÿä¸€èµ·å›ç­”ï¼Œä¸è¦æ¼æ‰
                
                è¦æ±‚ï¼š
                - ä»”ç»†é˜…è¯»æä¾›çš„æ–‡æ¡£å†…å®¹
                - æ ¹æ®ç”¨æˆ·çš„å…·ä½“è¦æ±‚æå–ç›¸å…³ä¿¡æ¯
                - æä¾›å‡†ç¡®ã€ä¸“ä¸šçš„åˆ†æ
                - ç¢ºä¿å›ç­”éƒ½ä¾†è‡ªæ–‡æª”å…§å®¹ï¼Œä¸è¦æ†‘ç©ºæƒ³åƒ
                - æˆ‘æ²’æ™‚é–“çœ‹ ç¢ºä¿å›ç­”ç›´æ¥èªªé‡é» ä¸ç”¨åƒäººä¸€æ¨£é‚„è¦å®¢å¥—è©±
                - markdownè¼¸å‡ºï¼Œå°‡æ‰€æœ‰è¡¨ç¤ºé‡‘é¡çš„ $ æ”¹ç‚º \$ï¼Œä»¥é¿å… Markdown è¢«èª¤åˆ¤ç‚ºæ•¸å­¸å…¬å¼ã€‚


                
                å›ç­”è¦æ±‚ï¼š
                - é–‹é ­ä»¥ğŸ“è¿™ä¸ªemojiå¼€é ­ï¼Œ ğŸ“å¾Œé¢æ¥é€™æ˜¯ä¸€ä»½ä»€éº¼æ–‡ä»¶ï¼Œæ–‡ä»¶ç›®çš„æ˜¯ä»€éº¼ï¼Œ
                - ç¬¬äºŒå¥ä¸‹ä¸€è¡Œï¼Œé–‹é ­ä»¥ ğŸ’¡ï¼Œè¨˜å¾—æ›è¡Œï¼Œç›´æ¥èªªçµè«–ï¼Œå›ç­”è·Ÿæˆ‘å¤„ç†è¦æ±‚æœ‰é—œçš„çµè«– éƒ½æ˜¯ç°¡çŸ­ä¸€å¥è©±
                - è¯·æä¾›ç»“æ„åŒ–çš„åˆ†æç»“æœï¼Œåªå›ç­”é‡é»å°±å¥½ï¼Œè¨˜å¾—ä¸å»¢è©±ã€‚
                - ç¬¬ä¸€å¥å°±èªªé‡é»ä¸ç”¨å®¢å¥—ã€‚ ä¸ç”¨èªª æ ¹æ®æ‚¨æä¾›çš„æ–‡æ¡£å†…å®¹... é€™ç¨®å»¢è©±ï¼Œç›´æ¥èªªé‡é»
                - å›ç­”è¦çµè«–å…ˆèªªï¼Œå¯ä»¥ä½¿ç”¨emojiå¹«åŠ©ä½¿ç”¨è€…é–±è®€ï¼Œmarkdownæ ¼å¼
                - å¦‚æœæ–‡æª”å…§æ²’æœ‰è·Ÿæˆ‘çš„å•é¡Œæœ‰é—œçš„è³‡è¨Šï¼Œå°±èªªä¸€å¥ æ–‡æª”å…§æœªæåŠ å¥è™Ÿ ä¸€å¥è©±å°±å¥½  ä¸å‡†å»¢è©± æˆ‘æ²’æ™‚é–“çœ‹

                æ–‡æ¡£å†…å®¹:
                {document.content}
                """
            
            logger.info("================================================")
            logger.info(f"Processing document: {document.title} in {language}")
            
            return self.gemini_service.call_api(prompt, model_type)
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡æ¡£å¤±è´¥: {e}")
            error_msg = f"å¤„ç†æ–‡æ¡£æ—¶å‡ºé”™: {e}" if language == "ä¸­æ–‡" else f"Error processing document: {e}"
            return error_msg
    
    def process_document_stream(self, document: Document, processing_prompt: str, model_type: str):
        """å¤„ç†å•ä¸ªæ–‡æ¡£ - æµå¼å“åº”ç‰ˆæœ¬"""
        try:
            # è·å–å½“å‰è¯­è¨€è®¾ç½®
            language = st.session_state.get("selected_language", "English")
            
            # å¦‚æœæ–‡æ¡£å†…å®¹ä¸ºç©ºï¼Œåˆ™ä¸‹è½½
            if not document.content:
                if document.type == 'SEC Filing':
                    # æ£€æŸ¥æ˜¯å¦ä¸º6-Kæ–‡ä»¶
                    if hasattr(document, 'form_type') and document.form_type == '6-K':
                        # 6-Kæ–‡ä»¶åº”è¯¥å·²ç»åœ¨SixKProcessorä¸­å¤„ç†è¿‡äº†
                        logger.warning(f"6-Kæ–‡ä»¶å†…å®¹ä¸ºç©ºï¼Œè¿™ä¸åº”è¯¥å‘ç”Ÿ: {document.title}")
                        document.content = "6-Kæ–‡ä»¶å†…å®¹å¤„ç†å¤±è´¥" if language == "ä¸­æ–‡" else "6-K file content processing failed"
                    else:
                        # æ™®é€šSECæ–‡ä»¶å¤„ç†
                        document.content = self.sec_service.download_filing(document.url)
                elif document.type == 'HK Stock Filing':
                    # æ¸¯è‚¡æ–‡ä»¶å¤„ç†
                    document.content = self.hk_service.download_hk_filing(document.url)
                elif document.type == 'Earnings Call':
                    # åœ¨æ–°çš„æµç¨‹ä¸­ï¼Œå†…å®¹å·²é¢„å…ˆè·å–
                    logger.warning(f"å¤„ç†æ–‡æ¡£æ—¶å‘ç°è´¢æŠ¥è®°å½•å†…å®¹ä¸ºç©º: {document.title}")
                    document.content = "å†…å®¹æœªæ‰¾åˆ°" if language == "ä¸­æ–‡" else "Content not found"
            
            # å‡†å¤‡prompt - æ ¹æ®è¯­è¨€é€‰æ‹©
            if language == "English":
                prompt = f"""
                You are a professional document analyst, specialized in extracting and analyzing information from financial documents.

                Document Title: {document.title}
                Document Date: {document.date}
                Document Type: {document.type}
                
                Processing Requirements: {processing_prompt}
                Also answer similar requirements, don't miss anything
                
                Requirements:
                - Carefully read the provided document content
                - Extract relevant information according to the user's specific requirements
                - Provide accurate, professional analysis
                - Ensure answers come from document content, don't imagine
                - I don't have time to read, ensure answers are direct and to the point, no need for polite conversation
                - Always answer in English
                - when markdown output, Escape all dollar signs $ for currency as \$ to prevent Markdown from rendering them as math.
                
                Answer Requirements:
                - Start with ğŸ“ emoji, followed by what type of document this is and its purpose, 
                - second line Start with ğŸ’¡ on next new line row, directly state conclusions, answer conclusions related to my processing requirements, all in short sentences
                - Please provide structured analysis results, only answer key points, remember no nonsense.
                - First sentence should state key points without pleasantries. Don't say "According to the document content you provided..." such nonsense, directly state key points
                - Answer should start with conclusions, can use emojis to help users read, markdown format
                - If the document doesn't contain information related to my question, just say "Not mentioned in document" period, one sentence only, no nonsense, I don't have time to read

                Document Content:
                {document.content}
                """
            else:  # ä¸­æ–‡
                prompt = f"""
                ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ†æå¸ˆï¼Œä¸“é—¨è´Ÿè´£ä»è´¢åŠ¡æ–‡æ¡£ä¸­æå–å’Œåˆ†æä¿¡æ¯ã€‚

                æ–‡æ¡£æ ‡é¢˜: {document.title}
                æ–‡æ¡£æ—¥æœŸ: {document.date}
                æ–‡æ¡£ç±»å‹: {document.type}
                
                å¤„ç†è¦æ±‚: {processing_prompt}
                æœ‰èˆ‡ä»¥ä¸Šè¦æ±‚é¡ä¼¼çš„ä¹Ÿä¸€èµ·å›ç­”ï¼Œä¸è¦æ¼æ‰
                
                è¦æ±‚ï¼š
                - ä»”ç»†é˜…è¯»æä¾›çš„æ–‡æ¡£å†…å®¹
                - æ ¹æ®ç”¨æˆ·çš„å…·ä½“è¦æ±‚æå–ç›¸å…³ä¿¡æ¯
                - æä¾›å‡†ç¡®ã€ä¸“ä¸šçš„åˆ†æ
                - ç¢ºä¿å›ç­”éƒ½ä¾†è‡ªæ–‡æª”å…§å®¹ï¼Œä¸è¦æ†‘ç©ºæƒ³åƒ
                - æˆ‘æ²’æ™‚é–“çœ‹ ç¢ºä¿å›ç­”ç›´æ¥èªªé‡é» ä¸ç”¨åƒäººä¸€æ¨£é‚„è¦å®¢å¥—è©±
                - markdownè¼¸å‡ºï¼Œå°‡æ‰€æœ‰è¡¨ç¤ºé‡‘é¡çš„ $ æ”¹ç‚º \$ï¼Œä»¥é¿å… Markdown è¢«èª¤åˆ¤ç‚ºæ•¸å­¸å…¬å¼ã€‚


                
                å›ç­”è¦æ±‚ï¼š
                - é–‹é ­ä»¥ğŸ“è¿™ä¸ªemojiå¼€é ­ï¼Œ ğŸ“å¾Œé¢æ¥é€™æ˜¯ä¸€ä»½ä»€éº¼æ–‡ä»¶ï¼Œæ–‡ä»¶ç›®çš„æ˜¯ä»€éº¼ï¼Œ
                - ç¬¬äºŒå¥ä¸‹ä¸€è¡Œï¼Œé–‹é ­ä»¥ ğŸ’¡ï¼Œè¨˜å¾—æ›è¡Œï¼Œç›´æ¥èªªçµè«–ï¼Œå›ç­”è·Ÿæˆ‘å¤„ç†è¦æ±‚æœ‰é—œçš„çµè«– éƒ½æ˜¯ç°¡çŸ­ä¸€å¥è©±
                - è¯·æä¾›ç»“æ„åŒ–çš„åˆ†æç»“æœï¼Œåªå›ç­”é‡é»å°±å¥½ï¼Œè¨˜å¾—ä¸å»¢è©±ã€‚
                - ç¬¬ä¸€å¥å°±èªªé‡é»ä¸ç”¨å®¢å¥—ã€‚ ä¸ç”¨èªª æ ¹æ®æ‚¨æä¾›çš„æ–‡æ¡£å†…å®¹... é€™ç¨®å»¢è©±ï¼Œç›´æ¥èªªé‡é»
                - å›ç­”è¦çµè«–å…ˆèªªï¼Œå¯ä»¥ä½¿ç”¨emojiå¹«åŠ©ä½¿ç”¨è€…é–±è®€ï¼Œmarkdownæ ¼å¼
                - å¦‚æœæ–‡æª”å…§æ²’æœ‰è·Ÿæˆ‘çš„å•é¡Œæœ‰é—œçš„è³‡è¨Šï¼Œå°±èªªä¸€å¥ æ–‡æª”å…§æœªæåŠ å¥è™Ÿ ä¸€å¥è©±å°±å¥½  ä¸å‡†å»¢è©± æˆ‘æ²’æ™‚é–“çœ‹

                æ–‡æ¡£å†…å®¹:
                {document.content}
                """
            
            logger.info("================================================")
            logger.info(f"Processing document (streaming): {document.title} in {language}")
            
            # è¿”å›æµå¼å“åº”ç”Ÿæˆå™¨
            return self.gemini_service.call_api_stream(prompt, model_type)
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡æ¡£å¤±è´¥: {e}")
            error_msg = f"å¤„ç†æ–‡æ¡£æ—¶å‡ºé”™: {e}" if language == "ä¸­æ–‡" else f"Error processing document: {e}"
            # å¯¹äºé”™è¯¯ï¼Œè¿”å›ä¸€ä¸ªç®€å•çš„ç”Ÿæˆå™¨
            def error_generator():
                yield error_msg
            return error_generator()

    def integrate_results(self, document_results: List[Dict], integration_prompt: str, user_question: str, ticker: str, model_type: str) -> str:
        """æ•´åˆåˆ†æç»“æœ"""
        try:
            # è·å–å½“å‰è¯­è¨€è®¾ç½®
            language = st.session_state.get("selected_language", "English")
            
            # æ„å»ºæ•´åˆæç¤ºè¯
            if language == "English":
                integration_input = f"""
                You are a professional financial analyst, specialized in integrating analysis results from multiple documents.

                User Question: {user_question}
                Stock Ticker: {ticker}
                
                Integration Requirements: {integration_prompt}
                
                Requirements:
                - If the content contains numbers for the same indicator at different time points, place a pivot table at the very beginning of the answer. Format: pivot table row names are different indicators, column names are the time when indicators were published, cells are the indicator numbers. Then explain below the pivot table after generation.
                - If the content contains business descriptions for the same indicator at different time points, place a pivot table at the very beginning of the answer. Format: pivot table row names are different indicators, column names are the time when indicators were published, cells are the indicator descriptions. Then explain below the pivot table after generation.
                - For example: row1 would be Indicator, 2025Q1, 2025Q2. row2 would be AI commercialization, Q2 expected to resume double-digit year-over-year growth, confident in achieving significant revenue growth for full year 2025
                - table output use markdown format, ensure markdown format is correct, no errors
                - Comprehensively analyze all provided document analysis results
                - Identify trends, patterns, and key changes
                - Provide deep insights and professional recommendations
                - Use tables, lists, and other formats to enhance readability
                - Highlight key information and critical findings
                - This is a comprehensive summary, don't repeat detailed content from individual documents
                - Focus on cross-document trends and correlations
                - Always answer in English
                
                Document Analysis Results:
                """
            else:  # ä¸­æ–‡
                integration_input = f"""
                ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆï¼Œä¸“é—¨è´Ÿè´£æ•´åˆå¤šä¸ªæ–‡æ¡£çš„åˆ†æç»“æœã€‚

                ç”¨æˆ·é—®é¢˜: {user_question}
                è‚¡ç¥¨ä»£ç : {ticker}
                
                æ•´åˆè¦æ±‚: {integration_prompt}
                
                è¦æ±‚ï¼š
                - å¦‚æœå…§æ–‡æœ‰ åŒæŒ‡æ¨™ä¸åŒæ™‚é–“é»çš„ æ•¸å­—ï¼Œå›ç­”çš„æœ€ä¸€é–‹å§‹ ä¸€å®šè¦æ”¾ä¸Šä¸€å€‹pivot tableï¼Œæ ¼å¼æ˜¯ pivot table row name æ˜¯ä¸åŒæŒ‡æ¨™ ï¼Œ column æŒ‡æ¨™å…¬å¸ƒçš„æ™‚é–“ï¼Œcell æ˜¯æŒ‡æ¨™çš„æ•¸å­—ã€‚ç„¶å¾Œpivot table ç”Ÿæˆå®Œ è¡¨æ ¼ä¸‹æ–¹è§£é‡‹ä¸€ä¸‹
                - å¦‚æœå…§æ–‡æœ‰ åŒæŒ‡æ¨™ä¸åŒæ™‚é–“é»çš„ æ¥­å‹™çš„æè¿°ï¼Œå›ç­”çš„æœ€ä¸€é–‹å§‹ ä¸€å®šè¦æ”¾ä¸Šä¸€å€‹pivot tableï¼Œæ ¼å¼æ˜¯ pivot table row name æ˜¯ä¸åŒæŒ‡æ¨™ ï¼Œ column æŒ‡æ¨™å…¬å¸ƒçš„æ™‚é–“ï¼Œcell æ˜¯æŒ‡æ¨™çš„æ•¸å­—ã€‚ç„¶å¾Œpivot table ç”Ÿæˆå®Œ è¡¨æ ¼ä¸‹æ–¹è§£é‡‹ä¸€ä¸‹
                - - èˆ‰ä¾‹é¡ä¼¼åƒæ˜¯  row1æœƒæ˜¯ æŒ‡æ¨™, 2025Q1, 2025Q2 ã€‚ row2æœƒæ˜¯ AIå•†ä¸šåŒ–, Q2é¢„è®¡å°†æ¢å¤ä¸¤ä½æ•°åŒæ¯”å¢é•¿, æœ‰ä¿¡å¿ƒåœ¨2025å…¨å¹´å¹´å®ç°æ˜¾è‘—æ”¶å…¥å¢é•¿
                - table éƒ½ç”¨markdownæ ¼å¼ï¼Œè¦ç¢ºä¿markdownæ ¼å¼æ­£ç¢ºï¼Œä¸è¦æœ‰éŒ¯èª¤
                - ç»¼åˆåˆ†ææ‰€æœ‰æä¾›çš„æ–‡æ¡£åˆ†æç»“æœ
                - è¯†åˆ«è¶‹åŠ¿ã€æ¨¡å¼å’Œå…³é”®å˜åŒ–
                - æä¾›æ·±å…¥çš„æ´å¯Ÿå’Œä¸“ä¸šå»ºè®®
                - ä½¿ç”¨è¡¨æ ¼ã€åˆ—è¡¨ç­‰æ ¼å¼å¢å¼ºå¯è¯»æ€§
                - çªå‡ºé‡ç‚¹ä¿¡æ¯å’Œå…³é”®å‘ç°
                - è¿™æ˜¯ä¸€ä¸ªç»¼åˆæ€»ç»“ï¼Œä¸è¦é‡å¤å•ä¸ªæ–‡æ¡£çš„è¯¦ç»†å†…å®¹
                - é‡ç‚¹å…³æ³¨è·¨æ–‡æ¡£çš„è¶‹åŠ¿å’Œå…³è”æ€§
                
                æ–‡æ¡£åˆ†æç»“æœ:
                """
            
            for result in document_results:
                integration_input += f"""
                
                === {result['title']} ({result['date']}) ===
                {result['analysis']}
                """
            
            completion_text = "Please provide a complete, professional comprehensive analysis report and summary." if language == "English" else "è¯·æä¾›å®Œæ•´ã€ä¸“ä¸šçš„ç»¼åˆåˆ†ææŠ¥å‘Šå’Œæ€»ç»“ã€‚"
            integration_input += f"\n\n{completion_text}"
            
            return self.gemini_service.call_api(integration_input, model_type)
            
        except Exception as e:
            logger.error(f"æ•´åˆç»“æœå¤±è´¥: {e}")
            error_msg = f"æ•´åˆç»“æœæ—¶å‡ºé”™: {e}" if st.session_state.get("selected_language", "English") == "ä¸­æ–‡" else f"Error integrating results: {e}"
            return error_msg
    
    def integrate_results_stream(self, document_results: List[Dict], integration_prompt: str, user_question: str, ticker: str, model_type: str):
        """æ•´åˆåˆ†æç»“æœ - æµå¼å“åº”ç‰ˆæœ¬"""
        try:
            # è·å–å½“å‰è¯­è¨€è®¾ç½®
            language = st.session_state.get("selected_language", "English")
            
            # æ„å»ºæ•´åˆæç¤ºè¯
            if language == "English":
                integration_input = f"""
                You are a professional financial analyst, specialized in integrating analysis results from multiple documents.

                User Question: {user_question}
                Stock Ticker: {ticker}
                
                Integration Requirements: {integration_prompt}
                
                Requirements:
                - If the content contains numbers for the same indicator at different time points, place a pivot table at the very beginning of the answer. Format: pivot table row names are different indicators, column names are the time when indicators were published, cells are the indicator numbers. Then explain below the pivot table after generation.
                - If the content contains business descriptions for the same indicator at different time points, place a pivot table at the very beginning of the answer. Format: pivot table row names are different indicators, column names are the time when indicators were published, cells are the indicator descriptions. Then explain below the pivot table after generation.
                - For example: row1 would be Indicator, 2025Q1, 2025Q2. row2 would be AI commercialization, Q2 expected to resume double-digit year-over-year growth, confident in achieving significant revenue growth for full year 2025
                - table output use markdown format, ensure markdown format is correct, no errors
                - Comprehensively analyze all provided document analysis results
                - Identify trends, patterns, and key changes
                - Provide deep insights and professional recommendations
                - Use tables, lists, and other formats to enhance readability
                - Highlight key information and critical findings
                - This is a comprehensive summary, don't repeat detailed content from individual documents
                - Focus on cross-document trends and correlations
                - Always answer in English
                
                Document Analysis Results:
                """
            else:  # ä¸­æ–‡
                integration_input = f"""
                ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆï¼Œä¸“é—¨è´Ÿè´£æ•´åˆå¤šä¸ªæ–‡æ¡£çš„åˆ†æç»“æœã€‚

                ç”¨æˆ·é—®é¢˜: {user_question}
                è‚¡ç¥¨ä»£ç : {ticker}
                
                æ•´åˆè¦æ±‚: {integration_prompt}
                
                è¦æ±‚ï¼š
                - å¦‚æœå…§æ–‡æœ‰ åŒæŒ‡æ¨™ä¸åŒæ™‚é–“é»çš„ æ•¸å­—ï¼Œå›ç­”çš„æœ€ä¸€é–‹å§‹ ä¸€å®šè¦æ”¾ä¸Šä¸€å€‹pivot tableï¼Œæ ¼å¼æ˜¯ pivot table row name æ˜¯ä¸åŒæŒ‡æ¨™ ï¼Œ column æŒ‡æ¨™å…¬å¸ƒçš„æ™‚é–“ï¼Œcell æ˜¯æŒ‡æ¨™çš„æ•¸å­—ã€‚ç„¶å¾Œpivot table ç”Ÿæˆå®Œ è¡¨æ ¼ä¸‹æ–¹è§£é‡‹ä¸€ä¸‹
                - å¦‚æœå…§æ–‡æœ‰ åŒæŒ‡æ¨™ä¸åŒæ™‚é–“é»çš„ æ¥­å‹™çš„æè¿°ï¼Œå›ç­”çš„æœ€ä¸€é–‹å§‹ ä¸€å®šè¦æ”¾ä¸Šä¸€å€‹pivot tableï¼Œæ ¼å¼æ˜¯ pivot table row name æ˜¯ä¸åŒæŒ‡æ¨™ ï¼Œ column æŒ‡æ¨™å…¬å¸ƒçš„æ™‚é–“ï¼Œcell æ˜¯æŒ‡æ¨™çš„æ•¸å­—ã€‚ç„¶å¾Œpivot table ç”Ÿæˆå®Œ è¡¨æ ¼ä¸‹æ–¹è§£é‡‹ä¸€ä¸‹
                - - èˆ‰ä¾‹é¡ä¼¼åƒæ˜¯  row1æœƒæ˜¯ æŒ‡æ¨™, 2025Q1, 2025Q2 ã€‚ row2æœƒæ˜¯ AIå•†ä¸šåŒ–, Q2é¢„è®¡å°†æ¢å¤ä¸¤ä½æ•°åŒæ¯”å¢é•¿, æœ‰ä¿¡å¿ƒåœ¨2025å…¨å¹´å¹´å®ç°æ˜¾è‘—æ”¶å…¥å¢é•¿
                - table éƒ½ç”¨markdownæ ¼å¼ï¼Œè¦ç¢ºä¿markdownæ ¼å¼æ­£ç¢ºï¼Œä¸è¦æœ‰éŒ¯èª¤
                - ç»¼åˆåˆ†ææ‰€æœ‰æä¾›çš„æ–‡æ¡£åˆ†æç»“æœ
                - è¯†åˆ«è¶‹åŠ¿ã€æ¨¡å¼å’Œå…³é”®å˜åŒ–
                - æä¾›æ·±å…¥çš„æ´å¯Ÿå’Œä¸“ä¸šå»ºè®®
                - ä½¿ç”¨è¡¨æ ¼ã€åˆ—è¡¨ç­‰æ ¼å¼å¢å¼ºå¯è¯»æ€§
                - çªå‡ºé‡ç‚¹ä¿¡æ¯å’Œå…³é”®å‘ç°
                - è¿™æ˜¯ä¸€ä¸ªç»¼åˆæ€»ç»“ï¼Œä¸è¦é‡å¤å•ä¸ªæ–‡æ¡£çš„è¯¦ç»†å†…å®¹
                - é‡ç‚¹å…³æ³¨è·¨æ–‡æ¡£çš„è¶‹åŠ¿å’Œå…³è”æ€§
                
                æ–‡æ¡£åˆ†æç»“æœ:
                """
            
            for result in document_results:
                integration_input += f"""
                
                === {result['title']} ({result['date']}) ===
                {result['analysis']}
                """
            
            completion_text = "Please provide a complete, professional comprehensive analysis report and summary." if language == "English" else "è¯·æä¾›å®Œæ•´ã€ä¸“ä¸šçš„ç»¼åˆåˆ†ææŠ¥å‘Šå’Œæ€»ç»“ã€‚"
            integration_input += f"\n\n{completion_text}"
            
            # è¿”å›æµå¼å“åº”ç”Ÿæˆå™¨
            return self.gemini_service.call_api_stream(integration_input, model_type)
            
        except Exception as e:
            logger.error(f"æ•´åˆç»“æœå¤±è´¥: {e}")
            error_msg = f"æ•´åˆç»“æœæ—¶å‡ºé”™: {e}" if st.session_state.get("selected_language", "English") == "ä¸­æ–‡" else f"Error integrating results: {e}"
            # å¯¹äºé”™è¯¯ï¼Œè¿”å›ä¸€ä¸ªç®€å•çš„ç”Ÿæˆå™¨
            def error_generator():
                yield error_msg
            return error_generator()

# åˆå§‹åŒ–åº”ç”¨
@st.cache_resource
def initialize_app():
    """åˆå§‹åŒ–åº”ç”¨"""
    return SECEarningsAnalyzer()

# ä¸»é¡µé¢
def main():
    """ä¸»é¡µé¢å‡½æ•°"""
    # åœ¨åº”ç”¨åˆå§‹åŒ–ä¹‹å‰ï¼Œç¡®ä¿session stateå·²ç»å­˜åœ¨
    SessionManager.init_session_state()
    
    # è™•ç†URLåƒæ•¸
    query_params = st.query_params
    if "p" in query_params:
        param_value = query_params["p"]
        if param_value.lower() == st.secrets["ACCESS_CODE"].lower():
            st.session_state.use_premium_api = True
            st.session_state.premium_access_code = param_value
            st.success("ğŸ‰ å·²å•Ÿç”¨ä»˜è²»APIæœå‹™ï¼")

    # åˆå§‹åŒ–åº”ç”¨
    analyzer = initialize_app()
    
    # è·å–å½“å‰è¯­è¨€è®¾ç½®
    current_language = st.session_state.get("selected_language", "English")
    lang_config = LANGUAGE_CONFIG[current_language]
    
    # é¡µé¢æ ‡é¢˜
    st.title(lang_config["title"])

    # å·¦ä¾§è¾¹æ 
    with st.sidebar:
        st.header(lang_config["sidebar_header"])
        
        # Tickerè¾“å…¥
        ticker_input = st.text_input(
            lang_config["ticker_label"],
            value=st.session_state.analyzer_ticker,
            placeholder=lang_config["ticker_placeholder"]
        )
        
        # æ™ºèƒ½å¤„ç†tickeræ ¼å¼
        if ticker_input:
            if is_hk_stock(ticker_input):
                ticker = normalize_hk_ticker(ticker_input)
                st.info(lang_config["hk_stock_info"].format(ticker))
            else:
                ticker = ticker_input.upper()
                st.info(lang_config["us_stock_info"])
        else:
            ticker = ""
        
        # å¹´ä»½é€‰æ‹©
        years = st.number_input(
            lang_config["years_label"],
            min_value=1,
            max_value=10,
            value=st.session_state.analyzer_years,
            step=1
        )
        
        # æ•°æ®ç±»å‹é€‰æ‹© - æ ¹æ®è‚¡ç¥¨ç±»å‹æ˜¾ç¤ºä¸åŒé€‰é¡¹
        st.subheader(lang_config["data_type_header"])
        
        if is_hk_stock(ticker):
            # æ¸¯è‚¡é€‰é¡¹
            use_sec_reports = st.checkbox(lang_config["sec_reports_hk"], value=st.session_state.analyzer_use_sec_reports)
            use_sec_others = st.checkbox(lang_config["sec_others_hk"], value=st.session_state.analyzer_use_sec_others)
            
            use_earnings = st.checkbox(lang_config["earnings_label"], value=st.session_state.analyzer_use_earnings)
            st.caption(lang_config["earnings_caption"])
        else:
            # ç¾è‚¡é€‰é¡¹
            use_sec_reports = st.checkbox(lang_config["sec_reports_us"], value=st.session_state.analyzer_use_sec_reports)
            use_sec_others = st.checkbox(lang_config["sec_others_us"], value=st.session_state.analyzer_use_sec_others)
            
            use_earnings = st.checkbox(lang_config["earnings_label"], value=st.session_state.analyzer_use_earnings)
            st.caption(lang_config["earnings_caption"])
        
        # æ¨¡å‹é€‰æ‹©
        st.subheader(lang_config["model_header"])
        model_type = st.selectbox(
            lang_config["model_label"],
            list(config.MODELS.keys()),
            index=list(config.MODELS.keys()).index(st.session_state.analyzer_model),
            format_func=lambda x: config.MODELS[x]
        )
        
        # ä»˜è²»APIè¨­ç½®
        st.subheader(lang_config["api_header"])
        
        # è¼¸å…¥æ¡†
        access_code = st.text_input(
            lang_config["access_code_label"],
            value=st.session_state.get("premium_access_code", ""),
            placeholder=lang_config["access_code_placeholder"]
        )
        
        # é¡¯ç¤ºç•¶å‰ç‹€æ…‹
        if st.session_state.get("use_premium_api", False):
            st.success(lang_config["premium_enabled"])
        else:
            st.info(lang_config["free_api"])
        
        # æª¢æŸ¥è¼¸å…¥
        if access_code:
            if access_code.lower() == st.secrets["ACCESS_CODE"].lower():
                if not st.session_state.get("use_premium_api", False):
                    st.session_state.use_premium_api = True
                    st.session_state.premium_access_code = access_code
                    st.success(lang_config["premium_success"])
                    st.rerun()
            else:
                st.error(lang_config["access_code_error"])
        
        # è¯­è¨€é€‰æ‹©
        st.subheader(lang_config["language_header"])
        selected_language = st.selectbox(
            lang_config["language_label"],
            options=["English", "ä¸­æ–‡"],
            index=0 if st.session_state.get("selected_language", "English") == "English" else 1
        )
        
        # å¦‚æœè¯­è¨€æ”¹å˜ï¼Œæ›´æ–°session stateå¹¶é‡æ–°è¿è¡Œ
        if selected_language != st.session_state.get("selected_language", "English"):
            st.session_state.selected_language = selected_language
            st.rerun()
        
        # æ›´æ–°session state
        st.session_state.analyzer_ticker = ticker
        st.session_state.analyzer_years = years
        st.session_state.analyzer_use_sec_reports = use_sec_reports
        st.session_state.analyzer_use_sec_others = use_sec_others
        st.session_state.analyzer_use_earnings = use_earnings
        st.session_state.analyzer_model = model_type
    
    # ä¸»å†…å®¹åŒºåŸŸ
    # æ˜¾ç¤ºå†å²ç»Ÿè®¡ç»“æœ
    if "analysis_results" in st.session_state and st.session_state.analysis_results:
        st.subheader("ğŸ“Š åˆ†æç»“æœ")
        
        # ä½¿ç”¨pandasåˆ›å»ºDataFrameä»¥è·å¾—æ›´å¥½çš„æ˜¾ç¤ºæ•ˆæœ
        df = pd.DataFrame(st.session_state.analysis_results)
        
        # è®¡ç®—æ€»è®¡
        total_word_count = df['word_count'].sum()
        total_token_count = df['token_count'].sum()
        
        st.dataframe(df, use_container_width=True)
        
        st.metric(label="æ€»å­—æ•°", value=f"{total_word_count:,}")
        st.metric(label="æ€»Tokenæ•°", value=f"{total_token_count:,}")
    
    # åˆ†ææ§åˆ¶åŒºåŸŸ
    if st.button("ğŸ” å¼€å§‹åˆ†æ", disabled=not ticker):
        # å¯åŠ¨å¤„ç†æµç¨‹
        status = analyzer.session_manager.get_processing_status()
        status.is_processing = True
        status.processing_step = 1
        status.stop_requested = False
        analyzer.session_manager.update_processing_status(status)
        
        st.rerun()

    # åœ¨æ¯æ¬¡é‡æ–°è¿è¡Œè„šæœ¬æ—¶ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦å¤„ç†
    status = analyzer.session_manager.get_processing_status()
    if status.is_processing:
        # å¦‚æœæ­£åœ¨å¤„ç†ï¼Œæ˜¾ç¤ºstatus
        current_step = status.current_status_label or (lang_config.get("processing_status", "Processing..."))
        
        with st.expander(status.current_status_label, expanded=True):
            st.markdown(f"**{status.current_status_label}**")
            
            if status.total_documents > 0:
                progress_text = lang_config["progress_text"].format(status.completed_documents, status.total_documents)
                st.progress(status.progress_percentage / 100, text=progress_text)
            
            # åœæ­¢æŒ‰é’®
            if st.button(lang_config["stop_button"], key="stop_processing"):
                status.stop_requested = True
                status.is_processing = False
                status.current_status_label = lang_config["stop_success"]
                analyzer.session_manager.update_processing_status(status)
                st.rerun()
            
            # æ˜¾ç¤ºæ–‡æ¡£åˆ—è¡¨å’Œå¤„ç†çŠ¶æ€
            if status.documents:
                st.markdown("---")
                for idx, doc in enumerate(status.documents):
                    if idx < status.completed_documents:
                        status_icon = "âœ…"
                    elif idx == status.completed_documents:
                        status_icon = "ğŸ”„"
                    else:
                        status_icon = "â³"
                    
                    doc_title = doc.title
                    if len(doc_title) > 80:
                        doc_title = doc_title[:77] + "..."
                    
                    st.markdown(f"{status_icon} {doc_title} ({doc.date})")
            
            # æ˜¾ç¤ºé”™è¯¯æ¶ˆæ¯
            if status.error_message:
                st.error(f"âŒ {status.error_message}")
        
        # è¿è¡Œæ–‡æ¡£ç»Ÿè®¡æµç¨‹
        process_and_count_documents(
            analyzer, ticker, years, 
            st.session_state.analyzer_use_sec_reports,
            st.session_state.analyzer_use_sec_others,
            use_earnings, model_type
        )

def process_and_count_documents(analyzer: SECEarningsAnalyzer, ticker: str, years: int, use_sec_reports: bool, use_sec_others: bool, use_earnings: bool, model_type: str):
    """å¤„ç†å¹¶ç»Ÿè®¡æ–‡æ¡£çš„å®Œæ•´æµç¨‹"""
    status = analyzer.session_manager.get_processing_status()
    language = st.session_state.get("selected_language", "English")
    
    if status.stop_requested:
        return
    
    try:
        # æ­¥éª¤1ï¼šè·å–æ–‡æ¡£
        if status.processing_step == 1:
            st.session_state.analysis_results = [] # æ¸…ç©ºæ—§ç»“æœ
            
                if language == "English":
                status.current_status_label = "ğŸ“‚ Retrieving documents..."
                status.add_status_message("ğŸ” Started document retrieval")
                    else:
                status.current_status_label = "ğŸ“‚ æ­£åœ¨è·å–æ–‡æ¡£..."
                status.add_status_message("ğŸ” å¼€å§‹è·å–æ–‡æ¡£")
            
                analyzer.session_manager.update_processing_status(status)
                
                all_docs = []
                REPORTS_FORMS = ['10-K', '10-Q', '20-F', '6-K', '424B4']
                OTHER_FORMS = ['8-K', 'S-8', 'DEF 14A', 'F-3']
                selected_forms = []
            if use_sec_reports: selected_forms.extend(REPORTS_FORMS)
            if use_sec_others: selected_forms.extend(OTHER_FORMS)

                if selected_forms:
                    if is_hk_stock(ticker):
                        hk_forms = []
                    if any(form in REPORTS_FORMS for form in selected_forms): hk_forms.append('quarterly_annual')
                    if any(form in OTHER_FORMS for form in selected_forms): hk_forms.append('others')
                    all_docs.extend(analyzer.hk_service.get_hk_filings(ticker, years, hk_forms))
                    else:
                    all_docs.extend(analyzer.sec_service.get_filings(ticker, years, selected_forms))
            
                if use_earnings:
                    all_earnings_urls = analyzer.earnings_service.get_available_quarters(ticker)
                    current_year = datetime.now().year
                cutoff_date = datetime(current_year - years + 1, 1, 1).date()
                
                earnings_docs = []
                for url_path in all_earnings_urls:
                    if status.stop_requested: break
                    transcript_info = analyzer.earnings_service.get_earnings_transcript(url_path)
                    if transcript_info and transcript_info['date'] and transcript_info['date'] >= cutoff_date:
                        earnings_docs.append(Document(
                                                    type='Earnings Call',
                                                    title=f"{transcript_info['ticker']} {transcript_info['year']} Q{transcript_info['quarter']} Earnings Call",
                            date=transcript_info['date'], url=url_path, content=transcript_info.get('content')
                        ))
                    elif transcript_info and transcript_info['date']:
                        break # æ—¥æœŸè¿‡æ—©ï¼Œåœæ­¢
                all_docs.extend(earnings_docs)
                
                all_docs.sort(key=lambda x: x.date, reverse=True)
                status.documents = all_docs
            status.update_progress(0, len(all_docs), "æ–‡æ¡£è·å–å®Œæˆ")
            status.processing_step = 2
                analyzer.session_manager.update_processing_status(status)
            st.rerun()

        # æ­¥éª¤2ï¼šä¸‹è½½ã€è®¡æ•°å¹¶æ˜¾ç¤ºç»“æœ
        elif status.processing_step == 2:
            if status.stop_requested: return
                
            docs_to_process = status.documents
            analysis_results = []
            
            for idx, doc in enumerate(docs_to_process):
                if status.stop_requested: break

                status.update_progress(idx, len(docs_to_process), f"æ­£åœ¨å¤„ç† {idx+1}/{len(docs_to_process)}")
                analyzer.session_manager.update_processing_status(status)
                
                                if not doc.content:
                                    if doc.type == 'SEC Filing':
                                        doc.content = analyzer.sec_service.download_filing(doc.url)
                    elif doc.type == 'HK Stock Filing':
                        doc.content = analyzer.hk_service.download_hk_filing(doc.url)
                
                word_count = len(doc.content.split())
                token_count = analyzer.gemini_service.count_tokens(doc.content, model_type)

                analysis_results.append({
                    "document_title": doc.title,
                    "date": doc.date.strftime("%Y-%m-%d"),
                    "word_count": word_count,
                    "token_count": token_count,
                    "url": doc.url
                })
                
                status.completed_documents = idx + 1

            st.session_state.analysis_results = analysis_results
            status.is_processing = False
            status.current_status_label = "âœ… åˆ†æå®Œæˆï¼"
                    analyzer.session_manager.update_processing_status(status)
            st.rerun()

    except Exception as e:
        logger.error(f"å¤„ç†æµç¨‹å‡ºé”™: {e}", exc_info=True)
        status.error_message = str(e)
        status.is_processing = False
        analyzer.session_manager.update_processing_status(status)
        st.rerun()

if __name__ == "__main__":
    main() 