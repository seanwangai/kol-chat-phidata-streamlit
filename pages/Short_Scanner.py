"""
SEC & è´¢æŠ¥ä¼šè®®è®°å½•åˆ†æå¸ˆ - Productionçº§åˆ«
ä¸“ä¸šçš„è´¢åŠ¡æ•°æ®ç»¼åˆåˆ†æå¹³å°

Features:
- SECæ–‡ä»¶è·å–å’Œåˆ†æ
- è´¢æŠ¥ä¼šè®®è®°å½•è·å–å’Œåˆ†æ
- AIé©±åŠ¨çš„æ–‡æ¡£å¤„ç†
- å®æ—¶è¿›åº¦è·Ÿè¸ª
- é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
- ç¼“å­˜ä¼˜åŒ–
- åšç©ºä¿¡å·è‡ªåŠ¨æ£€æµ‹ç³»ç»Ÿ
"""

import streamlit as st
import os
import json
import requests
import warnings
import logging
import time
import hashlib
import tempfile
import uuid
import fitz  # PyMuPDF for PDF processing
import shutil
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass, field, fields
from functools import lru_cache
from contextlib import contextmanager
import re
import html
from urllib.parse import urljoin
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from abc import ABC, abstractmethod
from enum import Enum

# é…ç½®é€‰é¡¹ï¼šæ˜¯å¦ä¿å­˜transcriptæ–‡ä»¶åˆ°ç£ç›˜
SAVE_TRANSCRIPT_FILES = os.getenv("SAVE_TRANSCRIPT_FILES", "false").lower() == "true"

# ç¬¬ä¸‰æ–¹åº“
from sec_edgar_api import EdgarClient
from bs4 import BeautifulSoup, XMLParsedAsHTMLWarning
import httpx
from google import genai
from google.genai import types
from itertools import cycle

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="Short Signal Scanner",
    page_icon="ğŸ¯",
    layout="wide"
)

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# åšç©ºä¿¡å·æ£€æµ‹ç›¸å…³çš„æ•°æ®ç±»
@dataclass
class ShortSignal:
    """åšç©ºä¿¡å·æ•°æ®ç±»"""
    signal_type: str
    severity: str  # "High", "Medium", "Low"
    confidence: float  # 0-1
    title: str
    description: str
    evidence: str
    recommendation: str
    source_documents: List[str]
    detected_at: datetime
    
class SignalSeverity(Enum):
    HIGH = "High"
    MEDIUM = "Medium"
    LOW = "Low"

@dataclass
class DetectionResult:
    """æ£€æµ‹ç»“æœæ•°æ®ç±»"""
    detector_name: str
    signals: List[ShortSignal]
    processing_time: float
    success: bool
    error_message: Optional[str] = None
    analyzed_documents: List[str] = field(default_factory=list)

# åšç©ºä¿¡å·æ£€æµ‹å™¨åŸºç±»
class ShortDetector(ABC):
    """åšç©ºä¿¡å·æ£€æµ‹å™¨åŸºç±»"""
    
    def __init__(self, name_zh: str, name_en: str, description_zh: str, description_en: str, priority: int = 50):
        self.name_zh = name_zh
        self.name_en = name_en
        self.description_zh = description_zh
        self.description_en = description_en
        self.priority = priority  # ä¼˜å…ˆçº§ï¼Œæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜
        self.gemini_service = None
    
    @property
    def name(self) -> str:
        """æ ¹æ®å½“å‰è¯­è¨€è¿”å›åç§°"""
        language = st.session_state.get("selected_language", "English")
        return self.name_zh if language == "ä¸­æ–‡" else self.name_en
    
    @property
    def description(self) -> str:
        """æ ¹æ®å½“å‰è¯­è¨€è¿”å›æè¿°"""
        language = st.session_state.get("selected_language", "English")
        return self.description_zh if language == "ä¸­æ–‡" else self.description_en
        
    def set_gemini_service(self, service):
        """è®¾ç½®GeminiæœåŠ¡"""
        self.gemini_service = service
    
    @abstractmethod
    def detect(self, documents: List, model_type: str) -> DetectionResult:
        """æ£€æµ‹åšç©ºä¿¡å·"""
        pass
    
    @abstractmethod
    def get_analysis_prompt(self, documents: List) -> str:
        """è·å–åˆ†ææç¤ºè¯"""
        pass
    
    def parse_ai_response(self, response: str) -> List[ShortSignal]:
        """è§£æAIå“åº”ä¸ºåšç©ºä¿¡å·"""
        try:
            # å°è¯•è§£æJSONå“åº”
            match = re.search(r"```json\s*(\{.*?\})\s*```", response, re.DOTALL)
            if match:
                json_str = match.group(1)
            else:
                json_str = response.strip()
            
            # æ¸…ç†å¯èƒ½çš„è½¬ä¹‰å­—ç¬¦é—®é¢˜
            json_str = self._clean_json_string(json_str)
            
            data = json.loads(json_str)
            signals = []
            
            for signal_data in data.get("signals", []):
                # ç¡®ä¿æ‰€æœ‰å­—ç¬¦ä¸²å­—æ®µéƒ½æ˜¯å­—ç¬¦ä¸²ç±»å‹
                def safe_str(value, default=""):
                    if value is None:
                        return default
                    if isinstance(value, (dict, list)):
                        return str(value)
                    return str(value)
                
                signal = ShortSignal(
                    signal_type=safe_str(signal_data.get("signal_type"), "Unknown"),
                    severity=safe_str(signal_data.get("severity"), "Low"),
                    confidence=float(signal_data.get("confidence", 0.5)),
                    title=safe_str(signal_data.get("title")),
                    description=safe_str(signal_data.get("description")),
                    evidence=safe_str(signal_data.get("evidence")),
                    recommendation=safe_str(signal_data.get("recommendation")),
                    source_documents=signal_data.get("source_documents", []),
                    detected_at=datetime.now()
                )
                signals.append(signal)
            
            return signals
            
        except (json.JSONDecodeError, ValueError) as e:
            logger.warning(f"è§£æAIå“åº”å¤±è´¥: {e}")
            logger.warning(f"åŸå§‹å“åº”: {response[:200]}...")
            # å¦‚æœJSONè§£æå¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªé€šç”¨ä¿¡å·
            return [ShortSignal(
                signal_type=self.name,
                severity="Medium",
                confidence=0.3,
                title="æ£€æµ‹ç»“æœ",
                description=response[:500] + "..." if len(response) > 500 else response,
                evidence="AIæ£€æµ‹ç»“æœ",
                recommendation="éœ€è¦äººå·¥å®¡æ ¸",
                source_documents=[],
                detected_at=datetime.now()
            )]
    
    def _clean_json_string(self, json_str: str) -> str:
        """æ¸…ç†JSONå­—ç¬¦ä¸²ä¸­çš„é—®é¢˜è½¬ä¹‰å­—ç¬¦"""
        # ä¿®å¤å¸¸è§çš„è½¬ä¹‰å­—ç¬¦é—®é¢˜
        json_str = json_str.replace('ï¼„', '$')  # ä¿®å¤ç¾å…ƒç¬¦å·è½¬ä¹‰
        json_str = json_str.replace('\\"', '"')  # ç¡®ä¿å¼•å·æ­£ç¡®è½¬ä¹‰
        json_str = json_str.replace('\\\\', '\\')  # ä¿®å¤åŒåæ–œæ 
        
        # ç§»é™¤å¯èƒ½çš„æ§åˆ¶å­—ç¬¦
        json_str = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', json_str)
        
        return json_str

# å…·ä½“çš„æ£€æµ‹å™¨å®ç°
class AccountsReceivableDetector(ShortDetector):
    """åº”æ”¶è´¦æ¬¾å¼‚å¸¸æ£€æµ‹å™¨"""
    
    def __init__(self):
        super().__init__(
            name_zh="åº”æ”¶è´¦æ¬¾å¼‚å¸¸æ£€æµ‹",
            name_en="Accounts Receivable Anomaly Detection",
            description_zh="æ£€æµ‹åº”æ”¶è´¦æ¬¾çš„å¼‚å¸¸å˜åŠ¨ï¼Œå¦‚çªç„¶å‡å°‘ä½†è½¬ç§»åˆ°é•¿æœŸåº”æ”¶æ¬¾çš„åŸå› ",
            description_en="Detects abnormal changes in accounts receivable, such as sudden decreases while transferring to long-term receivables",
            priority=50
        )
    
    def detect(self, documents: List, model_type: str) -> DetectionResult:
        start_time = time.time()
        
        try:
            prompt = self.get_analysis_prompt(documents)
            response = self.gemini_service.call_api(prompt, model_type)
            signals = self.parse_ai_response(response)
            
            return DetectionResult(
                detector_name=self.name,
                signals=signals,
                processing_time=time.time() - start_time,
                success=True,
                analyzed_documents=[doc.title for doc in documents]
            )
            
        except Exception as e:
            logger.error(f"åº”æ”¶è´¦æ¬¾æ£€æµ‹å¤±è´¥: {e}")
            return DetectionResult(
                detector_name=self.name,
                signals=[],
                processing_time=time.time() - start_time,
                success=False,
                error_message=str(e)
            )
    
    def get_analysis_prompt(self, documents: List) -> str:
        language = st.session_state.get("selected_language", "ä¸­æ–‡")
        
        if language == "ä¸­æ–‡":
            return f"""
            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è´¢åŠ¡é€ å‡æ£€æµ‹ä¸“å®¶ï¼Œä¸“é—¨æ£€æµ‹åº”æ”¶è´¦æ¬¾çš„å¼‚å¸¸å˜åŠ¨ã€‚ä½ é‚„æ˜¯å°ˆæ¥­çš„Hedge FundåŸºé‡‘ç¶“ç†åˆ†æå¸«ï¼Œç†ŸçŸ¥å„ç¨®é‡‘èçŸ¥è­˜ï¼Œç†ŸçŸ¥å„ç¨®é‡‘èçŸ¥è­˜

            æ£€æµ‹é‡ç‚¹ï¼š
            1. åº”æ”¶è´¦æ¬¾çªç„¶å¤§å¹…å‡å°‘ï¼Œä½†åŒæ—¶é•¿æœŸåº”æ”¶æ¬¾å¢åŠ 
            2. åº”æ”¶è´¦æ¬¾å‘¨è½¬ç‡å¼‚å¸¸å˜åŒ–
            3. åº”æ”¶è´¦æ¬¾å‡å°‘çš„åŸå› è¯´æ˜æ˜¯å¦åˆç†
            4. æ˜¯å¦å­˜åœ¨å°†æµåŠ¨èµ„äº§è½¬ä¸ºéæµåŠ¨èµ„äº§çš„é€ å‡è¡Œä¸º

            è¯·ä»”ç»†åˆ†æä»¥ä¸‹æ–‡æ¡£ï¼Œå¯»æ‰¾åº”æ”¶è´¦æ¬¾ç›¸å…³çš„å¼‚å¸¸ä¿¡å·ï¼š

            æ–‡æ¡£å†…å®¹ï¼š
            {self._format_documents(documents)}

            è¯·ä»¥JSONæ ¼å¼è¿”å›æ£€æµ‹ç»“æœï¼š
            ```json
            {{
                "signals": [
                    {{
                        "signal_type": "åº”æ”¶è´¦æ¬¾å¼‚å¸¸",
                        "severity": "High/Medium/Low",
                        "confidence": 0.85,
                        "title": "ä¿¡å·æ ‡é¢˜",
                        "description": "è¯¦ç»†æè¿°å‘ç°çš„å¼‚å¸¸",
                        "evidence": "å…·ä½“è¯æ®å’Œæ•°æ®",
                        "recommendation": "å»ºè®®é‡‡å–çš„è¡ŒåŠ¨",
                        "source_documents": ["æ–‡æ¡£1", "æ–‡æ¡£2"]
                    }}
                ]
            }}
            ```
            """
        else:
            return f"""
            You are a professional financial fraud detection expert specializing in accounts receivable anomaly detection.

            Detection Focus:
            1. Sudden significant decrease in accounts receivable with simultaneous increase in long-term receivables
            2. Abnormal changes in accounts receivable turnover ratio
            3. Whether explanations for accounts receivable decreases are reasonable
            4. Evidence of fraudulent conversion of current assets to non-current assets

            Please analyze the following documents for accounts receivable related anomalies:

            Document Content:
            {self._format_documents(documents)}

            Return detection results in JSON format:
            ```json
            {{
                "signals": [
                    {{
                        "signal_type": "Accounts Receivable Anomaly",
                        "severity": "High/Medium/Low",
                        "confidence": 0.85,
                        "title": "Signal Title",
                        "description": "Detailed description of anomaly",
                        "evidence": "Specific evidence and data",
                        "recommendation": "Recommended action",
                        "source_documents": ["Document1", "Document2"]
                    }}
                ]
            }}
            ```
            """
    
    def _format_documents(self, documents: List) -> str:
        """æ ¼å¼åŒ–æ–‡æ¡£å†…å®¹"""
        formatted = ""
        for doc in documents:
            formatted += f"\n=== {doc.title} ({doc.date}) ===\n"
            formatted += f"{doc.content}\n"
        return formatted

class MarketPositionDetector(ShortDetector):
    """å¸‚åœºåœ°ä½å˜åŒ–æ£€æµ‹å™¨"""
    
    def __init__(self):
        super().__init__(
            name_zh="å¸‚åœºåœ°ä½å˜åŒ–æ£€æµ‹",
            name_en="Market Position Change Detection",
            description_zh="æ£€æµ‹å…¬å¸ä»è¡Œä¸šé¾™å¤´åœ°ä½ä¸‹æ»‘æˆ–é¢ä¸´å¼ºåŠ²ç«äº‰å¯¹æ‰‹ï¼Œé‡ç‚¹å…³æ³¨æŠ¤åŸæ²³æ˜¯å¦è¢«æ‰“ç ´",
            description_en="Detects company's decline from industry leadership or facing strong competitors, focusing on whether competitive moats are being breached",
            priority=20
        )
    
    def detect(self, documents: List, model_type: str) -> DetectionResult:
        start_time = time.time()
        
        try:
            prompt = self.get_analysis_prompt(documents)
            response = self.gemini_service.call_api(prompt, model_type)
            signals = self.parse_ai_response(response)
            
            return DetectionResult(
                detector_name=self.name,
                signals=signals,
                processing_time=time.time() - start_time,
                success=True,
                analyzed_documents=[doc.title for doc in documents]
            )
            
        except Exception as e:
            logger.error(f"å¸‚åœºåœ°ä½æ£€æµ‹å¤±è´¥: {e}")
            return DetectionResult(
                detector_name=self.name,
                signals=[],
                processing_time=time.time() - start_time,
                success=False,
                error_message=str(e)
            )
    
    def get_analysis_prompt(self, documents: List) -> str:
        language = st.session_state.get("selected_language", "ä¸­æ–‡")
        
        if language == "ä¸­æ–‡":
            return f"""
            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¡Œä¸šåˆ†æå¸ˆï¼Œä¸“é—¨æ£€æµ‹å…¬å¸å¸‚åœºåœ°ä½çš„å˜åŒ–ã€‚ä½ é‚„æ˜¯å°ˆæ¥­çš„Hedge FundåŸºé‡‘ç¶“ç†åˆ†æå¸«ï¼Œç†ŸçŸ¥å„ç¨®é‡‘èçŸ¥è­˜

            æ£€æµ‹é‡ç‚¹ï¼š
            1. å…¬å¸å¸‚åœºä»½é¢æ˜¯å¦åœ¨ä¸‹é™
            2. æ˜¯å¦å‡ºç°å¼ºåŠ²çš„ç«äº‰å¯¹æ‰‹
            3. è¡Œä¸šæ’åæ˜¯å¦ä»ç¬¬ä¸€åæ»‘è½
            4. ç«äº‰ä¼˜åŠ¿æ˜¯å¦åœ¨å‡å¼±
            5. ç®¡ç†å±‚å¯¹ç«äº‰æ€åŠ¿çš„æè¿°å˜åŒ–
            6. æŠ¤åŸæ²³æ˜¯å¦è¢«æ‰“ç ´ï¼ˆæŠ€æœ¯å£å’ã€å“ç‰Œä¼˜åŠ¿ã€è§„æ¨¡ç»æµã€ç½‘ç»œæ•ˆåº”ç­‰ï¼‰
            7. ä¼ ç»Ÿç«äº‰ä¼˜åŠ¿æ˜¯å¦è¢«æ–°æŠ€æœ¯æˆ–å•†ä¸šæ¨¡å¼é¢ è¦†
            8. earning call transcript ä¸­ï¼ŒQ&Aç’°ç¯€åˆ†æå¸«æ˜¯å¦å•äº†å…¬å¸ç«¶çˆ­å°æ‰‹ç›¸é—œçš„å•é¡Œï¼Œç„¶å¾Œå…¬å¸å›ç­”æ˜¯å¦åˆç†

            è¯·ä»”ç»†åˆ†æä»¥ä¸‹æ–‡æ¡£ï¼Œå¯»æ‰¾å¸‚åœºåœ°ä½å˜åŒ–çš„ä¿¡å·ï¼š

            æ–‡æ¡£å†…å®¹ï¼š
            {self._format_documents(documents)}

            è¯·ä»¥JSONæ ¼å¼è¿”å›æ£€æµ‹ç»“æœï¼š
            ```json
            {{
                "signals": [
                    {{
                        "signal_type": "å¸‚åœºåœ°ä½ä¸‹æ»‘",
                        "severity": "High/Medium/Low",
                        "confidence": 0.85,
                        "title": "ä¿¡å·æ ‡é¢˜",
                        "description": "è¯¦ç»†æè¿°å¸‚åœºåœ°ä½å˜åŒ–",
                        "evidence": "å…·ä½“è¯æ®å’Œæ•°æ®",
                        "recommendation": "å»ºè®®é‡‡å–çš„è¡ŒåŠ¨",
                        "source_documents": ["æ–‡æ¡£1", "æ–‡æ¡£2"]
                    }}
                ]
            }}
            ```
            """
        else:
            return f"""
            You are a professional industry analyst specializing in detecting changes in company market position.

            Detection Focus:
            1. Declining market share
            2. Emergence of strong competitors
            3. Fall from industry leadership position
            4. Weakening competitive advantages
            5. Changes in management's description of competitive landscape
            6. Breach of competitive moats (technology barriers, brand advantages, economies of scale, network effects, etc.)
            7. Traditional competitive advantages being disrupted by new technologies or business models
            8. Whether analysts asked questions about competitors in the Q&A session of the earnings call transcript.

            Please analyze the following documents for market position change signals:

            Document Content:
            {self._format_documents(documents)}

            Return detection results in JSON format:
            ```json
            {{
                "signals": [
                    {{
                        "signal_type": "Market Position Decline",
                        "severity": "High/Medium/Low",
                        "confidence": 0.85,
                        "title": "Signal Title",
                        "description": "Detailed description of position change",
                        "evidence": "Specific evidence and data",
                        "recommendation": "Recommended action",
                        "source_documents": ["Document1", "Document2"]
                    }}
                ]
            }}
            ```
            """
    
    def _format_documents(self, documents: List) -> str:
        """æ ¼å¼åŒ–æ–‡æ¡£å†…å®¹"""
        formatted = ""
        for doc in documents:
            formatted += f"\n=== {doc.title} ({doc.date}) ===\n"
            formatted += f"{doc.content}\n"
        return formatted

class InconsistencyDetector(ShortDetector):
    """å‰åä¸ä¸€è‡´æ£€æµ‹å™¨"""
    
    def __init__(self):
        super().__init__(
            name_zh="å‰åä¸ä¸€è‡´æ£€æµ‹",
            name_en="Internal Inconsistency Detection",
            description_zh="æ£€æµ‹åŒä¸€æ–‡æ¡£å†…ä¸åŒéƒ¨é—¨æè¿°ä¸ä¸€è‡´æˆ–å‰åçŸ›ç›¾",
            description_en="Detects inconsistencies or contradictions between different departments or sections within the same document",
            priority=15
        )
    
    def detect(self, documents: List, model_type: str) -> DetectionResult:
        start_time = time.time()
        
        try:
            prompt = self.get_analysis_prompt(documents)
            response = self.gemini_service.call_api(prompt, model_type)
            signals = self.parse_ai_response(response)
            
            return DetectionResult(
                detector_name=self.name,
                signals=signals,
                processing_time=time.time() - start_time,
                success=True,
                analyzed_documents=[doc.title for doc in documents]
            )
            
        except Exception as e:
            logger.error(f"å‰åä¸ä¸€è‡´æ£€æµ‹å¤±è´¥: {e}")
            return DetectionResult(
                detector_name=self.name,
                signals=[],
                processing_time=time.time() - start_time,
                success=False,
                error_message=str(e)
            )
    
    def get_analysis_prompt(self, documents: List) -> str:
        language = st.session_state.get("selected_language", "ä¸­æ–‡")
        
        if language == "ä¸­æ–‡":
            return f"""
            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è´¢åŠ¡é€ å‡æ£€æµ‹ä¸“å®¶ï¼Œä¸“é—¨æ£€æµ‹æ–‡æ¡£å†…éƒ¨çš„å‰åä¸ä¸€è‡´ã€‚ä½ é‚„æ˜¯å°ˆæ¥­çš„Hedge FundåŸºé‡‘ç¶“ç†åˆ†æå¸«ï¼Œç†ŸçŸ¥å„ç¨®é‡‘èçŸ¥è­˜

            æ£€æµ‹é‡ç‚¹ï¼š
            1. åŒä¸€æ–‡æ¡£ä¸­ åŒä¸€å€‹æ¥­å‹™ åœ¨ä¸åŒç« ç¯€å…§çš„æè¿°æ˜¯å¦ä¸€è‡´ æ˜¯å¦çŸ›ç›¾
            2. æ•°å­—ä¸æ–‡å­—æè¿°æ˜¯å¦åŒ¹é… (ç‰¹åˆ¥æ˜¯å¹´å ±ä¸åŒç« ç¯€å¯èƒ½æ˜¯ä¸åŒéƒ¨é–€å¯«çš„ï¼Œæœ‰é€ å‡çš„å…¬å¸å¦‚æœéƒ¨é–€ä¹‹é–“æ²’æœ‰é…åˆå¥½ï¼Œæœƒæœ‰å‰å¾Œä¸ä¸€è‡´)
            3. å…³é”®æŒ‡æ ‡çš„æè¿°æ˜¯å¦å‰åä¸€è‡´
            4. æ¯”å°åŒä¸€å¤©çš„è²¡å ±å’Œearning call transcriptï¼Œå°åŒä¸€ä»¶äº‹æƒ…çš„æè¿°æ˜¯å¦å­˜åœ¨çŸ›ç›¾
            5. earning call transcript æ¯”å°ç®¡ç†å±¤å‰é¢èªªçš„ï¼Œå’Œå¾Œé¢Q&Aç’°ç¯€å›ç­”çš„ï¼Œæ˜¯å¦å­˜åœ¨çŸ›ç›¾

            è¯·ä»”ç»†åˆ†æä»¥ä¸‹æ–‡æ¡£ï¼Œå¯»æ‰¾å‰åä¸ä¸€è‡´çš„ä¿¡å·ï¼š

            æ–‡æ¡£å†…å®¹ï¼š
            {self._format_documents(documents)}

            è¯·ä»¥JSONæ ¼å¼è¿”å›æ£€æµ‹ç»“æœï¼š
            ```json
            {{
                "signals": [
                    {{
                        "signal_type": "å‰åä¸ä¸€è‡´",
                        "severity": "High/Medium/Low",
                        "confidence": 0.85,
                        "title": "ä¿¡å·æ ‡é¢˜",
                        "description": "è¯¦ç»†æè¿°ä¸ä¸€è‡´ä¹‹å¤„",
                        "evidence": "å…·ä½“è¯æ®å’ŒçŸ›ç›¾ç‚¹",
                        "recommendation": "å»ºè®®é‡‡å–çš„è¡ŒåŠ¨",
                        "source_documents": ["æ–‡æ¡£1", "æ–‡æ¡£2"]
                    }}
                ]
            }}
            ```
            """
        else:
            return f"""
            You are a professional financial fraud detection expert analyst specializing in internal inconsistency detection.

            Detection Focus:
            1. Whether the descriptions of the same business within different sections of the same document are consistent or contradictory.
            2. Whether the numerical data matches the written descriptions (especially since different sections of annual reports might be written by different departmentsâ€”if a company is engaged in fraud and there's a lack of coordination between departments, inconsistencies may arise).
            3. Whether the descriptions of key metrics are internally consistent throughout the document.
            4. Compare the financial report and the earnings call transcript from the same day to check if there are contradictions in the description of the same event.
            5. Compare the managementâ€™s statements in the main part of the earnings call and their responses during the Q&A session to see if any contradictions exist.

            Please analyze the following documents for inconsistency signals:

            Document Content:
            {self._format_documents(documents)}

            Return detection results in JSON format:
            ```json
            {{
                "signals": [
                    {{
                        "signal_type": "Internal Inconsistency",
                        "severity": "High/Medium/Low",
                        "confidence": 0.85,
                        "title": "Signal Title",
                        "description": "Detailed description of inconsistency",
                        "evidence": "Specific evidence and contradictions",
                        "recommendation": "Recommended action",
                        "source_documents": ["Document1", "Document2"]
                    }}
                ]
            }}
            ```
            """
    
    def _format_documents(self, documents: List) -> str:
        """æ ¼å¼åŒ–æ–‡æ¡£å†…å®¹"""
        formatted = ""
        for doc in documents:
            formatted += f"\n=== {doc.title} ({doc.date}) ===\n"
            formatted += f"{doc.content}\n"
        return formatted

class MetricsDisclosureDetector(ShortDetector):
    """å…³é”®æŒ‡æ ‡æŠ«éœ²åœæ­¢æ£€æµ‹å™¨"""
    
    def __init__(self):
        super().__init__(
            name_zh="å…³é”®æŒ‡æ ‡æŠ«éœ²åœæ­¢æ£€æµ‹",
            name_en="Key Metrics Disclosure Cessation Detection",
            description_zh="æ£€æµ‹åŸæœ¬æŠ«éœ²çš„å…³é”®æŒ‡æ ‡çªç„¶åœæ­¢å…¬å¸ƒ",
            description_en="Detects when previously disclosed key metrics suddenly stop being reported",
            priority=25
        )
    
    def detect(self, documents: List, model_type: str) -> DetectionResult:
        start_time = time.time()
        
        try:
            prompt = self.get_analysis_prompt(documents)
            response = self.gemini_service.call_api(prompt, model_type)
            signals = self.parse_ai_response(response)
            
            return DetectionResult(
                detector_name=self.name,
                signals=signals,
                processing_time=time.time() - start_time,
                success=True,
                analyzed_documents=[doc.title for doc in documents]
            )
            
        except Exception as e:
            logger.error(f"æŒ‡æ ‡æŠ«éœ²æ£€æµ‹å¤±è´¥: {e}")
            return DetectionResult(
                detector_name=self.name,
                signals=[],
                processing_time=time.time() - start_time,
                success=False,
                error_message=str(e)
            )
    
    def get_analysis_prompt(self, documents: List) -> str:
        language = st.session_state.get("selected_language", "ä¸­æ–‡")
        
        if language == "ä¸­æ–‡":
            return f"""
            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è´¢åŠ¡åˆ†æå¸ˆï¼Œä¸“é—¨æ£€æµ‹å…³é”®æŒ‡æ ‡æŠ«éœ²çš„å˜åŒ–ã€‚ä½ é‚„æ˜¯å°ˆæ¥­çš„Hedge FundåŸºé‡‘ç¶“ç†åˆ†æå¸«ï¼Œç†ŸçŸ¥å„ç¨®é‡‘èçŸ¥è­˜

            æ£€æµ‹é‡ç‚¹ï¼š
            1. åŸæœ¬å®šæœŸæŠ«éœ²çš„å…³é”®æŒ‡æ ‡æ˜¯å¦çªç„¶åœæ­¢å…¬å¸ƒ
            2. å…³é”®è¿è¥æŒ‡æ ‡çš„æŠ«éœ²å˜åŒ–ï¼ŒåŒ…æ‹¬ä½†æ˜¯ä¸é™äºGMVã€æ´»è·ƒç”¨æˆ·ã€è®¢å•é‡ç­‰
            3. åˆ†ä¸šåŠ¡çº¿æ•°æ®çš„æŠ«éœ²å˜åŒ–
            4. å¯¹åœæ­¢æŠ«éœ²çš„è§£é‡Šæ˜¯å¦å……åˆ†

            è¯·ä»”ç»†åˆ†æä»¥ä¸‹æ–‡æ¡£ï¼Œå¯»æ‰¾æŒ‡æ ‡æŠ«éœ²åœæ­¢çš„ä¿¡å·ï¼š

            æ–‡æ¡£å†…å®¹ï¼š
            {self._format_documents(documents)}

            è¯·ä»¥JSONæ ¼å¼è¿”å›æ£€æµ‹ç»“æœï¼š
            ```json
            {{
                "signals": [
                    {{
                        "signal_type": "å…³é”®æŒ‡æ ‡æŠ«éœ²åœæ­¢",
                        "severity": "High/Medium/Low",
                        "confidence": 0.85,
                        "title": "ä¿¡å·æ ‡é¢˜",
                        "description": "è¯¦ç»†æè¿°åœæ­¢æŠ«éœ²çš„æŒ‡æ ‡",
                        "evidence": "å…·ä½“è¯æ®å’Œæ—¶é—´ç‚¹",
                        "recommendation": "å»ºè®®é‡‡å–çš„è¡ŒåŠ¨",
                        "source_documents": ["æ–‡æ¡£1", "æ–‡æ¡£2"]
                    }}
                ]
            }}
            ```
            """
        else:
            return f"""
            You are a professional financial analyst specializing in detecting changes in key metrics disclosure.

            Detection Focus:
            1. Previously disclosed key metrics suddenly stopped being reported
            2. Changes in disclosure of GMV, active users, order volume, etc.
            3. Changes in business segment data disclosure
            4. Adequacy of explanations for discontinued disclosure

            Please analyze the following documents for metrics disclosure cessation signals:

            Document Content:
            {self._format_documents(documents)}

            Return detection results in JSON format:
            ```json
            {{
                "signals": [
                    {{
                        "signal_type": "Key Metrics Disclosure Cessation",
                        "severity": "High/Medium/Low",
                        "confidence": 0.85,
                        "title": "Signal Title",
                        "description": "Detailed description of discontinued metrics",
                        "evidence": "Specific evidence and timeline",
                        "recommendation": "Recommended action",
                        "source_documents": ["Document1", "Document2"]
                    }}
                ]
            }}
            ```
            """
    
    def _format_documents(self, documents: List) -> str:
        """æ ¼å¼åŒ–æ–‡æ¡£å†…å®¹"""
        formatted = ""
        for doc in documents:
            formatted += f"\n=== {doc.title} ({doc.date}) ===\n"
            formatted += f"{doc.content}\n"
        return formatted

class EarningsCallAnalysisDetector(ShortDetector):
    """è´¢æŠ¥ä¼šè®®è®°å½•åˆ†ææ£€æµ‹å™¨"""
    
    def __init__(self):
        super().__init__(
            name_zh="è´¢æŠ¥ä¼šè®®è®°å½•åˆ†æ",
            name_en="Earnings Call Analysis",
            description_zh="åˆ†æè´¢æŠ¥ä¼šè®®è®°å½•ä¸­çš„ç®¡ç†å±‚å›ç­”è´¨é‡ã€æƒ…ç»ªå’Œæ¨¡å¼å˜åŒ–",
            description_en="Analyzes management response quality, sentiment, and pattern changes in earnings call transcripts",
            priority=30
        )
    
    def detect(self, documents: List, model_type: str) -> DetectionResult:
        start_time = time.time()
        
        try:
            # åªåˆ†æè´¢æŠ¥ä¼šè®®è®°å½•
            earnings_docs = [doc for doc in documents if doc.type == 'Earnings Call']
            if not earnings_docs:
                return DetectionResult(
                    detector_name=self.name,
                    signals=[],
                    processing_time=time.time() - start_time,
                    success=True,
                    analyzed_documents=[]
                )
            
            prompt = self.get_analysis_prompt(earnings_docs)
            response = self.gemini_service.call_api(prompt, model_type)
            signals = self.parse_ai_response(response)
            
            return DetectionResult(
                detector_name=self.name,
                signals=signals,
                processing_time=time.time() - start_time,
                success=True,
                analyzed_documents=[doc.title for doc in earnings_docs]
            )
            
        except Exception as e:
            logger.error(f"è´¢æŠ¥ä¼šè®®è®°å½•åˆ†æå¤±è´¥: {e}")
            return DetectionResult(
                detector_name=self.name,
                signals=[],
                processing_time=time.time() - start_time,
                success=False,
                error_message=str(e)
            )
    
    def get_analysis_prompt(self, documents: List) -> str:
        language = st.session_state.get("selected_language", "ä¸­æ–‡")
        
        if language == "ä¸­æ–‡":
            return f"""
            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è´¢æŠ¥ä¼šè®®è®°å½•åˆ†æä¸“å®¶ï¼Œä¸“é—¨åˆ†æç®¡ç†å±‚çš„å›ç­”è´¨é‡å’Œæ¨¡å¼ã€‚ä½ é‚„æ˜¯å°ˆæ¥­çš„Hedge FundåŸºé‡‘ç¶“ç†åˆ†æå¸«ï¼Œç†ŸçŸ¥å„ç¨®é‡‘èçŸ¥è­˜

            æ£€æµ‹é‡ç‚¹ï¼š
            1. ç®¡ç†å±‚Q&Aå›ç­”æ˜¯å¦åœ¨æ‰¾å€Ÿå£ï¼Œé¿å…æ­£é¢å›ç­”
            2. å¯¹å…·ä½“æ•°å­—çš„å›ç­”æ˜¯å¦å˜å¾—æ¨¡ç³Š (éå»éƒ½æœ‰å›ç­”æ•¸å­—ï¼Œå¾Œä¾†æ”¹æ¨¡ç³Šå›ç­”)
            3. Q&Aç¯èŠ‚é—®é¢˜æ•°é‡çš„å˜åŒ–
            4. Q&Aç¯èŠ‚ä¸åŒé«˜ç®¡å›ç­”è´¨é‡å˜åŒ– ä¸“ä¸šæ€§å’Œé€æ˜åº¦å˜åŒ–
            5 ä¸åŒç®¡ç†äººå“¡ å°ä¸åŒæ¥­å‹™çš„æè¿° æƒ…ç»ªå˜åŒ–ï¼ˆğŸ”´ ğŸŸ¡ ğŸŸ¢ï¼‰
            6. Q&Aç¯èŠ‚ä¸åŒåˆ†æå¸«çš„å•é¡Œ æƒ…ç»ªå˜åŒ–ï¼ˆğŸ”´ ğŸŸ¡ ğŸŸ¢ï¼‰
            
            å…§æ–‡éƒ½ç”¨markdownè¼¸å‡ºï¼Œå¯ä»¥ä½¿ç”¨markdown tableå°æ¯”è®ŠåŒ–

            è¯·ä»”ç»†åˆ†æä»¥ä¸‹è´¢æŠ¥ä¼šè®®è®°å½•ï¼Œå¯»æ‰¾ç®¡ç†å±‚è¡Œä¸ºå¼‚å¸¸çš„ä¿¡å·ï¼š

            æ–‡æ¡£å†…å®¹ï¼š
            {self._format_documents(documents)}

            è¯·ä»¥JSONæ ¼å¼è¿”å›æ£€æµ‹ç»“æœï¼š
            ```json
            {{
                "signals": [
                    {{
                        "signal_type": "è´¢æŠ¥ä¼šè®®è®°å½•å¼‚å¸¸",
                        "severity": "High/Medium/Low",
                        "confidence": 0.85,
                        "title": "ä¿¡å·æ ‡é¢˜",
                        "description": "è¯¦ç»†æè¿°ç®¡ç†å±‚è¡Œä¸ºå¼‚å¸¸",
                        "evidence": "å…·ä½“è¯æ®å’Œæ¨¡å¼å˜åŒ–",
                        "recommendation": "å»ºè®®é‡‡å–çš„è¡ŒåŠ¨",
                        "source_documents": ["æ–‡æ¡£1", "æ–‡æ¡£2"]
                    }}
                ]
            }}
            ```
            """
        else:
            return f"""
            You are a professional earnings call analysis expert specializing in management response quality and patterns.

            Detection Focus:
            1. Whether management's Q&A responses are making excuses to avoid giving direct answers
            2. Whether responses to specific numbers have become vague (e.g., previously gave exact figures, but later shifted to vague answers)
            3. Changes in the number of questions asked during the Q&A session
            4. Changes in the quality of responses from different executives during the Q&A â€” in terms of professionalism and transparency
            5. Emotional shifts (ğŸ”´ ğŸŸ¡ ğŸŸ¢) in how different managers describe different parts of the business
            6. Emotional shifts (ğŸ”´ ğŸŸ¡ ğŸŸ¢) in the questions asked by different analysts during the Q&A session

            å…§æ–‡éƒ½ç”¨markdownè¼¸å‡ºï¼Œå¯ä»¥ä½¿ç”¨markdown tableå°æ¯”è®ŠåŒ–
            
            Please analyze the following earnings call transcripts for management behavior anomalies:

            Document Content:
            {self._format_documents(documents)}

            Return detection results in JSON format:
            ```json
            {{
                "signals": [
                    {{
                        "signal_type": "Earnings Call Anomaly",
                        "severity": "High/Medium/Low",
                        "confidence": 0.85,
                        "title": "Signal Title",
                        "description": "Detailed description of management behavior anomaly",
                        "evidence": "Specific evidence and pattern changes",
                        "recommendation": "Recommended action",
                        "source_documents": ["Document1", "Document2"]
                    }}
                ]
            }}
            ```
            """
    
    def _format_documents(self, documents: List) -> str:
        """æ ¼å¼åŒ–æ–‡æ¡£å†…å®¹"""
        formatted = ""
        for doc in documents:
            formatted += f"\n=== {doc.title} ({doc.date}) ===\n"
            formatted += f"{doc.content}\n"
        return formatted

# åšç©ºä¿¡å·åˆ†æå™¨ä¸»ç±»
class ShortSignalAnalyzer:
    """åšç©ºä¿¡å·åˆ†æå™¨"""
    
    def __init__(self, gemini_service):
        self.gemini_service = gemini_service
        self.detectors = self._initialize_detectors()
    
    def _initialize_detectors(self) -> List[ShortDetector]:
        """åˆå§‹åŒ–æ‰€æœ‰æ£€æµ‹å™¨"""
        detectors = [
            AccountsReceivableDetector(),
            MarketPositionDetector(),
            InconsistencyDetector(),
            MetricsDisclosureDetector(),
            EarningsCallAnalysisDetector(),
        ]
        
        # è®¾ç½®GeminiæœåŠ¡
        for detector in detectors:
            detector.set_gemini_service(self.gemini_service)
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        detectors.sort(key=lambda x: x.priority)
        return detectors
    
    def get_available_detectors(self) -> List[ShortDetector]:
        """è·å–å¯ç”¨çš„æ£€æµ‹å™¨åˆ—è¡¨"""
        return self.detectors
    
    def analyze_documents(self, documents: List, selected_detector_classes: List[str], model_type: str) -> List[DetectionResult]:
        """åˆ†ææ–‡æ¡£å¹¶è¿”å›æ£€æµ‹ç»“æœ"""
        results = []
        
        for detector in self.detectors:
            if detector.__class__.__name__ in selected_detector_classes:
                logger.info(f"å¼€å§‹è¿è¡Œæ£€æµ‹å™¨: {detector.name}")
                try:
                    result = detector.detect(documents, model_type)
                    results.append(result)
                    logger.info(f"æ£€æµ‹å™¨ {detector.name} å®Œæˆï¼Œå‘ç° {len(result.signals)} ä¸ªä¿¡å·")
                    
                    # ç«‹å³æ›´æ–°session_stateï¼Œè®©ç”¨æˆ·å®æ—¶çœ‹åˆ°ç»“æœ
                    st.session_state.current_scan_results = results.copy()
                    
                except Exception as e:
                    logger.error(f"æ£€æµ‹å™¨ {detector.name} æ‰§è¡Œå¤±è´¥: {e}")
                    error_result = DetectionResult(
                        detector_name=detector.name,
                        signals=[],
                        processing_time=0,
                        success=False,
                        error_message=str(e)
                    )
                    results.append(error_result)
                    
                    # å³ä½¿å¤±è´¥ä¹Ÿè¦æ›´æ–°session_state
                    st.session_state.current_scan_results = results.copy()
        
        return results
    
    def generate_comprehensive_report(self, results: List[DetectionResult], ticker: str, model_type: str) -> str:
        """ç”Ÿæˆç»¼åˆåšç©ºä¿¡å·æŠ¥å‘Š"""
        language = st.session_state.get("selected_language", "ä¸­æ–‡")
        
        if language == "ä¸­æ–‡":
            report_prompt = f"""
            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åšç©ºåˆ†æå¸ˆï¼Œè¯·åŸºäºä»¥ä¸‹æ£€æµ‹ç»“æœç”Ÿæˆä¸€ä»½ç»¼åˆçš„åšç©ºä¿¡å·æŠ¥å‘Šã€‚

            è‚¡ç¥¨ä»£ç : {ticker}
            åˆ†ææ—¥æœŸ: {datetime.now().strftime('%Y-%m-%d')}

            æ£€æµ‹ç»“æœï¼š
            {self._format_results(results)}

            è¯·ç”Ÿæˆä¸€ä»½ä¸“ä¸šçš„åšç©ºä¿¡å·æŠ¥å‘Šï¼ŒåŒ…å«ï¼š
            1. æ‰§è¡Œæ‘˜è¦ï¼ˆæ€»ä½“é£é™©è¯„ä¼°ï¼‰
            2. é«˜é£é™©ä¿¡å·æ±‡æ€»
            3. å„æ£€æµ‹å™¨è¯¦ç»†å‘ç°
            4. ç»¼åˆé£é™©è¯„åˆ†ï¼ˆ1-100åˆ†ï¼‰
            5. åšç©ºå»ºè®®å’Œæ—¶æœº
            6. é£é™©æç¤º

            æŠ¥å‘Šè¦æ±‚ï¼š
            - ä½¿ç”¨ä¸“ä¸šçš„é‡‘èåˆ†æè¯­è¨€
            - çªå‡ºé‡ç‚¹é£é™©ä¿¡å·
            - æä¾›å…·ä½“çš„è¡ŒåŠ¨å»ºè®®
            - ä½¿ç”¨è¡¨æ ¼å’Œåˆ—è¡¨å¢å¼ºå¯è¯»æ€§
            - åŸºäºè¯æ®å¾—å‡ºç»“è®º
            - markdownè¼¸å‡ºï¼Œå°‡æ‰€æœ‰è¡¨ç¤ºé‡‘é¡çš„ $ æ”¹ç‚º ï¼„ï¼Œä»¥é¿å… Markdown è¢«èª¤åˆ¤ç‚ºæ•¸å­¸å…¬å¼ã€‚
            """
        else:
            report_prompt = f"""
            You are a professional short-selling analyst. Please generate a comprehensive short signal report based on the following detection results.

            Stock Symbol: {ticker}
            Analysis Date: {datetime.now().strftime('%Y-%m-%d')}

            Detection Results:
            {self._format_results(results)}

            Please generate a professional short signal report including:
            1. Executive Summary (Overall Risk Assessment)
            2. High-Risk Signal Summary
            3. Detailed Findings by Detector
            4. Comprehensive Risk Score (1-100)
            5. Short-selling Recommendation and Timing
            6. Risk Warnings

            Report Requirements:
            - Use professional financial analysis language
            - Highlight key risk signals
            - Provide specific action recommendations
            - Use tables and lists for readability
            - Base conclusions on evidence
            - when markdown output, Escape all dollar signs $ for currency as ï¼„ to prevent Markdown from rendering them as math.
            """
        
        return self.gemini_service.call_api(report_prompt, model_type)
    
    def _format_results(self, results: List[DetectionResult]) -> str:
        """æ ¼å¼åŒ–æ£€æµ‹ç»“æœ"""
        formatted = ""
        for result in results:
            # è½¬ä¹‰æ£€æµ‹å™¨åç§°ä¸­çš„ç¾å…ƒç¬¦å·
            detector_name = self._escape_dollars(result.detector_name)
            formatted += f"\n=== {detector_name} ===\n"
            formatted += f"æ‰§è¡ŒçŠ¶æ€: {'æˆåŠŸ' if result.success else 'å¤±è´¥'}\n"
            formatted += f"å¤„ç†æ—¶é—´: {result.processing_time:.2f}ç§’\n"
            formatted += f"å‘ç°ä¿¡å·æ•°: {len(result.signals)}\n"
            
            if result.error_message:
                # è½¬ä¹‰é”™è¯¯ä¿¡æ¯ä¸­çš„ç¾å…ƒç¬¦å·
                error_message = self._escape_dollars(result.error_message)
                formatted += f"é”™è¯¯ä¿¡æ¯: {error_message}\n"
            
            for signal in result.signals:
                # è½¬ä¹‰æ‰€æœ‰ä¿¡å·å­—æ®µä¸­çš„ç¾å…ƒç¬¦å·
                signal_type = self._escape_dollars(signal.signal_type)
                severity = self._escape_dollars(signal.severity)
                title = self._escape_dollars(signal.title)
                description = self._escape_dollars(signal.description)
                evidence = self._escape_dollars(signal.evidence)
                logger.info('--------------------------------')
                logger.info(evidence)
                recommendation = self._escape_dollars(signal.recommendation)
                
                formatted += f"\n- ä¿¡å·ç±»å‹: {signal_type}\n"
                formatted += f"  ä¸¥é‡ç¨‹åº¦: {severity}\n"
                # formatted += f"  ç½®ä¿¡åº¦: {signal.confidence:.2f}\n"
                formatted += f"  æ ‡é¢˜: {title}\n"
                formatted += f"  æè¿°: {description}\n"
                formatted += f"  è¯æ®: {evidence}\n"
                formatted += f"  å»ºè®®: {recommendation}\n"
            
            formatted += "\n"
        
        return formatted
    
    def _escape_dollars(self, text) -> str:
        """è½¬ä¹‰å­—ç¬¦ä¸²ä¸­çš„ç¾å…ƒç¬¦å·ä»¥é¿å…è¢«Markdownè§£æä¸ºæ•°å­¦å…¬å¼"""
        if not text:
            return ""
        
        # å¦‚æœæ˜¯å­—å…¸æˆ–åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
        if isinstance(text, (dict, list)):
            text = str(text)
        
        # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²ç±»å‹
        if not isinstance(text, str):
            text = str(text)
        
        # å°†ç¾å…ƒç¬¦å·ç”¨åå¼•å·åŒ…è£¹ï¼Œä½¿å…¶åœ¨Markdownä¸­è¢«æ¸²æŸ“ä¸ºè¡Œå†…ä»£ç 
        # è¿™æ ·å¯ä»¥é¿å…è¢«KaTeXè§£æä¸ºæ•°å­¦å…¬å¼
        return text.replace('$', 'ï¼„')

# è¯­è¨€é…ç½®
LANGUAGE_CONFIG = {
    "English": {
        "title": "ğŸ¯ Short Signal Scanner",
        "sidebar_header": "ğŸ“‹ Scanner Configuration",
        "ticker_label": "Ticker",
        "ticker_placeholder": "e.g., AAPL, 1024 HK",
        "years_label": "Years of Data",
        "data_type_header": "ğŸ“„ Data Type",
        "sec_reports_us": "Quarterly & Annual (10-K, 10-Q, 20-F, 6-K, 424B4)",
        "sec_others_us": "Others (8-K, S-8, DEF 14A, F-3)",
        "sec_reports_hk": "Quarterly & Annual Results",
        "sec_others_hk": "Other Announcements",
        "earnings_label": "Earnings Call Transcripts",
        "earnings_caption": "Earnings call transcripts",
        "detectors_header": "ğŸ” Detection Modules",
        "detectors_label": "Select Detectors",
        "model_header": "ğŸ¤– AI Model",
        "model_label": "Select Model",
        "api_header": "ğŸ’³ API Configuration",
        "access_code_label": "Enter Access Code",
        "access_code_placeholder": "Enter access code to enable premium API",
        "premium_enabled": "âœ… Premium API Service Enabled",
        "free_api": "â„¹ï¸ Using Free API Service",
        "access_code_error": "âŒ Invalid Access Code",
        "premium_success": "ğŸ‰ Premium API Service Enabled!",
        "language_header": "ğŸŒ Language",
        "language_label": "Select Language",
        "hk_stock_info": "ğŸ¢ Hong Kong Stock - Standardized to: {}",
        "us_stock_info": "ğŸ‡ºğŸ‡¸ US Stock",
        "scan_button": "ğŸ” Start Short Signal Scan",
        "status_header": "ğŸ“‹ STATUS",
        "stop_button": "â¹ï¸ Stop Processing",
        "progress_text": "Progress: {}/{} documents",
        "stop_success": "â¹ï¸ Processing stopped by user",
        "processing_stopped": "Processing has been stopped by user request."
    },
    "ä¸­æ–‡": {
        "title": "ğŸ¯ åšç©ºä¿¡å·æ‰«æå™¨",
        "sidebar_header": "ğŸ“‹ æ‰«æå™¨é…ç½®",
        "ticker_label": "Ticker",
        "ticker_placeholder": "e.g., AAPL, 1024 HK",
        "years_label": "æ•°æ®å¹´ä»½",
        "data_type_header": "ğŸ“„ æ•°æ®ç±»å‹",
        "sec_reports_us": "å­£æŠ¥å¹´æŠ¥ (10-K, 10-Q, 20-F, 6-K, 424B4)",
        "sec_others_us": "å…¶ä»–æ–‡ä»¶ (8-K, S-8, DEF 14A, F-3)",
        "sec_reports_hk": "å­£æŠ¥å¹´æŠ¥",
        "sec_others_hk": "å…¶ä»–å…¬å‘Š",
        "earnings_label": "è´¢æŠ¥ä¼šè®®è®°å½•",
        "earnings_caption": "è´¢æŠ¥ä¼šè®®è®°å½•",
        "detectors_header": "ğŸ” æ£€æµ‹æ¨¡å—",
        "detectors_label": "é€‰æ‹©æ£€æµ‹å™¨",
        "model_header": "ğŸ¤– AIæ¨¡å‹",
        "model_label": "é€‰æ‹©æ¨¡å‹",
        "api_header": "ğŸ’³ APIé…ç½®",
        "access_code_label": "è¾“å…¥è®¿é—®ä»£ç ",
        "access_code_placeholder": "è¾“å…¥è®¿é—®ä»£ç ä»¥å¯ç”¨é«˜çº§API",
        "premium_enabled": "âœ… é«˜çº§APIæœåŠ¡å·²å¯ç”¨",
        "free_api": "â„¹ï¸ ä½¿ç”¨å…è´¹APIæœåŠ¡",
        "access_code_error": "âŒ æ— æ•ˆè®¿é—®ä»£ç ",
        "premium_success": "ğŸ‰ é«˜çº§APIæœåŠ¡å·²å¯ç”¨ï¼",
        "language_header": "ğŸŒ è¯­è¨€",
        "language_label": "é€‰æ‹©è¯­è¨€",
        "hk_stock_info": "ğŸ¢ æ¸¯è‚¡ - å·²æ ‡å‡†åŒ–ä¸º: {}",
        "us_stock_info": "ğŸ‡ºğŸ‡¸ ç¾è‚¡",
        "scan_button": "ğŸ” å¼€å§‹åšç©ºä¿¡å·æ‰«æ",
        "status_header": "ğŸ“‹ çŠ¶æ€",
        "stop_button": "â¹ï¸ åœæ­¢å¤„ç†",
        "progress_text": "è¿›åº¦: {}/{} ä¸ªæ–‡æ¡£",
        "stop_success": "â¹ï¸ ç”¨æˆ·å·²åœæ­¢å¤„ç†",
        "processing_stopped": "å¤„ç†å·²è¢«ç”¨æˆ·åœæ­¢ã€‚"
    }
}

# å¸¸é‡é…ç½®
@dataclass
class Config:
    """åº”ç”¨é…ç½®"""
    # æ¨¡å‹é…ç½®
    MODELS: Dict[str, str] = field(default_factory=lambda: {
        "gemini-2.5-flash": "Gemini 2.5 Flash",
        "gemini-2.5-pro": "Gemini 2.5 Pro"
    })
    
    # SECé…ç½®
    SEC_USER_AGENT: str = "SEC Earnings Analyzer <analysis@example.com>"
    SEC_FORMS: List[str] = field(default_factory=lambda: [
        '10-K', '10-Q', '8-K', '20-F', '6-K', '424B4', 'DEF 14A', 'S-8'
    ])
    
    # è¯·æ±‚é…ç½®
    REQUEST_TIMEOUT: int = 30
    MAX_RETRIES: int = 3
    RETRY_DELAY: float = 1.0
    
    # å†…å®¹é™åˆ¶
    MAX_CONTENT_LENGTH: int = 900000
    
    # ç¼“å­˜é…ç½®
    CACHE_TTL: int = 3600  # 1å°æ—¶
    
    # æ—¥æœŸè§£ææ ¼å¼
    DATE_FORMATS: List[str] = field(default_factory=lambda: [
        '%B %d, %Y',    # January 1, 2023
        '%b %d, %Y',    # Jan 1, 2023
        '%Y-%m-%d',     # 2023-01-01
        '%m/%d/%Y',     # 01/01/2023
    ])
    
    # æ—¥æœŸæ¨¡å¼
    DATE_PATTERNS: List[str] = field(default_factory=lambda: [
        r'\b\w+ \d{1,2}, \d{4}\b',  # January 1, 2023
        r'\b\d{4}-\d{1,2}-\d{1,2}\b',  # 2023-01-01
        r'\b\d{1,2}/\d{1,2}/\d{4}\b',  # 01/01/2023
    ])

config = Config()

# æ•°æ®ç±»å®šä¹‰
@dataclass
class Document:
    """æ–‡æ¡£æ•°æ®ç±»"""
    type: str
    title: str
    date: datetime.date
    url: str
    content: Optional[str] = None
    form_type: Optional[str] = None
    year: Optional[int] = None
    quarter: Optional[int] = None
    temp_file_path: Optional[str] = None  # æ·»åŠ ä¸´æ—¶æ–‡ä»¶è·¯å¾„å­—æ®µ

@dataclass
class ProcessingStatus:
    """å¤„ç†çŠ¶æ€æ•°æ®ç±»"""
    is_processing: bool = False
    current_step: Optional[str] = None
    completed_documents: int = 0
    total_documents: int = 0
    document_results: List[Dict] = field(default_factory=list)
    processing_step: int = 0
    processing_prompt: str = ""
    integration_prompt: str = ""
    documents: List[Document] = field(default_factory=list)
    user_question: str = ""
    error_message: Optional[str] = None
    status_messages: List[str] = field(default_factory=list)
    current_status_label: str = ""
    progress_percentage: float = 0.0
    stop_requested: bool = False

    def __post_init__(self):
        # Ensure fields are initialized if not provided
        if self.processing_step is None:
            self.processing_step = 0
        if self.completed_documents is None:
            self.completed_documents = 0
        if self.total_documents is None:
            self.total_documents = 0
        if self.document_results is None:
            self.document_results = []
        if self.processing_prompt is None:
            self.processing_prompt = ""
        if self.integration_prompt is None:
            self.integration_prompt = ""
        if self.documents is None:
            self.documents = []
        if self.user_question is None:
            self.user_question = ""
        if self.error_message is None:
            self.error_message = None
        if self.status_messages is None:
            self.status_messages = []
        if self.current_status_label is None:
            self.current_status_label = ""
        if self.progress_percentage is None:
            self.progress_percentage = 0.0
        if self.stop_requested is None:
            self.stop_requested = False
    
    def add_status_message(self, message: str):
        """æ·»åŠ çŠ¶æ€æ¶ˆæ¯"""
        self.status_messages.append(f"â±ï¸ {datetime.now().strftime('%H:%M:%S')} - {message}")
        # åªä¿ç•™æœ€è¿‘çš„20æ¡æ¶ˆæ¯
        if len(self.status_messages) > 20:
            self.status_messages = self.status_messages[-20:]
    
    def update_progress(self, completed: int, total: int, label: str = ""):
        """æ›´æ–°è¿›åº¦"""
        self.completed_documents = completed
        self.total_documents = total
        self.progress_percentage = (completed / total) * 100 if total > 0 else 0
        self.current_status_label = label

# å¼‚å¸¸ç±»å®šä¹‰
class SECAnalyzerError(Exception):
    """SECåˆ†æå™¨åŸºç¡€å¼‚å¸¸"""
    pass

class APIError(SECAnalyzerError):
    """APIè°ƒç”¨å¼‚å¸¸"""
    pass

class DataRetrievalError(SECAnalyzerError):
    """æ•°æ®è·å–å¼‚å¸¸"""
    pass

class DateParsingError(SECAnalyzerError):
    """æ—¥æœŸè§£æå¼‚å¸¸"""
    pass

# å·¥å…·ç±»
def is_hk_stock(ticker: str) -> bool:
    """æ£€æµ‹æ˜¯å¦ä¸ºæ¸¯è‚¡ä»£ç """
    if not ticker:
        return False
    
    ticker_upper = ticker.upper().strip()
    
    # æ£€æŸ¥æ˜¯å¦ä»¥.HKç»“å°¾
    if ticker_upper.endswith('.HK'):
        return True
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯ "æ•°å­— HK" æ ¼å¼
    if ' HK' in ticker_upper:
        return True
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯çº¯æ•°å­—ï¼ˆæ¸¯è‚¡ä»£ç é€šå¸¸æ˜¯æ•°å­—ï¼‰
    ticker_clean = ticker.strip()
    if ticker_clean.isdigit():
        return True
    
    return False

def normalize_hk_ticker(ticker: str) -> str:
    """æ ‡å‡†åŒ–æ¸¯è‚¡ä»£ç ä¸º XXXX.HK æ ¼å¼ï¼Œè‡ªå‹•è£œ0æˆå››ä½æ•¸"""
    if not ticker:
        return ticker
    
    ticker_clean = ticker.strip()
    
    # å¦‚æœå·²ç»æ˜¯æ ‡å‡†æ ¼å¼ï¼Œæå–æ•¸å­—éƒ¨åˆ†è™•ç†
    if ticker_clean.upper().endswith('.HK'):
        number_part = ticker_clean.upper().replace('.HK', '').strip()
        if number_part.isdigit():
            # è£œ0æˆå››ä½æ•¸
            padded_number = number_part.zfill(4)
            return f"{padded_number}.HK"
        return ticker_clean.upper()
    
    # å¤„ç† "æ•°å­— HK" æ ¼å¼
    if ' HK' in ticker_clean.upper():
        number_part = ticker_clean.upper().replace(' HK', '').strip()
        if number_part.isdigit():
            # è£œ0æˆå››ä½æ•¸
            padded_number = number_part.zfill(4)
            return f"{padded_number}.HK"
    
    # å¤„ç†çº¯æ•°å­—
    if ticker_clean.isdigit():
        # è£œ0æˆå››ä½æ•¸
        padded_number = ticker_clean.zfill(4)
        return f"{padded_number}.HK"
    
    # å…¶ä»–æƒ…å†µè¿”å›åŸå€¼
    return ticker_clean.upper()

def clean_hk_ticker(ticker: str) -> str:
    """æ¸…ç†æ¸¯è‚¡ä»£ç ï¼Œç§»é™¤.HKåç¼€ï¼Œè¿”å›çº¯æ•°å­—"""
    normalized = normalize_hk_ticker(ticker)
    return normalized.replace('.HK', '').replace('.hk', '')

class RateLimiter:
    """APIè¯·æ±‚é™æµå™¨"""
    def __init__(self, max_calls: int = 30, window: int = 60):
        self.max_calls = max_calls
        self.window = window
        self.calls = []
    
    def wait_if_needed(self):
        """å¦‚æœéœ€è¦ï¼Œç­‰å¾…ç›´åˆ°å¯ä»¥å‘å‡ºè¯·æ±‚"""
        now = time.time()
        self.calls = [call_time for call_time in self.calls if now - call_time < self.window]
        
        if len(self.calls) >= self.max_calls:
            wait_time = self.window - (now - self.calls[0])
            if wait_time > 0:
                time.sleep(wait_time)
                self.calls = []
        
        self.calls.append(now)

class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨"""
    def __init__(self):
        # Session state is now initialized by SessionManager
        pass
    
    def get_cache_key(self, *args) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_string = "|".join(str(arg) for arg in args)
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def get(self, key: str, default=None):
        """è·å–ç¼“å­˜å€¼"""
        cache_data = st.session_state.cache.get(key)
        if cache_data:
            timestamp, value = cache_data
            if time.time() - timestamp < config.CACHE_TTL:
                return value
            else:
                # ç¼“å­˜è¿‡æœŸï¼Œåˆ é™¤
                del st.session_state.cache[key]
        return default
    
    def set(self, key: str, value: Any):
        """è®¾ç½®ç¼“å­˜å€¼"""
        st.session_state.cache[key] = (time.time(), value)
    
    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        st.session_state.cache.clear()

class DocumentManager:
    """æ–‡æ¡£ç®¡ç†å™¨ï¼Œè´Ÿè´£ä¿å­˜å’Œç®¡ç†ä¸´æ—¶æ–‡ä»¶"""
    
    def __init__(self):
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        self.temp_dir = tempfile.mkdtemp(prefix="sec_analyzer_")
        logger.info(f"åˆ›å»ºä¸´æ—¶ç›®å½•: {self.temp_dir}")
    
    def save_document_content(self, document: Document) -> str:
        """ä¿å­˜æ–‡æ¡£å†…å®¹åˆ°ä¸´æ—¶æ–‡ä»¶å¹¶è¿”å›æ–‡ä»¶è·¯å¾„"""
        try:
            # ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶å
            file_id = str(uuid.uuid4())[:8]
            safe_title = re.sub(r'[^\w\-_\.]', '_', document.title)
            filename = f"{file_id}_{safe_title}.txt"
            file_path = os.path.join(self.temp_dir, filename)
            
            # ä¿å­˜å†…å®¹
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(f"æ–‡æ¡£æ ‡é¢˜: {document.title}\n")
                f.write(f"æ–‡æ¡£ç±»å‹: {document.type}\n")
                f.write(f"æ–‡æ¡£æ—¥æœŸ: {document.date}\n")
                f.write(f"åŸå§‹URL: {document.url}\n")
                f.write("="*80 + "\n\n")
                f.write(document.content or "å†…å®¹ä¸ºç©º")
            
            document.temp_file_path = file_path
            logger.info(f"æ–‡æ¡£å·²ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶: {filename}")
            return file_path
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ–‡æ¡£å†…å®¹å¤±è´¥: {e}")
            return None
    
    def get_download_content(self, file_path: str) -> Optional[bytes]:
        """è·å–æ–‡ä»¶å†…å®¹ç”¨äºä¸‹è½½"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read().encode('utf-8')
        except Exception as e:
            logger.error(f"è¯»å–æ–‡ä»¶å†…å®¹å¤±è´¥: {e}")
            return None
    
    def cleanup(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        try:
            import shutil
            shutil.rmtree(self.temp_dir)
            logger.info(f"å·²æ¸…ç†ä¸´æ—¶ç›®å½•: {self.temp_dir}")
        except Exception as e:
            logger.error(f"æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")

class SixKProcessor:
    """6-Kæ–‡ä»¶å¤„ç†å™¨"""
    
    def __init__(self, temp_dir: str):
        self.temp_dir = temp_dir
        self.headers = {"User-Agent": config.SEC_USER_AGENT}
    
    def process_6k_filing(self, ticker: str, cik: str, filing_url: str, document: Document) -> List[Document]:
        """å¤„ç†6-Kæ–‡ä»¶ï¼Œä¸‹è½½é™„ä»¶å¹¶æå–ex99æ–‡ä»¶"""
        try:
            # ä»URLä¸­æå–accession numberï¼ˆæ— ç ´æŠ˜å·æ ¼å¼ï¼‰
            # URLæ ¼å¼ï¼šhttps://www.sec.gov/Archives/edgar/data/{cik}/{accession_no_no_dashes}/{primary_doc}
            accession_match = re.search(r'/(\d{18})/', filing_url)
            if not accession_match:
                logger.error(f"æ— æ³•ä»URLä¸­æå–accession number: {filing_url}")
                return [document]  # è¿”å›åŸæ–‡æ¡£
            
            accession_no_no_dashes = accession_match.group(1)
            # é‡æ–°æ„é€ å¸¦ç ´æŠ˜å·çš„æ ¼å¼ç”¨äºæ˜¾ç¤º
            accession_no = f"{accession_no_no_dashes[:10]}-{accession_no_no_dashes[10:12]}-{accession_no_no_dashes[12:]}"
            
            # åˆ›å»º6-Kæ–‡ä»¶ä¸“ç”¨ç›®å½•
            filing_dir = os.path.join(self.temp_dir, f"6K_{ticker}_{accession_no}")
            os.makedirs(filing_dir, exist_ok=True)
            
            logger.info(f"å¼€å§‹å¤„ç†6-Kæ–‡ä»¶: {ticker} - {accession_no}")
            
            # ä¸‹è½½æ‰€æœ‰é™„ä»¶
            attachments = self._download_6k_attachments(cik, accession_no_no_dashes, filing_dir)
            
            if not attachments:
                logger.warning(f"æœªæ‰¾åˆ°6-Ké™„ä»¶: {ticker} - {accession_no}")
                return [document]
            
            # å¤„ç†ex99æ–‡ä»¶
            ex99_documents = self._process_ex99_files(attachments, filing_dir, document, ticker)
            
            if not ex99_documents:
                logger.info(f"æœªæ‰¾åˆ°ex99æ–‡ä»¶: {ticker} - {accession_no}")
                return [document]
            
            logger.info(f"æˆåŠŸå¤„ç†6-Kæ–‡ä»¶ï¼Œç”Ÿæˆ {len(ex99_documents)} ä¸ªæ–‡æ¡£")
            return ex99_documents
            
        except Exception as e:
            logger.error(f"å¤„ç†6-Kæ–‡ä»¶å¤±è´¥: {e}")
            return [document]  # è¿”å›åŸæ–‡æ¡£
    
    def _download_6k_attachments(self, cik: str, accession_no_no_dashes: str, filing_dir: str) -> List[str]:
        """ä¸‹è½½6-Kæ–‡ä»¶çš„æ‰€æœ‰é™„ä»¶ï¼Œåªä¸‹è½½pdf/htm/htmlæ–‡ä»¶"""
        base_url = f"https://www.sec.gov/Archives/edgar/data/{cik}/{accession_no_no_dashes}/"
        index_url = base_url + "index.json"
        
        try:
            response = httpx.get(index_url, headers=self.headers, timeout=config.REQUEST_TIMEOUT)
            response.raise_for_status()
            
            index_data = response.json()
            directory = index_data.get('directory', {})
            items = directory.get('item', [])
            
            if not items:
                logger.warning(f"6-Kæ–‡ä»¶æ— é™„ä»¶åˆ—è¡¨: {index_url}")
                return []
            
            # ç­›é€‰åªéœ€è¦çš„æ–‡ä»¶ç±»å‹
            target_files = []
            for item in items:
                file_name = item.get('name', '')
                if not file_name:
                    continue
                
                # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
                ext = os.path.splitext(file_name)[1].lower()
                if ext in ['.pdf', '.htm', '.html']:
                    target_files.append(item)
                else:
                    logger.info(f"è·³è¿‡éç›®æ ‡æ–‡ä»¶ç±»å‹: {file_name}")
            
            if not target_files:
                logger.warning(f"æœªæ‰¾åˆ°pdf/htm/htmlæ–‡ä»¶")
                return []
            
            downloaded_files = []
            logger.info(f"æ‰¾åˆ° {len(target_files)} ä¸ªç›®æ ‡æ–‡ä»¶ï¼Œå¼€å§‹ä¸‹è½½...")
            
            for item in target_files:
                file_name = item.get('name', '')
                file_url = base_url + file_name
                file_path = os.path.join(filing_dir, file_name)
                
                try:
                    file_response = httpx.get(file_url, headers=self.headers, timeout=config.REQUEST_TIMEOUT)
                    file_response.raise_for_status()
                    
                    # åˆ¤æ–­æ–‡ä»¶ç±»å‹å¹¶ä¿å­˜
                    content_type = file_response.headers.get('content-type', '').lower()
                    if 'text' in content_type or 'html' in content_type or 'xml' in content_type:
                        with open(file_path, 'w', encoding='utf-8') as f:
                            f.write(file_response.text)
                    else:
                        with open(file_path, 'wb') as f:
                            f.write(file_response.content)
                    
                    downloaded_files.append(file_path)
                    logger.info(f"å·²ä¸‹è½½6-Ké™„ä»¶: {file_name}")
                    
                except Exception as e:
                    logger.warning(f"ä¸‹è½½6-Ké™„ä»¶å¤±è´¥ {file_name}: {e}")
            
            logger.info(f"æˆåŠŸä¸‹è½½ {len(downloaded_files)} ä¸ª6-Kç›®æ ‡é™„ä»¶")
            return downloaded_files
            
        except Exception as e:
            logger.error(f"è·å–6-Ké™„ä»¶åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def _process_ex99_files(self, attachments: List[str], filing_dir: str, original_doc: Document, ticker: str) -> List[Document]:
        """å¤„ç†ex99æ–‡ä»¶ï¼ŒæŒ‰ç…§è¦æ±‚åˆ†ç±»å¤„ç†HTMLå’ŒPDF"""
        ex99_files = []
        
        # æ‰¾åˆ°æ‰€æœ‰åŒ…å«_ex99çš„æ–‡ä»¶
        for file_path in attachments:
            file_name = os.path.basename(file_path).lower()
            if 'ex99' in file_name:
                ex99_files.append(file_path)
        
        if not ex99_files:
            logger.info("æœªæ‰¾åˆ°åŒ…å«_ex99çš„æ–‡ä»¶")
            return []
        
        logger.info(f"æ‰¾åˆ° {len(ex99_files)} ä¸ªex99æ–‡ä»¶: {[os.path.basename(f) for f in ex99_files]}")
        
        # åˆ†ç±»å¤„ç†ä¸åŒç±»å‹çš„æ–‡ä»¶
        html_files = []
        pdf_files = []
        
        for file_path in ex99_files:
            ext = os.path.splitext(file_path)[1].lower()
            if ext in ['.html', '.htm']:
                html_files.append(file_path)
            elif ext == '.pdf':
                pdf_files.append(file_path)
            else:
                logger.info(f"è·³è¿‡ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_path}")
        
        documents = []
        
        # å¤„ç†HTML/HTMæ–‡ä»¶ - è½¬æ¢ä¸ºmarkdownåæ£€æŸ¥æ˜¯å¦å¯ä»¥åˆå¹¶
        if html_files:
            logger.info(f"å¤„ç† {len(html_files)} ä¸ªHTMLæ–‡ä»¶")
            
            # å…ˆè½¬æ¢æ‰€æœ‰HTMLæ–‡ä»¶ä¸ºmarkdown
            html_contents = []
            for file_path in html_files:
                content = self._convert_html_to_markdown(file_path)
                if content:
                    file_name = os.path.basename(file_path)
                    html_contents.append({
                        'file_name': file_name,
                        'content': content
                    })
            
            if html_contents:
                # è®¡ç®—æ€»å­—ç¬¦æ•°
                total_chars = sum(len(item['content']) for item in html_contents)
                
                if total_chars <= config.MAX_CONTENT_LENGTH:
                    # åˆå¹¶æ‰€æœ‰HTMLå†…å®¹
                    combined_content = ""
                    for item in html_contents:
                        combined_content += f"\n\n=== {item['file_name']} ===\n\n{item['content']}"
                    
                    html_doc = Document(
                        type=original_doc.type,
                        title=f"{ticker} 6-K ex99 files",
                        date=original_doc.date,
                        url=original_doc.url,
                        content=combined_content.strip(),
                        form_type="6-K-Ex99-HTML-Combined"
                    )
                    documents.append(html_doc)
                    logger.info(f"HTMLæ–‡ä»¶å·²åˆå¹¶å¤„ç†ï¼Œæ€»å­—ç¬¦æ•°: {total_chars}")
                else:
                    # åˆ†åˆ«å¤„ç†æ¯ä¸ªHTMLæ–‡ä»¶
                    for i, item in enumerate(html_contents):
                        html_doc = Document(
                            type=original_doc.type,
                            title=f"{ticker} 6-K ex99-{i+1}",
                            date=original_doc.date,
                            url=original_doc.url,
                            content=item['content'],
                            form_type="6-K-Ex99-HTML"
                        )
                        documents.append(html_doc)
                    logger.info(f"HTMLæ–‡ä»¶åˆ†åˆ«å¤„ç†ï¼Œæ€»å­—ç¬¦æ•°è¶…è¿‡é™åˆ¶: {total_chars}")
        
        # å¤„ç†PDFæ–‡ä»¶ - å¿…é¡»åˆ†å¼€å¤„ç†
        if pdf_files:
            logger.info(f"å¤„ç† {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")
            
            for i, file_path in enumerate(pdf_files):
                content = self._convert_pdf_to_text(file_path)
                if content:
                    file_name = os.path.basename(file_path)
                    pdf_doc = Document(
                        type=original_doc.type,
                        title=f"{ticker} 6-K ex99 PDF-{i+1}",
                        date=original_doc.date,
                        url=original_doc.url,
                        content=content,
                        form_type="6-K-Ex99-PDF"
                    )
                    documents.append(pdf_doc)
                    logger.info(f"PDFæ–‡ä»¶å·²å¤„ç†: {file_name}")
                else:
                    logger.warning(f"PDFæ–‡ä»¶è½¬æ¢å¤±è´¥: {file_path}")
        
        logger.info(f"Ex99æ–‡ä»¶å¤„ç†å®Œæˆï¼Œç”Ÿæˆ {len(documents)} ä¸ªæ–‡æ¡£")
        return documents
    
    def _convert_html_to_markdown(self, file_path: str) -> str:
        """å°†HTMLè½¬æ¢ä¸ºmarkdownæ ¼å¼çš„æ–‡æœ¬"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
            
            # ä½¿ç”¨BeautifulSoupè§£æHTML
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾
            for script in soup(["script", "style"]):
                script.decompose()
            
            # å¤„ç†è¡¨æ ¼ - ä¿æŒç»“æ„
            for table in soup.find_all('table'):
                # ç®€å•çš„è¡¨æ ¼è½¬æ¢
                rows = table.find_all('tr')
                table_text = "\n"
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    if cells:
                        cell_texts = [cell.get_text(strip=True) for cell in cells]
                        table_text += "| " + " | ".join(cell_texts) + " |\n"
                table_text += "\n"
                table.replace_with(table_text)
            
            # å¤„ç†æ ‡é¢˜
            for i in range(1, 7):
                for heading in soup.find_all(f'h{i}'):
                    heading_text = heading.get_text(strip=True)
                    heading.replace_with(f"\n{'#' * i} {heading_text}\n")
            
            # å¤„ç†æ®µè½
            for p in soup.find_all('p'):
                p_text = p.get_text(strip=True)
                if p_text:
                    p.replace_with(f"\n{p_text}\n")
            
            # å¤„ç†åˆ—è¡¨
            for ul in soup.find_all('ul'):
                items = ul.find_all('li')
                list_text = "\n"
                for item in items:
                    item_text = item.get_text(strip=True)
                    if item_text:
                        list_text += f"- {item_text}\n"
                list_text += "\n"
                ul.replace_with(list_text)
            
            for ol in soup.find_all('ol'):
                items = ol.find_all('li')
                list_text = "\n"
                for i, item in enumerate(items, 1):
                    item_text = item.get_text(strip=True)
                    if item_text:
                        list_text += f"{i}. {item_text}\n"
                list_text += "\n"
                ol.replace_with(list_text)
            
            # è·å–æœ€ç»ˆæ–‡æœ¬
            text = soup.get_text()
            
            # æ¸…ç†æ–‡æœ¬
            lines = text.split('\n')
            cleaned_lines = []
            for line in lines:
                line = line.strip()
                if line:
                    cleaned_lines.append(line)
            
            # åˆå¹¶è¿ç»­çš„ç©ºè¡Œ
            final_text = '\n'.join(cleaned_lines)
            
            # å»é™¤å¤šä½™çš„ç©ºè¡Œ
            final_text = re.sub(r'\n{3,}', '\n\n', final_text)
            
            return final_text.strip()
            
        except Exception as e:
            logger.error(f"è½¬æ¢HTMLæ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return ""
    
    def _convert_pdf_to_text(self, file_path: str) -> str:
        """å°†PDFè½¬æ¢ä¸ºæ–‡æœ¬"""
        try:
            doc = fitz.open(file_path)
            text = ""
            
            for page_num in range(doc.page_count):
                page = doc[page_num]
                page_text = page.get_text()
                if page_text.strip():
                    text += f"\n--- ç¬¬ {page_num + 1} é¡µ ---\n"
                    text += page_text
                    text += "\n"
            
            doc.close()
            
            # æ¸…ç†æ–‡æœ¬
            text = re.sub(r'\n{3,}', '\n\n', text)
            return text.strip()
            
        except Exception as e:
            logger.error(f"è½¬æ¢PDFæ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return ""

# æ¸¯è‚¡æ–‡ä»¶ä¸‹è½½å™¨
class HKStockFilingsDownloader:
    """æ¸¯è‚¡å…¬å‘Šä¸‹è½½å™¨"""
    def __init__(self):
        self.base_url = "https://www1.hkexnews.hk"
        self.prefix_url = f"{self.base_url}/search/prefix.do"
        self.search_url = f"{self.base_url}/search/titlesearch.xhtml"
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,zh-TW;q=0.8,zh;q=0.7,zh-CN;q=0.6,ja;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'same-origin',
            'Sec-Fetch-User': '?1',
        }
        
        # æ–‡ä»¶ç±»å‹æ˜ å°„
        self.filing_types = {
            'Annual Report': 'å¹´æŠ¥',
            'Quarterly Results': 'å­£æŠ¥',
            'Interim Results': 'ä¸­æœŸæŠ¥å‘Š',
            'Final Results': 'å…¨å¹´ä¸šç»©',
            'Financial Statements': 'è´¢åŠ¡æŠ¥è¡¨'
        }
        
    def get_stock_id(self, ticker):
        """æ ¹æ®è‚¡ç¥¨ä»£ç è·å–è‚¡ç¥¨ID"""
        # æ¸…ç†è‚¡ç¥¨ä»£ç ï¼Œç§»é™¤.HKåç¼€
        clean_ticker = clean_hk_ticker(ticker)
        
        # æ„å»ºè¯·æ±‚URL
        timestamp = int(time.time() * 1000)
        params = {
            'callback': 'callback',
            'lang': 'EN',
            'type': 'A',
            'name': clean_ticker,
            'market': 'SEHK',
            '_': timestamp
        }
        
        # è®¾ç½®AJAXè¯·æ±‚å¤´
        ajax_headers = self.headers.copy()
        ajax_headers.update({
            'Accept': 'text/javascript, application/javascript, application/ecmascript, application/x-ecmascript, */*; q=0.01',
            'X-Requested-With': 'XMLHttpRequest',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Referer': f'{self.base_url}/search/titlesearch.xhtml?lang=en'
        })
        
        try:
            with httpx.Client(headers=ajax_headers, timeout=30) as client:
                response = client.get(self.prefix_url, params=params)
                response.raise_for_status()
                
                # è§£æJSONPå“åº”
                content = response.text
                logger.debug(f"æ¸¯è‚¡APIåŸå§‹å“åº”: {content}")
                
                # ç§»é™¤JSONPåŒ…è£…
                data = None
                if content.startswith('callback(') and content.endswith(');'):
                    json_str = content[9:-2]
                    data = json.loads(json_str)
                elif content.startswith('callback(') and content.endswith('});'):
                    json_str = content[9:-3]
                    data = json.loads(json_str)
                else:
                    # å°è¯•ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–JSONéƒ¨åˆ†
                    match = re.search(r'callback\((.*)\);?\s*$', content)
                    if match:
                        json_str = match.group(1)
                        data = json.loads(json_str)
                    else:
                        logger.error(f"æ— æ³•è§£æJSONPæ ¼å¼: {content}")
                        return None, None, None
                
                if 'stockInfo' in data and data['stockInfo']:
                    stock_info = data['stockInfo'][0]
                    stock_id = stock_info['stockId']
                    stock_code = stock_info['code']
                    stock_name = stock_info['name']
                    
                    logger.info(f"æ‰¾åˆ°æ¸¯è‚¡: {stock_code} - {stock_name} (ID: {stock_id})")
                    return stock_id, stock_code, stock_name
                else:
                    logger.warning(f"æœªæ‰¾åˆ°æ¸¯è‚¡ä»£ç  {ticker} çš„ä¿¡æ¯")
                    return None, None, None
                    
        except Exception as e:
            logger.error(f"è·å–æ¸¯è‚¡IDæ—¶å‡ºé”™: {str(e)}")
            return None, None, None
    
    def get_filings_list(self, stock_id, from_date=None, to_date=None, cutoff_date=None, status_callback=None):
        """è·å–æŒ‡å®šè‚¡ç¥¨çš„æ‰€æœ‰å…¬å‘Šåˆ—è¡¨ï¼Œæ”¯æŒè‡ªåŠ¨ç¿»é¡µ"""
        if not from_date:
            from_date = "19990401"  # é»˜è®¤ä»1999å¹´å¼€å§‹
        if not to_date:
            to_date = datetime.now().strftime("%Y%m%d")
        
        # è§£ææˆªæ­¢æ—¥æœŸ
        cutoff_datetime = None
        if cutoff_date:
            cutoff_datetime = datetime.strptime(cutoff_date, "%Y%m%d")
        
        all_filings = []
        row_range = 0  # ä»ç¬¬ä¸€é¡µå¼€å§‹
        page_size = 100  # æ¯é¡µ100æ¡è®°å½•
        total_record_count = None  # æ€»è®°å½•æ•°
        
        while True:
            # æ„å»ºGETè¯·æ±‚URLï¼ˆç¬¬ä¸€é¡µç”¨POSTï¼Œåç»­é¡µé¢ç”¨GETï¼‰
            if row_range == 0:
                # ç¬¬ä¸€é¡µä½¿ç”¨POSTè¯·æ±‚
                post_data = {
                    'lang': 'EN',
                    'category': '0',
                    'market': 'SEHK',
                    'searchType': '0',
                    'documentType': '-1',
                    't1code': '-2',
                    't2Gcode': '-2',
                    't2code': '-2',
                    'stockId': str(stock_id),
                    'from': from_date,
                    'to': to_date,
                    'MB-Daterange': '0',
                    'title': ''
                }
                
                # è®¾ç½®POSTè¯·æ±‚å¤´
                post_headers = self.headers.copy()
                post_headers.update({
                    'Content-Type': 'application/x-www-form-urlencoded',
                    'Cache-Control': 'max-age=0',
                    'Referer': f'{self.base_url}/search/titlesearch.xhtml?lang=en'
                })
                
                try:
                    with httpx.Client(headers=post_headers, timeout=30) as client:
                        response = client.post(self.search_url, data=post_data)
                        response.raise_for_status()
                        
                        page_filings = self.parse_filings_html(response.text)
                        
                except Exception as e:
                    logger.error(f"è·å–æ¸¯è‚¡å…¬å‘Šåˆ—è¡¨ç¬¬1é¡µæ—¶å‡ºé”™: {str(e)}")
                    break
            else:
                # åç»­é¡µé¢ä½¿ç”¨GETè¯·æ±‚
                get_url = f"{self.base_url}/search/titleSearchServlet.do"
                params = {
                    'sortDir': '0',
                    'sortByOptions': 'DateTime',
                    'category': '0',
                    'market': 'SEHK',
                    'stockId': str(stock_id),
                    'documentType': '-1',
                    'fromDate': from_date,
                    'toDate': to_date,
                    'title': '',
                    'searchType': '0',
                    't1code': '-2',
                    't2Gcode': '-2',
                    't2code': '-2',
                    'rowRange': str(row_range),
                    'lang': 'E'
                }
                
                # è®¾ç½®GETè¯·æ±‚å¤´
                get_headers = self.headers.copy()
                get_headers.update({
                    'accept': '*/*',
                    'x-requested-with': 'XMLHttpRequest',
                    'Referer': f'{self.base_url}/search/titlesearch.xhtml?lang=en'
                })
                
                try:
                    with httpx.Client(headers=get_headers, timeout=30) as client:
                        response = client.get(get_url, params=params)
                        response.raise_for_status()
                        
                        # è§£æJSONå“åº”
                        json_data = response.json()
                        page_filings = self.parse_filings_json(json_data.get('result', '[]'))
                        
                        # ä»JSONå“åº”ä¸­è·å–æ€»è®°å½•æ•°
                        if total_record_count is None:
                            try:
                                import json
                                result_data = json.loads(json_data.get('result', '[]'))
                                if result_data and len(result_data) > 0:
                                    total_record_count = int(result_data[0].get('TOTAL_COUNT', 0))
                                    logger.info(f"æ¸¯è‚¡å…¬å‘Šæ€»è®°å½•æ•°: {total_record_count}")
                            except (json.JSONDecodeError, ValueError, KeyError) as e:
                                logger.warning(f"æ— æ³•è§£ææ€»è®°å½•æ•°: {e}")
                        
                except Exception as e:
                    logger.error(f"è·å–æ¸¯è‚¡å…¬å‘Šåˆ—è¡¨ç¬¬{row_range//page_size + 1}é¡µæ—¶å‡ºé”™: {str(e)}")
                    break
            
            if not page_filings:
                logger.info(f"æ¸¯è‚¡å…¬å‘Šåˆ—è¡¨ç¬¬{row_range//page_size + 1}é¡µæ— æ•°æ®ï¼Œåœæ­¢ç¿»é¡µ")
                break
            
            # å…ˆå°†æœ¬é¡µæ‰€æœ‰å…¬å‘Šæ·»åŠ åˆ°ç»“æœä¸­
            all_filings.extend(page_filings)
            
            # æ£€æŸ¥æœ¬é¡µæœ€åä¸€ä¸ªå…¬å‘Šçš„æ—¥æœŸæ˜¯å¦æ—©äºæˆªæ­¢æ—¥æœŸ
            if cutoff_datetime and page_filings:
                last_filing = page_filings[-1]  # è·å–æœ€åä¸€ä¸ªå…¬å‘Š
                last_filing_date = self.parse_filing_date(last_filing.get('release_time', ''))
                if last_filing_date and last_filing_date < cutoff_datetime:
                    logger.info(f"æ¸¯è‚¡å…¬å‘Šæœ€åä¸€ä¸ªæ—¥æœŸ {last_filing_date} æ—©äºæˆªæ­¢æ—¥æœŸ {cutoff_datetime}ï¼Œåœæ­¢ç¿»é¡µ")
                    break
            
            # å¦‚æœè¿™ä¸€é¡µçš„è®°å½•æ•°å°‘äºé¡µé¢å¤§å°ï¼Œè¯´æ˜å·²ç»æ˜¯æœ€åä¸€é¡µ
            if len(page_filings) < page_size:
                logger.info(f"æ¸¯è‚¡å…¬å‘Šåˆ—è¡¨ç¬¬{row_range//page_size + 1}é¡µè®°å½•æ•° {len(page_filings)} å°‘äºé¡µé¢å¤§å°ï¼Œåœæ­¢ç¿»é¡µ")
                break
            
            # å‡†å¤‡ä¸‹ä¸€é¡µ
            row_range += page_size
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»è¶…è¿‡æ€»è®°å½•æ•°
            if total_record_count is not None and row_range >= total_record_count:
                logger.info(f"æ¸¯è‚¡å…¬å‘Šåˆ—è¡¨rowRange({row_range}) å·²è¾¾åˆ°æ€»è®°å½•æ•°({total_record_count})ï¼Œåœæ­¢ç¿»é¡µ")
                break
            
            logger.info(f"æ¸¯è‚¡å…¬å‘Šåˆ—è¡¨å‡†å¤‡è·å–ç¬¬{row_range//page_size + 1}é¡µ (rowRange={row_range})")
            if status_callback:
                status_callback(f"æ­£åœ¨è·å–ç¬¬ {row_range//page_size + 1} é¡µæ¸¯è‚¡å…¬å‘Š...")
        
        logger.info(f"æ¸¯è‚¡å…¬å‘Šåˆ—è¡¨å…±è·å– {len(all_filings)} æ¡è®°å½•")
        return all_filings
    
    def parse_filings_html(self, html_content):
        """è§£æHTMLå“åº”ï¼Œæå–å…¬å‘Šé“¾æ¥"""
        soup = BeautifulSoup(html_content, 'html.parser')
        filings = []
        
        # æŸ¥æ‰¾æ‰€æœ‰è¡¨æ ¼è¡Œ
        table_rows = soup.find_all('tr')
        
        for row in table_rows:
            # æŸ¥æ‰¾åŒ…å«PDFé“¾æ¥çš„å•å…ƒæ ¼
            doc_link_cell = row.find('div', class_='doc-link')
            if doc_link_cell:
                # è·å–PDFé“¾æ¥
                pdf_link = doc_link_cell.find('a')
                if pdf_link and pdf_link.get('href'):
                    href = pdf_link.get('href')
                    
                    # è·å–å‘å¸ƒæ—¶é—´
                    time_cell = row.find('td', class_='release-time')
                    release_time = time_cell.get_text(strip=True).replace('Release Time: ', '') if time_cell else 'N/A'
                    
                    # è·å–è‚¡ç¥¨ä»£ç 
                    code_cell = row.find('td', class_='stock-short-code')
                    stock_code = code_cell.get_text(strip=True).replace('Stock Code: ', '').split('\n')[0] if code_cell else 'N/A'
                    
                    # è·å–è‚¡ç¥¨åç§°
                    name_cell = row.find('td', class_='stock-short-name')
                    stock_name = name_cell.get_text(strip=True).replace('Stock Short Name: ', '').split('\n')[0] if name_cell else 'N/A'
                    
                    # è·å–æ–‡æ¡£æ ‡é¢˜
                    doc_title = pdf_link.get_text(strip=True)
                    
                    # è·å–æ–‡æ¡£ç±»å‹ï¼ˆä»headline divè·å–ï¼‰
                    headline_div = row.find('div', class_='headline')
                    doc_type = headline_div.get_text(strip=True) if headline_div else 'Unknown'
                    
                    # è·å–æ–‡ä»¶å¤§å°
                    filesize_span = doc_link_cell.find('span', class_='attachment_filesize')
                    filesize = filesize_span.get_text(strip=True) if filesize_span else 'N/A'
                    
                    # æ„å»ºå®Œæ•´URL
                    full_url = urljoin(self.base_url, href)
                    
                    filing_info = {
                        'release_time': release_time,
                        'stock_code': stock_code,
                        'stock_name': stock_name,
                        'doc_type': doc_type,
                        'doc_title': doc_title,
                        'file_size': filesize,
                        'url': full_url,
                        'href': href
                    }
                    
                    filings.append(filing_info)
        
        return filings
    
    def parse_filings_json(self, json_string):
        """è§£æJSONæ ¼å¼çš„å…¬å‘Šå“åº”"""
        try:
            import json
            filings_data = json.loads(json_string)
            filings = []
            
            for item in filings_data:
                filing_info = {
                    'release_time': item.get('DATE_TIME', ''),
                    'stock_code': item.get('STOCK_CODE', '').replace('<br/>', '/'),
                    'stock_name': item.get('STOCK_NAME', '').replace('<br/>', '/'),
                    'doc_type': item.get('LONG_TEXT', ''),
                    'doc_title': item.get('TITLE', ''),
                    'file_size': item.get('FILE_INFO', ''),
                    'url': f"{self.base_url}{item.get('FILE_LINK', '')}",
                    'href': item.get('FILE_LINK', '')
                }
                filings.append(filing_info)
            
            return filings
            
        except Exception as e:
            logger.error(f"è§£æJSONå…¬å‘Šæ•°æ®å¤±è´¥: {e}")
            return []
    
    def parse_filing_date(self, date_str):
        """è§£æå…¬å‘Šæ—¥æœŸå­—ç¬¦ä¸²ä¸ºdatetimeå¯¹è±¡"""
        try:
            # æ¸¯è‚¡æ—¥æœŸæ ¼å¼ï¼šDD/MM/YYYY HH:MM
            if '/' in date_str and ' ' in date_str:
                date_part = date_str.split(' ')[0]  # å–æ—¥æœŸéƒ¨åˆ†
                parts = date_part.split('/')
                if len(parts) == 3:
                    day, month, year = parts
                    return datetime(int(year), int(month), int(day))
            return None
        except Exception as e:
            logger.warning(f"è§£ææ¸¯è‚¡æ—¥æœŸå¤±è´¥: {date_str} - {e}")
            return None
    
    def categorize_filings(self, filings):
        """å°†å…¬å‘Šåˆ†ä¸ºä¸¤ç»„ï¼šå­£æŠ¥å¹´æŠ¥ç»„å’Œå…¶ä»–ç»„"""
        # å­£æŠ¥å¹´æŠ¥å…³é”®è¯ï¼ˆåŸºäºheadline divå†…å®¹ï¼‰
        quarterly_annual_keywords = [
            'Final Results',
            'Quarterly Results', 
            'Interim Results',
            'Offer for Subscription'
        ]
        
        quarterly_annual_filings = []
        other_filings = []
        
        for filing in filings:
            doc_type = filing['doc_type']
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å­£æŠ¥å¹´æŠ¥å…³é”®è¯
            is_quarterly_annual = any(keyword in doc_type for keyword in quarterly_annual_keywords)
            
            if is_quarterly_annual:
                quarterly_annual_filings.append(filing)
            else:
                other_filings.append(filing)
        
        return quarterly_annual_filings, other_filings
    
    def download_filing_content(self, filing_info):
        """ä¸‹è½½å•ä¸ªå…¬å‘Šæ–‡ä»¶å†…å®¹"""
        try:
            with httpx.Client(headers=self.headers, timeout=60) as client:
                response = client.get(filing_info['url'])
                response.raise_for_status()
                
                return response.content
                
        except Exception as e:
            logger.error(f"ä¸‹è½½æ¸¯è‚¡æ–‡ä»¶æ—¶å‡ºé”™ {filing_info['url']}: {str(e)}")
            return None

# è£…é¥°å™¨
def retry_on_failure(max_retries: int = config.MAX_RETRIES, delay: float = config.RETRY_DELAY):
    """é‡è¯•è£…é¥°å™¨"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            last_exception = None
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    logger.warning(f"å‡½æ•° {func.__name__} ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {e}")
                    if attempt < max_retries - 1:
                        time.sleep(delay * (2 ** attempt))  # æŒ‡æ•°é€€é¿
            
            logger.error(f"å‡½æ•° {func.__name__} åœ¨ {max_retries} æ¬¡å°è¯•åä»ç„¶å¤±è´¥")
            raise last_exception
        return wrapper
    return decorator

@contextmanager
def error_handler(operation_name: str):
    """é”™è¯¯å¤„ç†ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    try:
        yield
    except Exception as e:
        logger.error(f"{operation_name} å¤±è´¥: {e}")
        st.error(f"{operation_name} å¤±è´¥: {e}")
        raise

# Session State ç®¡ç†
class SessionManager:
    """SessionçŠ¶æ€ç®¡ç†å™¨"""
    
    @staticmethod
    def init_session_state():
        """åˆå§‹åŒ–æ‰€æœ‰å¿…è¦çš„session stateå˜é‡"""
        defaults = {
            "analyzer_messages": [],
            "analyzer_ticker": "",
            "analyzer_years": 2,
            "analyzer_use_sec_reports": True,
            "analyzer_use_sec_others": False,
            "analyzer_use_earnings": True,
            "analyzer_model": "gemini-2.5-flash",
            "api_key_cycle": cycle(st.secrets["GOOGLE_API_KEYS"]),
            "processing_status": ProcessingStatus().__dict__,
            "cache": {},
            "use_premium_api": False,
            "premium_access_code": "",
            "selected_language": "English"
        }
        
        for key, default_value in defaults.items():
            if key not in st.session_state:
                st.session_state[key] = default_value
    
    @staticmethod
    def get_processing_status() -> ProcessingStatus:
        """è·å–å¤„ç†çŠ¶æ€å¯¹è±¡"""
        # Get the field names from the dataclass
        known_fields = {f.name for f in fields(ProcessingStatus)}
        # Filter the session state dict to only include known fields
        current_status_dict = st.session_state.get("processing_status", {})
        filtered_args = {k: v for k, v in current_status_dict.items() if k in known_fields}
        
        return ProcessingStatus(**filtered_args)
    
    @staticmethod
    def update_processing_status(status: ProcessingStatus):
        """æ›´æ–°å¤„ç†çŠ¶æ€"""
        st.session_state.processing_status = status.__dict__

# AI æœåŠ¡
class GeminiService:
    """Gemini AIæœåŠ¡"""
    
    def __init__(self):
        self.rate_limiter = RateLimiter(max_calls=30, window=60)
    
    def get_next_api_key(self) -> str:
        """è·å–ä¸‹ä¸€ä¸ªAPIå¯†é’¥"""
        # æª¢æŸ¥æ˜¯å¦ä½¿ç”¨ä»˜è²»API
        if st.session_state.get("use_premium_api", False):
            return st.secrets["PREMIUM_API_KEY"]
        
        # ä½¿ç”¨ä¸€èˆ¬çš„è¼ªæ›API
        if hasattr(st.session_state, 'api_key_cycle'):
            return next(st.session_state.api_key_cycle)
        else:
            # å¦‚æœsession stateæœªåˆå§‹åŒ–ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
            if not hasattr(self, '_api_key_cycle'):
                api_keys = st.secrets["GOOGLE_API_KEYS"]
                self._api_key_cycle = cycle(api_keys)
            return next(self._api_key_cycle)
    
    def init_client(self) -> genai.Client:
        """åˆå§‹åŒ–Geminiå®¢æˆ·ç«¯"""
        return genai.Client(api_key=self.get_next_api_key())
    
    @retry_on_failure(max_retries=3)
    def call_api(self, prompt: str, model_type: str = "gemini-2.5-flash") -> str:
        """è°ƒç”¨Gemini API"""
        self.rate_limiter.wait_if_needed()
        
        try:
            client = self.init_client()
            
            contents = [
                types.Content(
                    role="user",
                    parts=[types.Part.from_text(text=prompt)],
                ),
            ]

            response = client.models.generate_content(
                model=model_type,
                contents=contents,
            )

            return response.candidates[0].content.parts[0].text
            
        except Exception as e:
            logger.error(f"Gemini APIè°ƒç”¨å¤±è´¥: {e}")
            raise APIError(f"Gemini APIè°ƒç”¨å¤±è´¥: {e}")
    
    @retry_on_failure(max_retries=3)
    def call_api_stream(self, prompt: str, model_type: str = "gemini-2.5-flash"):
        """è°ƒç”¨Gemini API æµå¼å“åº”"""
        self.rate_limiter.wait_if_needed()
        
        try:
            client = self.init_client()
            
            contents = [
                types.Content(
                    role="user",
                    parts=[types.Part.from_text(text=prompt)],
                ),
            ]

            # ä½¿ç”¨æµå¼å“åº”
            response_stream = client.models.generate_content_stream(
                model=model_type,
                contents=contents,
            )
            
            # ç”Ÿæˆå™¨å‡½æ•°ï¼Œé€æ­¥è¿”å›æ–‡æœ¬ç‰‡æ®µ
            for chunk in response_stream:
                if chunk.candidates and chunk.candidates[0].content.parts:
                    yield chunk.candidates[0].content.parts[0].text
            
        except Exception as e:
            logger.error(f"Gemini APIæµå¼è°ƒç”¨å¤±è´¥: {e}")
            raise APIError(f"Gemini APIæµå¼è°ƒç”¨å¤±è´¥: {e}")
    
    @retry_on_failure(max_retries=3)
    def classify_6k_document(self, document_content: str) -> bool:
        """ä½¿ç”¨ä¾¿å®œæ¨¡å‹åˆ¤æ–­6-Kæ–‡ä»¶æ˜¯å¦ä¸ºå­£æŠ¥/å¹´æŠ¥/IPOæŠ¥å‘Š"""
        try:
            # è·å–å½“å‰è¯­è¨€è®¾ç½®
            language = st.session_state.get("selected_language", "English")
            
            if language == "English":
                prompt = f"""
                You are a financial document classifier. Please analyze the following 6-K filing content and determine if it is a Quarterly Report, Annual Report, or IPO Report.

                Classification criteria:
                - Quarterly Report: Contains quarterly financial results, earnings data, quarterly business updates
                - Annual Report: Contains annual financial results, yearly business summary, annual shareholder information
                - IPO Report: Contains initial public offering information, prospectus data, listing announcements

                Please respond with ONLY a JSON object in this exact format:
                {{
                    "is_quarterly_annual_ipo": true/false,
                    "document_type": "quarterly/annual/ipo/other",
                    "confidence": "high/medium/low"
                }}

                Document content (first 5000 characters):
                {document_content[:5000]}
                """
            else:
                prompt = f"""
                ä½ æ˜¯ä¸€ä¸ªé‡‘èæ–‡æ¡£åˆ†ç±»å™¨ã€‚è¯·åˆ†æä»¥ä¸‹6-Kæ–‡ä»¶å†…å®¹ï¼Œåˆ¤æ–­å®ƒæ˜¯å¦æ˜¯å­£æŠ¥ã€å¹´æŠ¥æˆ–IPOæŠ¥å‘Šã€‚

                åˆ†ç±»æ ‡å‡†ï¼š
                - å­£æŠ¥ï¼šåŒ…å«å­£åº¦è´¢åŠ¡ç»“æœã€ç›ˆåˆ©æ•°æ®ã€å­£åº¦ä¸šåŠ¡æ›´æ–°
                - å¹´æŠ¥ï¼šåŒ…å«å¹´åº¦è´¢åŠ¡ç»“æœã€å¹´åº¦ä¸šåŠ¡æ€»ç»“ã€å¹´åº¦è‚¡ä¸œä¿¡æ¯
                - IPOæŠ¥å‘Šï¼šåŒ…å«é¦–æ¬¡å…¬å¼€å‘è¡Œä¿¡æ¯ã€æ‹›è‚¡è¯´æ˜ä¹¦æ•°æ®ã€ä¸Šå¸‚å…¬å‘Š

                è¯·åªå›ç­”JSONæ ¼å¼ï¼š
                {{
                    "is_quarterly_annual_ipo": true/false,
                    "document_type": "quarterly/annual/ipo/other",
                    "confidence": "high/medium/low"
                }}

                æ–‡æ¡£å†…å®¹ï¼ˆå‰5000å­—ç¬¦ï¼‰ï¼š
                {document_content[:5000]}
                """
            
            result = self.call_api(prompt, "gemini-2.5-flash-lite-preview-06-17")
            
            # å°è¯•è§£æJSON
            try:
                # å°è¯•ä»Markdownä»£ç å—ä¸­æå–JSON
                import re
                match = re.search(r"```json\s*(\{.*?\})\s*```", result, re.DOTALL)
                if match:
                    json_str = match.group(1)
                else:
                    json_str = result
                
                import json
                classification = json.loads(json_str)
                return classification.get("is_quarterly_annual_ipo", False)
                
            except (json.JSONDecodeError, ValueError) as e:
                logger.warning(f"è§£æ6-Kåˆ†ç±»JSONå¤±è´¥: {e}. æ¨¡å‹è¿”å›: {result}")
                # å¦‚æœè§£æå¤±è´¥ï¼Œä¿å®ˆå¤„ç†ï¼Œè¿”å›Trueç»§ç»­åˆ†æ
                return True
                
        except Exception as e:
            logger.error(f"6-Kæ–‡æ¡£åˆ†ç±»å¤±è´¥: {e}")
            # å¦‚æœåˆ†ç±»å¤±è´¥ï¼Œä¿å®ˆå¤„ç†ï¼Œè¿”å›Trueç»§ç»­åˆ†æ
            return True

# SEC æœåŠ¡
class SECService:
    """SECæ–‡ä»¶æœåŠ¡"""
    
    def __init__(self, cache_manager: CacheManager):
        self.rate_limiter = RateLimiter(max_calls=30, window=60)
        warnings.filterwarnings("ignore", category=XMLParsedAsHTMLWarning)
        self.cache_manager = cache_manager
        self.sixk_processor = None  # å°†åœ¨éœ€è¦æ—¶åˆå§‹åŒ–
    
    def _init_sixk_processor(self, temp_dir: str):
        """åˆå§‹åŒ–6-Kå¤„ç†å™¨"""
        if self.sixk_processor is None:
            self.sixk_processor = SixKProcessor(temp_dir)

    @lru_cache(maxsize=100)
    def get_cik_map(self) -> Dict[str, str]:
        """è·å–tickeråˆ°CIKçš„æ˜ å°„ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        cache_key = "cik_map"
        cached_result = self.cache_manager.get(cache_key)
        if cached_result:
            return cached_result
        
        try:
            headers = {'User-Agent': config.SEC_USER_AGENT}
            response = httpx.get(
                "https://www.sec.gov/files/company_tickers.json", 
                headers=headers,
                timeout=config.REQUEST_TIMEOUT
            )
            response.raise_for_status()
            
            all_companies = response.json()
            cik_map = {company['ticker']: company['cik_str'] for company in all_companies.values()}
            
            self.cache_manager.set(cache_key, cik_map)
            return cik_map
            
        except Exception as e:
            logger.error(f"è·å–CIKæ˜ å°„å¤±è´¥: {e}")
            raise DataRetrievalError(f"è·å–CIKæ˜ å°„å¤±è´¥: {e}")
    
    @retry_on_failure(max_retries=3)
    def get_filings(self, ticker: str, years: int = 3, forms_to_include: Optional[List[str]] = None, status_callback=None) -> List[Document]:
        """è·å–SECæ–‡ä»¶åˆ—è¡¨"""
        self.rate_limiter.wait_if_needed()
        
        # å¦‚æœæœªæä¾›è¦åŒ…å«çš„è¡¨å•ï¼Œåˆ™ä½¿ç”¨é…ç½®ä¸­çš„æ‰€æœ‰è¡¨å•
        if forms_to_include is None:
            forms_to_include = config.SEC_FORMS
        
        cache_key = self.cache_manager.get_cache_key("sec_filings", ticker, years, tuple(sorted(forms_to_include)))
        cached_result = self.cache_manager.get(cache_key)
        if cached_result:
            return cached_result
        
        try:
            edgar = EdgarClient(user_agent=config.SEC_USER_AGENT)
            
            # ä¿®æ­£å¹´ä»½è®¡ç®—é€»è¾‘ï¼šå¦‚æœæ˜¯2å¹´ï¼Œå°±æ˜¯2024/1/1åˆ°2025/1/1
            current_year = datetime.now().year
            end_date = datetime(current_year + 1, 1, 1)  # å»¶ä¼¸åˆ°ä¸‹ä¸€å¹´çš„1æœˆ1å·
            start_date = datetime(current_year - years + 1, 1, 1)  # å¾€å‰æ¨yearså¹´
            
            logger.info(f"SECæ–‡ä»¶æ—¥æœŸèŒƒå›´: {start_date.date()} åˆ° {end_date.date()}")
            
            ticker_map = self.get_cik_map()
            cik = ticker_map.get(ticker.upper())
            
            if not cik:
                logger.warning(f"æœªæ‰¾åˆ°ticker {ticker} çš„CIK")
                return []
            
            submissions = edgar.get_submissions(cik=str(cik).zfill(10))
            
            # è·å–æ‰€æœ‰æ–‡ä»¶æ•°æ®ï¼ˆåŒ…æ‹¬recentå’Œfilesï¼‰
            all_forms = []
            all_accession_numbers = []
            all_filing_dates = []
            all_primary_documents = []
            
            # å¤„ç†recentæ•°æ®
            recent = submissions.get('filings', {}).get('recent', {})
            if recent and 'form' in recent:
                all_forms.extend(recent.get('form', []))
                all_accession_numbers.extend(recent.get('accessionNumber', []))
                all_filing_dates.extend(recent.get('filingDate', []))
                all_primary_documents.extend(recent.get('primaryDocument', []))
                logger.info(f"SEC recentæ–‡ä»¶æ•°: {len(recent.get('form', []))}")
            
            # å¤„ç†filesæ•°æ®ï¼ˆå†å²æ–‡ä»¶ï¼‰
            files = submissions.get('filings', {}).get('files', [])
            if files:
                logger.info(f"SECéœ€è¦å¤„ç† {len(files)} ä¸ªå†å²æ–‡ä»¶æ‰¹æ¬¡")
                max_batches = 100  # æœ€å¤šå¤„ç†10ä¸ªæ‰¹æ¬¡ï¼Œé¿å…è¿‡å¤šAPIè°ƒç”¨
                processed_batches = 0
                
                for idx, file_info in enumerate(files):
                    if processed_batches >= max_batches:
                        logger.info(f"SECå·²å¤„ç† {max_batches} ä¸ªæ‰¹æ¬¡ï¼Œåœæ­¢å¤„ç†æ›´å¤šå†å²æ–‡ä»¶")
                        break
                        
                    file_name = file_info.get('name', '')
                    if file_name:
                        try:
                            if status_callback:
                                status_callback(f"æ­£åœ¨è·å–ç¬¬ {idx + 1}/{min(len(files), max_batches)} æ‰¹SECå†å²æ–‡ä»¶...")
                            # è·å–å†å²æ–‡ä»¶æ•°æ®
                            historical_data = edgar.get_submissions(cik=str(cik).zfill(10), file_name=file_name)
                            if historical_data and 'form' in historical_data:
                                batch_size = len(historical_data.get('form', []))
                                logger.info(f"SECå†å²æ–‡ä»¶æ‰¹æ¬¡ {file_name}: {batch_size} ä¸ªæ–‡ä»¶")
                                
                                # æ£€æŸ¥æ‰¹æ¬¡ä¸­æœ€æ—©çš„æ—¥æœŸï¼Œå¦‚æœå¤ªæ—©åˆ™åœæ­¢
                                batch_dates = historical_data.get('filingDate', [])
                                if batch_dates:
                                    earliest_date = min(batch_dates)
                                    earliest_datetime = datetime.strptime(earliest_date, '%Y-%m-%d').date()
                                    if earliest_datetime < start_date.date():
                                        logger.info(f"SECå†å²æ–‡ä»¶æ‰¹æ¬¡ {file_name} æœ€æ—©æ—¥æœŸ {earliest_datetime} æ—©äºæˆªæ­¢æ—¥æœŸ {start_date.date()}ï¼Œåœæ­¢å¤„ç†")
                                        break
                                
                                all_forms.extend(historical_data.get('form', []))
                                all_accession_numbers.extend(historical_data.get('accessionNumber', []))
                                all_filing_dates.extend(historical_data.get('filingDate', []))
                                all_primary_documents.extend(historical_data.get('primaryDocument', []))
                                processed_batches += 1
                        except Exception as e:
                            logger.warning(f"è·å–SECå†å²æ–‡ä»¶æ‰¹æ¬¡ {file_name} å¤±è´¥: {e}")
                            continue
            
            if not all_forms:
                logger.warning(f"æœªæ‰¾åˆ°SECæ–‡ä»¶æ•°æ®")
                return []
            
            logger.info(f"SECæ€»æ–‡ä»¶æ•°: {len(all_forms)}")
            
            documents = []
            cutoff_reached = False
            
            for i in range(len(all_forms)):
                form_type = all_forms[i]
                if form_type in forms_to_include:
                    filing_date = datetime.strptime(all_filing_dates[i], '%Y-%m-%d').date()
                    
                    # æ£€æŸ¥æ˜¯å¦æ—©äºæˆªæ­¢æ—¥æœŸ
                    if filing_date < start_date.date():
                        logger.info(f"SECæ–‡ä»¶æ—¥æœŸ {filing_date} æ—©äºæˆªæ­¢æ—¥æœŸ {start_date.date()}ï¼Œåœæ­¢å¤„ç†")
                        cutoff_reached = True
                        break
                    
                    if start_date.date() <= filing_date < end_date.date():  # ä¸åŒ…å«ç»“æŸæ—¥æœŸ
                        accession_no_no_dashes = all_accession_numbers[i].replace('-', '')
                        filing_url = f"https://www.sec.gov/Archives/edgar/data/{cik}/{accession_no_no_dashes}/{all_primary_documents[i]}"
                        
                        documents.append(Document(
                            type='SEC Filing',
                            title=f"{ticker} {form_type}",
                            date=filing_date,
                            url=filing_url,
                            form_type=form_type
                        ))
            
            # æŒ‰æ—¥æœŸæ’åºï¼ˆæ–°åˆ°æ—§ï¼‰
            documents.sort(key=lambda x: x.date, reverse=True)
            
            logger.info(f"æ‰¾åˆ° {len(documents)} ä¸ªSECæ–‡ä»¶")
            self.cache_manager.set(cache_key, documents)
            return documents
            
        except Exception as e:
            logger.error(f"è·å–SECæ–‡ä»¶å¤±è´¥: {e}")
            raise DataRetrievalError(f"è·å–SECæ–‡ä»¶å¤±è´¥: {e}")
    
    @retry_on_failure(max_retries=3)
    def download_filing(self, filing_url: str) -> str:
        """ä¸‹è½½SECæ–‡ä»¶å†…å®¹"""
        self.rate_limiter.wait_if_needed()
        
        try:
            response = httpx.get(
                filing_url, 
                headers={"User-Agent": config.SEC_USER_AGENT},
                timeout=config.REQUEST_TIMEOUT
            )
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'lxml')
            document_tag = soup.find('document')
            
            content = document_tag.get_text(separator='\n', strip=True) if document_tag else soup.get_text(separator='\n', strip=True)
            
            # é™åˆ¶å†…å®¹é•¿åº¦
            if len(content) > config.MAX_CONTENT_LENGTH:
                content = content[:config.MAX_CONTENT_LENGTH] + "\n[å†…å®¹å·²æˆªæ–­]"
            
            return content
            
        except Exception as e:
            logger.error(f"ä¸‹è½½SECæ–‡ä»¶å¤±è´¥: {e}")
            return f"ä¸‹è½½æ–‡ä»¶æ—¶å‡ºé”™: {e}"

# æ¸¯è‚¡æœåŠ¡
class HKStockService:
    """æ¸¯è‚¡æ–‡ä»¶æœåŠ¡"""
    
    def __init__(self, cache_manager: CacheManager):
        self.rate_limiter = RateLimiter(max_calls=30, window=60)
        self.cache_manager = cache_manager
        self.downloader = HKStockFilingsDownloader()
    
    def parse_hk_date(self, date_str: str) -> Optional[datetime.date]:
        """è§£ææ¸¯è‚¡æ—¥æœŸæ ¼å¼"""
        try:
            # æ¸¯è‚¡æ—¥æœŸæ ¼å¼ï¼šDD/MM/YYYY HH:MM æˆ– Release Time:DD/MM/YYYY HH:MM
            date_part = date_str
            if 'Release Time:' in date_str:
                date_part = date_str.replace('Release Time:', '').strip()
            
            if '/' in date_part and ' ' in date_part:
                date_only = date_part.split(' ')[0]  # å–æ—¥æœŸéƒ¨åˆ†
                parts = date_only.split('/')
                if len(parts) == 3:
                    day, month, year = parts
                    return datetime(int(year), int(month), int(day)).date()
            return None
        except Exception as e:
            logger.warning(f"è§£ææ¸¯è‚¡æ—¥æœŸå¤±è´¥: {date_str} - {e}")
            return None
    
    @retry_on_failure(max_retries=3)
    def get_hk_filings(self, ticker: str, years: int = 3, forms_to_include: Optional[List[str]] = None, status_callback=None) -> List[Document]:
        """è·å–æ¸¯è‚¡æ–‡ä»¶åˆ—è¡¨"""
        self.rate_limiter.wait_if_needed()
        
        # å¦‚æœæœªæä¾›è¦åŒ…å«çš„è¡¨å•ï¼Œåˆ™ä½¿ç”¨é»˜è®¤çš„å­£æŠ¥å¹´æŠ¥
        if forms_to_include is None:
            forms_to_include = ['quarterly_annual']
        
        cache_key = self.cache_manager.get_cache_key("hk_filings", ticker, years, tuple(sorted(forms_to_include)))
        cached_result = self.cache_manager.get(cache_key)
        if cached_result:
            return cached_result
        
        try:
            # è·å–è‚¡ç¥¨ID
            if status_callback:
                status_callback("æ­£åœ¨è·å–æ¸¯è‚¡ID...")
            stock_id, stock_code, stock_name = self.downloader.get_stock_id(ticker)
            if not stock_id:
                logger.warning(f"æœªæ‰¾åˆ°æ¸¯è‚¡ticker {ticker} çš„ä¿¡æ¯")
                return []
            
            # è®¡ç®—æ—¥æœŸèŒƒå›´
            current_year = datetime.now().year
            end_date = datetime(current_year + 1, 1, 1)
            start_date = datetime(current_year - years + 1, 1, 1)
            
            # è½¬æ¢ä¸ºæ¸¯è‚¡APIæ ¼å¼
            from_date = start_date.strftime("%Y%m%d")
            to_date = end_date.strftime("%Y%m%d")
            
            logger.info(f"æ¸¯è‚¡æ–‡ä»¶æ—¥æœŸèŒƒå›´: {start_date.date()} åˆ° {end_date.date()}")
            
            # è·å–å…¬å‘Šåˆ—è¡¨
            if status_callback:
                status_callback("æ­£åœ¨è·å–æ¸¯è‚¡å…¬å‘Šåˆ—è¡¨...")
            all_filings = self.downloader.get_filings_list(stock_id, from_date, to_date, from_date, status_callback)
            
            if not all_filings:
                logger.warning(f"æœªæ‰¾åˆ°æ¸¯è‚¡ {ticker} çš„å…¬å‘Š")
                return []
            
            # åˆ†ç±»å…¬å‘Š
            quarterly_annual_filings, other_filings = self.downloader.categorize_filings(all_filings)
            
            # æ ¹æ®ç”¨æˆ·é€‰æ‹©å†³å®šä½¿ç”¨å“ªäº›å…¬å‘Š
            selected_filings = []
            if 'quarterly_annual' in forms_to_include:
                selected_filings.extend(quarterly_annual_filings)
            if 'others' in forms_to_include:
                selected_filings.extend(other_filings)
            
            # è½¬æ¢ä¸ºDocumentå¯¹è±¡
            documents = []
            for filing in selected_filings:
                # è§£ææ—¥æœŸ
                filing_date = self.parse_hk_date(filing['release_time'])
                if filing_date and start_date.date() <= filing_date < end_date.date():
                    documents.append(Document(
                        type='HK Stock Filing',
                        title=f"{stock_code} {filing['doc_type']} - {filing['doc_title']}",
                        date=filing_date,
                        url=filing['url'],
                        form_type=filing['doc_type'],
                        content=None  # å†…å®¹å°†åœ¨éœ€è¦æ—¶ä¸‹è½½
                    ))
            
            # æŒ‰æ—¥æœŸæ’åºï¼ˆæ–°åˆ°æ—§ï¼‰
            documents.sort(key=lambda x: x.date, reverse=True)
            
            logger.info(f"æ‰¾åˆ° {len(documents)} ä¸ªæ¸¯è‚¡æ–‡ä»¶")
            self.cache_manager.set(cache_key, documents)
            return documents
            
        except Exception as e:
            logger.error(f"è·å–æ¸¯è‚¡æ–‡ä»¶å¤±è´¥: {e}")
            raise DataRetrievalError(f"è·å–æ¸¯è‚¡æ–‡ä»¶å¤±è´¥: {e}")
    
    @retry_on_failure(max_retries=3)
    def download_hk_filing(self, filing_url: str) -> str:
        """ä¸‹è½½æ¸¯è‚¡æ–‡ä»¶å†…å®¹"""
        self.rate_limiter.wait_if_needed()
        
        try:
            # æ„å»ºfiling_infoå¯¹è±¡
            filing_info = {'url': filing_url}
            
            # ä¸‹è½½PDFå†…å®¹
            pdf_content = self.downloader.download_filing_content(filing_info)
            
            if not pdf_content:
                return "ä¸‹è½½æ¸¯è‚¡æ–‡ä»¶å¤±è´¥"
            
            # ä½¿ç”¨PyMuPDFå¤„ç†PDF
            with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as temp_file:
                temp_file.write(pdf_content)
                temp_file_path = temp_file.name
            
            try:
                # ä½¿ç”¨PyMuPDFæå–æ–‡æœ¬
                doc = fitz.open(temp_file_path)
                text = ""
                
                for page_num in range(doc.page_count):
                    page = doc[page_num]
                    page_text = page.get_text()
                    if page_text.strip():
                        text += f"\n--- ç¬¬ {page_num + 1} é¡µ ---\n"
                        text += page_text
                        text += "\n"
                
                doc.close()
                
                # æ¸…ç†æ–‡æœ¬
                text = re.sub(r'\n{3,}', '\n\n', text)
                
                # é™åˆ¶å†…å®¹é•¿åº¦
                if len(text) > config.MAX_CONTENT_LENGTH:
                    text = text[:config.MAX_CONTENT_LENGTH] + "\n[å†…å®¹å·²æˆªæ–­]"
                
                return text.strip()
                
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                os.unlink(temp_file_path)
            
        except Exception as e:
            logger.error(f"ä¸‹è½½æ¸¯è‚¡æ–‡ä»¶å¤±è´¥: {e}")
            return f"ä¸‹è½½æ¸¯è‚¡æ–‡ä»¶æ—¶å‡ºé”™: {e}"

# è´¢æŠ¥ä¼šè®®è®°å½•æœåŠ¡
class EarningsService:
    """è´¢æŠ¥ä¼šè®®è®°å½•æœåŠ¡"""
    
    def __init__(self, cache_manager: CacheManager):
        self.rate_limiter = RateLimiter(max_calls=30, window=60)
        self.cache_manager = cache_manager
        self.session = requests.Session() # ä½¿ç”¨æŒä¹…åŒ–ä¼šè¯å¤„ç†cookies
        self._lock = threading.Lock()  # æ·»åŠ çº¿ç¨‹é”ç”¨äºå¹¶è¡Œå¤„ç†

    @staticmethod
    def parse_transcript_url(url_path: str) -> Optional[Tuple[str, int, str]]:
        """
        ä»URLè·¯å¾„ä¸­è§£æè‚¡ç¥¨ä»£ç ã€å¹´ä»½å’Œå­£åº¦ä¿¡æ¯
        
        Args:
            url_path (str): transcript URLè·¯å¾„, ä¾‹å¦‚ '/company/BABA/transcripts/2025/4/'
        
        Returns:
            tuple: (ticker, year, quarter) æˆ– None
        """
        # å…è®¸tickerä¸­åŒ…å«ç‚¹(.)å’Œæ•°å­—
        pattern = r'/company/([A-Z0-9\.]+)/transcripts/(\d{4})/(\d+)/'
        match = re.match(pattern, url_path)
        
        if match:
            ticker, year, quarter = match.groups()
            return ticker, int(year), str(quarter)
        
        return None

    def get_earnings_transcript_batch(self, url_paths: List[str], max_workers: int = 1) -> List[Optional[Dict]]:
        """
        å¹¶è¡Œè·å–å¤šä¸ªè´¢æŠ¥ä¼šè®®è®°å½•
        
        Args:
            url_paths: URLè·¯å¾„åˆ—è¡¨
            max_workers: æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°
        
        Returns:
            List[Optional[Dict]]: è´¢æŠ¥è®°å½•ä¿¡æ¯åˆ—è¡¨ï¼Œä¸è¾“å…¥é¡ºåºä¸€è‡´
        """
        results = [None] * len(url_paths)  # é¢„åˆ†é…ç»“æœåˆ—è¡¨
        
        def process_single_transcript(index_url_pair):
            """å¤„ç†å•ä¸ªè´¢æŠ¥è®°å½•çš„å†…éƒ¨å‡½æ•°"""
            index, url_path = index_url_pair
            try:
                with self._lock:
                    # ä½¿ç”¨é”æ¥é™åˆ¶å¹¶å‘è¯·æ±‚
                    self.rate_limiter.wait_if_needed()
                
                result = self.get_earnings_transcript(url_path)
                return index, result
            except Exception as e:
                logger.error(f"å¹¶è¡Œå¤„ç†è´¢æŠ¥è®°å½•å¤±è´¥ {url_path}: {e}")
                return index, None
        
        # ä½¿ç”¨ThreadPoolExecutorè¿›è¡Œå¹¶è¡Œå¤„ç†
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_index = {
                executor.submit(process_single_transcript, (i, url_path)): i 
                for i, url_path in enumerate(url_paths)
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_index):
                try:
                    index, result = future.result()
                    results[index] = result
                except Exception as e:
                    index = future_to_index[future]
                    logger.error(f"å¹¶è¡Œå¤„ç†ä»»åŠ¡å¤±è´¥ index {index}: {e}")
                    results[index] = None
        
        return results

    @retry_on_failure(max_retries=3)
    def get_available_quarters(self, ticker: str) -> List[str]:
        """è·å–æŒ‡å®šå…¬å¸æ‰€æœ‰å¯ç”¨çš„å­£åº¦URLåˆ—è¡¨ (ä¸æµ‹è¯•è„šæœ¬å¯¹é½)"""
        self.rate_limiter.wait_if_needed()
        
        cache_key = self.cache_manager.get_cache_key("earnings_quarters_urls", ticker)
        cached_result = self.cache_manager.get(cache_key)
        if cached_result:
            logger.info(f"ä»ç¼“å­˜åŠ è½½ {ticker} çš„å¯ç”¨å­£åº¦åˆ—è¡¨")
            return cached_result
        
        try:
            ticker_upper = ticker.upper()
            logger.info(f"è·å– {ticker_upper} çš„æ‰€æœ‰å¯ç”¨å­£åº¦...")
            url = f"https://discountingcashflows.com/company/{ticker_upper}/transcripts/?org.htmx.cache-buster=baseContent"
            headers = {
                "accept": "*/*",
                "accept-language": "en-US,en;q=0.9,zh-TW;q=0.8,zh;q=0.7,zh-CN;q=0.6,ja;q=0.5",
                "hx-boosted": "true",
                "hx-current-url": f"https://discountingcashflows.com/company/{ticker_upper}/chart/",
                "hx-request": "true",
                "hx-target": "baseContent",
                "priority": "u=1, i",
                "sec-ch-ua": '"Not)A;Brand";v="8", "Chromium";v="138", "Google Chrome";v="138"',
                "sec-ch-ua-mobile": "?0",
                "sec-ch-ua-platform": '"macOS"',
                "sec-fetch-dest": "empty",
                "sec-fetch-mode": "cors",
                "sec-fetch-site": "same-origin",
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36"
            }
            
            response = self.session.get(url, headers=headers, timeout=config.REQUEST_TIMEOUT)
            logger.info(f"è·å–å¯ç”¨å­£åº¦åˆ—è¡¨ å“åº”çŠ¶æ€ç : {response.status_code}")
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            transcript_links = []
            
            pattern = re.compile(rf'/company/{ticker_upper}/transcripts/(\d{{4}})/(\d+)/')
            
            for link in soup.find_all('a', href=pattern):
                href = link.get('href')
                if href and href != f'/company/{ticker_upper}/transcripts/':
                    transcript_links.append(href)
            
            transcript_links = sorted(list(set(transcript_links)), reverse=True)
            logger.info(f"æ‰¾åˆ° {len(transcript_links)} ä¸ªå¯ç”¨å­£åº¦ for {ticker_upper}")
            
            self.cache_manager.set(cache_key, transcript_links)
            return transcript_links
            
        except Exception as e:
            logger.error(f"è·å–è´¢æŠ¥ä¼šè®®è®°å½•å­£åº¦åˆ—è¡¨å¤±è´¥: {e}")
            raise DataRetrievalError(f"è·å–è´¢æŠ¥ä¼šè®®è®°å½•å­£åº¦åˆ—è¡¨å¤±è´¥: {e}")
    
    def parse_date_text(self, date_text: str) -> Optional[datetime.date]:
        """è§£ææ—¥æœŸæ–‡æœ¬"""
        if not date_text:
            return None
        
        for fmt in config.DATE_FORMATS:
            try:
                return datetime.strptime(date_text, fmt).date()
            except ValueError:
                continue
        
        logger.warning(f"æ— æ³•è§£ææ—¥æœŸæ ¼å¼: {date_text}")
        return None
    
    @retry_on_failure(max_retries=3)
    def get_earnings_transcript(self, url_path: str) -> Optional[Dict]:
        """
        è·å–å•ä¸ªè´¢æŠ¥ä¼šè®®è®°å½•ï¼ŒåŒ…å«æ—¥æœŸè§£æå’Œæ–‡ä»¶ä¿å­˜ï¼Œä¸æµ‹è¯•è„šæœ¬é€»è¾‘å®Œå…¨å¯¹é½ã€‚
        """
        self.rate_limiter.wait_if_needed()

        parsed_info = self.parse_transcript_url(url_path)
        if not parsed_info:
            logger.error(f"æ— æ³•ä»URLè·¯å¾„è§£æä¿¡æ¯: {url_path}")
            return None
        ticker, year, quarter_num = parsed_info
        logger.info(f"--- å¼€å§‹è·å–: {ticker} Q{quarter_num} {year} ---")
        logger.info(f"é€‰æ‹©çš„URL: {url_path}")

        try:
            base_url = f"https://discountingcashflows.com{url_path}"
            cache_buster_url = f"{base_url}?org.htmx.cache-buster=transcriptsContent"
            
            headers = {
                "accept": "*/*",
                "accept-language": "en-US,en;q=0.9,zh-TW;q=0.8,zh;q=0.7,zh-CN;q=0.6,ja;q=0.5",
                "hx-boosted": "true",
                "hx-current-url": base_url,
                "hx-request": "true",
                "hx-target": "transcriptsContent",
                "priority": "u=1, i",
                "sec-ch-ua": '"Google Chrome";v="137", "Chromium";v="137", "Not/A)Brand";v="24"',
                "sec-ch-ua-mobile": "?0",
                "sec-ch-ua-platform": '"macOS"',
                "sec-fetch-dest": "empty",
                "sec-fetch-mode": "cors",
                "sec-fetch-site": "same-origin",
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36"
            }

            logger.info(f"Getting main page: {base_url}")
            main_response = self.session.get(base_url, headers={"User-Agent": headers["User-Agent"]})
            logger.info(f"Main page response status: {main_response.status_code}")
            main_response.raise_for_status()

            csrf_match = re.search(r'csrftoken["\']?\s*:\s*["\']([^"\']+)["\']', main_response.text)
            if not csrf_match:
                csrf_match = re.search(r'name=["\']csrfmiddlewaretoken["\'] value=["\']([^"\']+)["\']', main_response.text)
            if csrf_match:
                csrf_token = csrf_match.group(1)
                headers["x-csrftoken"] = csrf_token
                self.session.cookies['csrftoken'] = csrf_token
                logger.info(f"Extracted CSRF token: {csrf_token[:20]}...")

            logger.info(f"Making transcript request: {cache_buster_url}")
            response = self.session.get(cache_buster_url, headers=headers, timeout=config.REQUEST_TIMEOUT)
            logger.info(f"Transcript response status code: {response.status_code}")
            response.raise_for_status()
            
            transcript_data, fiscal_info = self._parse_html_transcript(response.text)
            
            if transcript_data:
                # è§£ææ—¥æœŸ
                parsed_date = self.parse_date_text(fiscal_info.get('date'))

                # ä¿å­˜æ–‡ä»¶ (å¯é€‰)
                txt_filename = None
                html_filename = None
                if SAVE_TRANSCRIPT_FILES:
                    txt_filename = self._save_transcript_as_txt(
                        transcript_data,
                        fiscal_info,
                        f"transcript_{ticker}_FY{fiscal_info.get('fiscal_year', year)}_Q{quarter_num}.txt"
                    )
                    html_filename = self._save_raw_html(response.text, ticker, fiscal_info, quarter_num)
                
                # è¿”å›ç»“æ„åŒ–æ•°æ®
                return {
                    'ticker': ticker,
                    'year': year,
                    'quarter': quarter_num,
                    'date': parsed_date,
                    'content': transcript_data.get('content', ''),
                    'txt_filename': txt_filename,
                    'html_filename': html_filename,
                    'parsed_successfully': True
                }
            else:
                logger.error(f"æœªèƒ½ä»HTMLè§£æè´¢æŠ¥æ•°æ®: {url_path}")
                return None
            
        except Exception as e:
            logger.error(f"è·å–è´¢æŠ¥ä¼šè®®è®°å½•æ—¶å‡ºé”™: {e} from {url_path}")
            raise DataRetrievalError(f"è·å–è´¢æŠ¥ä¼šè®®è®°å½•æ—¶å‡ºé”™: {e}")

    def _parse_html_transcript(self, html_content: str) -> Tuple[Optional[Dict], Dict]:
        """ä»HTMLå†…å®¹ä¸­è§£æç»“æ„åŒ–æ•°æ®"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            fiscal_info = {}
            
            # æŸ¥æ‰¾è´¢å¹´ä¿¡æ¯
            fiscal_span = soup.find('span', string=re.compile(r'Fiscal Year.*Quarter'))
            if fiscal_span:
                fiscal_text = fiscal_span.get_text()
                fy_match = re.search(r'Fiscal Year \(FY\) (\d+), Quarter (\d+)', fiscal_text)
                if fy_match:
                    fiscal_info['fiscal_year'] = fy_match.group(1)
                    fiscal_info['quarter'] = fy_match.group(2)
            
            # æŸ¥æ‰¾æ—¥æœŸä¿¡æ¯
            date_span = soup.find('span', class_='text-xs')
            if date_span:
                date_text = date_span.get_text().strip()
                fiscal_info['date'] = date_text
            
            # ä»textareaä¸­æå–å†…å®¹
            textarea = soup.find('textarea', id='AIInsightsContent')
            if not textarea:
                return None, fiscal_info
            
            json_content = html.unescape(textarea.get_text())
            transcript_data = json.loads(json_content)
            return transcript_data, fiscal_info
            
        except Exception as e:
            logger.error(f"è§£æHTMLæ—¶å‡ºé”™: {e}")
            return None, {}

    def _extract_speaker_content(self, transcript_content: str) -> List[Dict]:
        """ä»è®°å½•ä¸­æå–å‘è¨€äººå’Œå†…å®¹"""
        speakers_content = []
        lines = transcript_content.split('\n')
        current_speaker = None
        current_content = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            speaker_match = re.match(r'^([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s*[-:](.*)$', line)
            if speaker_match:
                if current_speaker and current_content:
                    speakers_content.append({
                        'speaker': current_speaker,
                        'content': ' '.join(current_content).strip()
                    })
                current_speaker = speaker_match.group(1).strip()
                current_content = [speaker_match.group(2).strip()] if speaker_match.group(2).strip() else []
            else:
                if current_speaker:
                    current_content.append(line)
        
        if current_speaker and current_content:
            speakers_content.append({
                'speaker': current_speaker,
                'content': ' '.join(current_content).strip()
            })
        return speakers_content

    def _save_raw_html(self, html_content: str, ticker: str, fiscal_info: Dict, quarter_num: str) -> Optional[str]:
        """ä¿å­˜åŸå§‹HTMLæ–‡ä»¶"""
        try:
            folder_name = f"{ticker}_transcripts"
            os.makedirs(folder_name, exist_ok=True)
            html_filename = f"{folder_name}/transcript_{ticker}_FY{fiscal_info.get('fiscal_year', 'UNKNOWN')}_Q{quarter_num}.html"
            with open(html_filename, 'w', encoding='utf-8') as f:
                f.write(html_content)
            logger.info(f"åŸå§‹HTMLå·²ä¿å­˜åˆ°: {html_filename}")
            return html_filename
        except Exception as e:
            logger.error(f"ä¿å­˜HTMLæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return None

    def _save_transcript_as_txt(self, transcript_data: Dict, fiscal_info: Dict, filename: str) -> Optional[str]:
        """å°†è§£æåçš„è®°å½•ä¿å­˜ä¸ºTXTæ–‡ä»¶"""
        try:
            ticker = transcript_data.get('symbol', 'UNKNOWN')
            folder_name = f"{ticker}_transcripts"
            os.makedirs(folder_name, exist_ok=True)
            txt_filename = f"{folder_name}/{filename}"
            
            with open(txt_filename, 'w', encoding='utf-8') as f:
                f.write("="*60 + "\nEARNINGS CALL TRANSCRIPT\n" + "="*60 + "\n\n")
                if fiscal_info:
                    f.write(f"Company: {transcript_data.get('symbol', 'N/A')}\n")
                    f.write(f"Fiscal Year: {fiscal_info.get('fiscal_year', 'N/A')}\n")
                    f.write(f"Quarter: {fiscal_info.get('quarter', 'N/A')}\n")
                    f.write(f"Date: {fiscal_info.get('date', 'N/A')}\n")
                    f.write(f"Transcript Date: {transcript_data.get('date', 'N/A')}\n")
                    f.write("\n" + "="*60 + "\n\n")
                
                content = transcript_data.get('content', '')
                speakers_content = self._extract_speaker_content(content)
                
                if speakers_content:
                    for item in speakers_content:
                        f.write(f"[{item['speaker']}]\n{item['content']}\n\n" + "-"*40 + "\n\n")
                else:
                    f.write("RAW TRANSCRIPT CONTENT:\n" + "-"*40 + "\n" + content)
            
            logger.info(f"Transcript saved to: {txt_filename}")
            return txt_filename
        except Exception as e:
            logger.error(f"ä¿å­˜transcript TXTæ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return None

# ä¸»ä¸šåŠ¡é€»è¾‘
class SECEarningsAnalyzer:
    """SEC & è´¢æŠ¥åˆ†æå™¨ä¸»ç±»"""
    
    def __init__(self):
        self.gemini_service = GeminiService()
        self.cache_manager = CacheManager()
        self.sec_service = SECService(self.cache_manager)
        self.hk_service = HKStockService(self.cache_manager)
        self.earnings_service = EarningsService(self.cache_manager)
        self.session_manager = SessionManager()
        self.document_manager = DocumentManager()

    def analyze_question(self, question: str, ticker: str, model_type: str) -> Tuple[str, str]:
        """åˆ†æç”¨æˆ·é—®é¢˜å¹¶ç”Ÿæˆæç¤ºè¯"""
        # è·å–å½“å‰è¯­è¨€è®¾ç½®
        language = st.session_state.get("selected_language", "English")
        
        if language == "English":
            analysis_prompt = f"""
            You are a professional financial analyst assistant, specialized in analyzing user questions and breaking them down into two key processing steps.

            User Question: {question}
            Stock Ticker: {ticker}

            Your Task:
            1. Analyze the user's question and understand its core intent
            2. Generate two high-quality prompts:
               - Document Processing Prompt: Used to extract information related to the user's question from a single document
               - Integration Prompt: Used to integrate analysis results from multiple document processing responses

            Requirements:
            - The document processing prompt must be specific and professional, capable of extracting all relevant information from a single document
            - The integration prompt must be able to integrate results from multiple documents, providing complete analysis and insights
            - Both prompts should be concise and clear, highlighting key points
            - The generated prompts should produce professional, readable analysis results
            - The user's original question MUST appear in both the document processing prompt and integration prompt
            - Always answer in English
            - **Must return only JSON format, do not include any other text or explanations.**

            Please return directly in JSON format:
            ```json
            {{
                "processing_prompt": "Document processing prompt",
                "integration_prompt": "Integration prompt"
            }}
            ```
            """
        else:  # ä¸­æ–‡
            analysis_prompt = f"""
            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆåŠ©æ‰‹ï¼Œä¸“é—¨è´Ÿè´£åˆ†æç”¨æˆ·çš„é—®é¢˜å¹¶å°†å…¶åˆ†è§£ä¸ºä¸¤ä¸ªå…³é”®çš„å¤„ç†æ­¥éª¤ã€‚

            ç”¨æˆ·é—®é¢˜: {question}
            è‚¡ç¥¨ä»£ç : {ticker}

            ä½ çš„ä»»åŠ¡ï¼š
            1. åˆ†æç”¨æˆ·çš„é—®é¢˜ï¼Œç†è§£å…¶æ ¸å¿ƒæ„å›¾
            2. ç”Ÿæˆä¸¤ä¸ªé«˜è´¨é‡çš„æç¤ºè¯ï¼š
               - å¤„ç†èµ„æ–™promptï¼šç”¨äºä»å•ä¸ªæ–‡æ¡£ä¸­æå– èˆ‡ç”¨æˆ·é—®é¢˜ ç›¸å…³çš„ä¿¡æ¯
               - ç»Ÿæ•´promptï¼šç”¨äºæ•´åˆå¤šä¸ª å¤„ç†èµ„æ–™å›ç­” çš„åˆ†æç»“æœ

            è¦æ±‚ï¼š
            - å¤„ç†èµ„æ–™promptå¿…é¡»å…·ä½“ã€ä¸“ä¸šï¼Œèƒ½å¤Ÿä»å•ä¸ªæ–‡æ¡£ä¸­æå–æ‰€æœ‰ç›¸å…³ä¿¡æ¯
            - ç»Ÿæ•´promptå¿…é¡»èƒ½å¤Ÿæ•´åˆå¤šä¸ªæ–‡æ¡£çš„ç»“æœï¼Œæä¾›å®Œæ•´çš„åˆ†æå’Œæ´å¯Ÿ
            - ä¸¤ä¸ªpromptéƒ½è¦ç®€æ´æ˜äº†ï¼Œé‡ç‚¹çªå‡º
            - ç”Ÿæˆçš„promptåº”è¯¥èƒ½å¤Ÿäº§ç”Ÿä¸“ä¸šã€æ˜“è¯»çš„åˆ†æç»“æœ
            - ç”¨æˆ¶åŸå§‹å•é¡Œ å¿…é ˆåœ¨è™•ç†è³‡æ–™prompt å’Œ çµ±æ•´prompt ä¸­éƒ½å‡ºç¾
            - **å¿…é¡»åªè¿”å›JSONæ ¼å¼ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡æœ¬æˆ–è§£é‡Šã€‚**

            è¯·ç›´æ¥è¿”å›JSONæ ¼å¼ï¼š
            ```json
            {{
                "processing_prompt": "å¤„ç†èµ„æ–™prompt",
                "integration_prompt": "ç»Ÿæ•´prompt"
            }}
            ```
            """
        
        try:
            result = self.gemini_service.call_api(analysis_prompt, model_type)
            # å°è¯•ä»Markdownä»£ç å—ä¸­æå–JSON
            match = re.search(r"```json\s*(\{.*?\})\s*```", result, re.DOTALL)
            if match:
                json_str = match.group(1)
            else:
                json_str = result # å¦‚æœæ²¡æœ‰markdownï¼Œåˆ™å‡å®šæ•´ä¸ªå­—ç¬¦ä¸²æ˜¯JSON
            
            prompt_data = json.loads(json_str)
            
            processing_prompt = prompt_data.get("processing_prompt", "")
            integration_prompt = prompt_data.get("integration_prompt", "")
            
            if not processing_prompt or not integration_prompt:
                raise ValueError("ç”Ÿæˆçš„æç¤ºè¯ä¸ºç©º")
            
            return processing_prompt, integration_prompt
            
        except (json.JSONDecodeError, ValueError) as e:
            logger.error(f"è§£æprompt JSONå¤±è´¥: {e}. æ¨¡å‹è¿”å›: {result}")
            # ä½¿ç”¨é»˜è®¤æç¤ºè¯
            if language == "English":
                processing_prompt = f"Please extract information related to '{question}' from the following document and provide detailed analysis. Keep answers concise and to the point. Start with conclusions, use appropriate emojis and markdown format. If not found, briefly state 'Not mentioned in document'. Always answer in English."
                integration_prompt = f"Please integrate all the following analysis results to answer the user's question: '{question}', and provide a complete analysis report. Keep answers concise and to the point. Start with conclusions, use appropriate emojis and markdown format. If not found, briefly state 'Not mentioned in documents'. Always answer in English."
            else:
                processing_prompt = f"è¯·ä»ä»¥ä¸‹æ–‡æ¡£ä¸­æå–ä¸'{question}'ç›¸å…³çš„ä¿¡æ¯ï¼Œå¹¶è¿›è¡Œè¯¦ç»†åˆ†æï¼Œåªå›ç­”é‡é»å°±å¥½ï¼Œè¨˜å¾—ä¸å»¢è©±ã€‚å›ç­”è¦çµè«–å…ˆèªªï¼Œå¯ä»¥é©ç•¶ä½¿ç”¨emojiï¼Œmarkdownæ ¼å¼ï¼Œå¦‚æœæ²’æ‰¾åˆ°å°±ç°¡çŸ­å›ç­”ï¼ŒèªªæœªæåŠå°±å¥½ã€‚"
                integration_prompt = f"è¯·æ•´åˆä»¥ä¸‹æ‰€æœ‰åˆ†æç»“æœï¼Œå›ç­”ç”¨æˆ·é—®é¢˜ï¼š'{question}'ï¼Œå¹¶æä¾›å®Œæ•´çš„åˆ†ææŠ¥å‘Šï¼Œåªå›ç­”é‡é»å°±å¥½ï¼Œè¨˜å¾—ä¸å»¢è©±ã€‚å›ç­”è¦çµè«–å…ˆèªªï¼Œå¯ä»¥é©ç•¶ä½¿ç”¨emojiï¼Œmarkdownæ ¼å¼ï¼Œå¦‚æœæ²’æ‰¾åˆ°å°±ç°¡çŸ­å›ç­”ï¼ŒèªªæœªæåŠå°±å¥½ã€‚"
            
            return processing_prompt, integration_prompt
    
    def process_document(self, document: Document, processing_prompt: str, model_type: str) -> str:
        """å¤„ç†å•ä¸ªæ–‡æ¡£"""
        try:
            # è·å–å½“å‰è¯­è¨€è®¾ç½®
            language = st.session_state.get("selected_language", "English")
            
            # å¦‚æœæ–‡æ¡£å†…å®¹ä¸ºç©ºï¼Œåˆ™ä¸‹è½½
            if not document.content:
                if document.type == 'SEC Filing':
                    # æ£€æŸ¥æ˜¯å¦ä¸º6-Kæ–‡ä»¶
                    if hasattr(document, 'form_type') and document.form_type == '6-K':
                        # 6-Kæ–‡ä»¶åº”è¯¥å·²ç»åœ¨SixKProcessorä¸­å¤„ç†è¿‡äº†
                        logger.warning(f"6-Kæ–‡ä»¶å†…å®¹ä¸ºç©ºï¼Œè¿™ä¸åº”è¯¥å‘ç”Ÿ: {document.title}")
                        document.content = "6-Kæ–‡ä»¶å†…å®¹å¤„ç†å¤±è´¥" if language == "ä¸­æ–‡" else "6-K file content processing failed"
                    else:
                        # æ™®é€šSECæ–‡ä»¶å¤„ç†
                        document.content = self.sec_service.download_filing(document.url)
                elif document.type == 'HK Stock Filing':
                    # æ¸¯è‚¡æ–‡ä»¶å¤„ç†
                    document.content = self.hk_service.download_hk_filing(document.url)
                elif document.type == 'Earnings Call':
                    # åœ¨æ–°çš„æµç¨‹ä¸­ï¼Œå†…å®¹å·²é¢„å…ˆè·å–
                    logger.warning(f"å¤„ç†æ–‡æ¡£æ—¶å‘ç°è´¢æŠ¥è®°å½•å†…å®¹ä¸ºç©º: {document.title}")
                    document.content = "å†…å®¹æœªæ‰¾åˆ°" if language == "ä¸­æ–‡" else "Content not found"
            
            # å‡†å¤‡prompt - æ ¹æ®è¯­è¨€é€‰æ‹©
            if language == "English":
                prompt = f"""
                You are a professional document analyst, specialized in extracting and analyzing information from financial documents.

                Document Title: {document.title}
                Document Date: {document.date}
                Document Type: {document.type}
                
                Processing Requirements: {processing_prompt}
                Also answer similar requirements, don't miss anything
                
                Requirements:
                - Carefully read the provided document content
                - Extract relevant information according to the user's specific requirements
                - Provide accurate, professional analysis
                - Ensure answers come from document content, don't imagine
                - I don't have time to read, ensure answers are direct and to the point, no need for polite conversation
                - Always answer in English
                - when markdown output, Escape all dollar signs $ for currency as ï¼„ to prevent Markdown from rendering them as math.
                
                Answer Requirements:
                - Start with ğŸ“ emoji, followed by what type of document this is and its purpose, 
                - second line Start with ğŸ’¡ on next new line row, directly state conclusions, answer conclusions related to my processing requirements, all in short sentences
                - Please provide structured analysis results, only answer key points, remember no nonsense.
                - First sentence should state key points without pleasantries. Don't say "According to the document content you provided..." such nonsense, directly state key points
                - Answer should start with conclusions, can use emojis to help users read, markdown format
                - If the document doesn't contain information related to my question, just say "Not mentioned in document" period, one sentence only, no nonsense, I don't have time to read

                Document Content:
                {document.content}
                """
            else:  # ä¸­æ–‡
                prompt = f"""
                ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ†æå¸ˆï¼Œä¸“é—¨è´Ÿè´£ä»è´¢åŠ¡æ–‡æ¡£ä¸­æå–å’Œåˆ†æä¿¡æ¯ã€‚

                æ–‡æ¡£æ ‡é¢˜: {document.title}
                æ–‡æ¡£æ—¥æœŸ: {document.date}
                æ–‡æ¡£ç±»å‹: {document.type}
                
                å¤„ç†è¦æ±‚: {processing_prompt}
                æœ‰èˆ‡ä»¥ä¸Šè¦æ±‚é¡ä¼¼çš„ä¹Ÿä¸€èµ·å›ç­”ï¼Œä¸è¦æ¼æ‰
                
                è¦æ±‚ï¼š
                - ä»”ç»†é˜…è¯»æä¾›çš„æ–‡æ¡£å†…å®¹
                - æ ¹æ®ç”¨æˆ·çš„å…·ä½“è¦æ±‚æå–ç›¸å…³ä¿¡æ¯
                - æä¾›å‡†ç¡®ã€ä¸“ä¸šçš„åˆ†æ
                - ç¢ºä¿å›ç­”éƒ½ä¾†è‡ªæ–‡æª”å…§å®¹ï¼Œä¸è¦æ†‘ç©ºæƒ³åƒ
                - æˆ‘æ²’æ™‚é–“çœ‹ ç¢ºä¿å›ç­”ç›´æ¥èªªé‡é» ä¸ç”¨åƒäººä¸€æ¨£é‚„è¦å®¢å¥—è©±
                - å…§æ–‡markdownè¼¸å‡ºï¼Œå°‡æ‰€æœ‰è¡¨ç¤ºé‡‘é¡çš„ $ æ”¹ç‚º ï¼„ï¼Œä»¥é¿å… Markdown è¢«èª¤åˆ¤ç‚ºæ•¸å­¸å…¬å¼ã€‚


                
                å›ç­”è¦æ±‚ï¼š
                - é–‹é ­ä»¥ğŸ“è¿™ä¸ªemojiå¼€é ­ï¼Œ ğŸ“å¾Œé¢æ¥é€™æ˜¯ä¸€ä»½ä»€éº¼æ–‡ä»¶ï¼Œæ–‡ä»¶ç›®çš„æ˜¯ä»€éº¼ï¼Œ
                - ç¬¬äºŒå¥ä¸‹ä¸€è¡Œï¼Œé–‹é ­ä»¥ ğŸ’¡ï¼Œè¨˜å¾—æ›è¡Œï¼Œç›´æ¥èªªçµè«–ï¼Œå›ç­”è·Ÿæˆ‘å¤„ç†è¦æ±‚æœ‰é—œçš„çµè«– éƒ½æ˜¯ç°¡çŸ­ä¸€å¥è©±
                - è¯·æä¾›ç»“æ„åŒ–çš„åˆ†æç»“æœï¼Œåªå›ç­”é‡é»å°±å¥½ï¼Œè¨˜å¾—ä¸å»¢è©±ã€‚
                - ç¬¬ä¸€å¥å°±èªªé‡é»ä¸ç”¨å®¢å¥—ã€‚ ä¸ç”¨èªª æ ¹æ®æ‚¨æä¾›çš„æ–‡æ¡£å†…å®¹... é€™ç¨®å»¢è©±ï¼Œç›´æ¥èªªé‡é»
                - å›ç­”è¦çµè«–å…ˆèªªï¼Œå¯ä»¥ä½¿ç”¨emojiå¹«åŠ©ä½¿ç”¨è€…é–±è®€ï¼Œmarkdownæ ¼å¼
                - å¦‚æœæ–‡æª”å…§æ²’æœ‰è·Ÿæˆ‘çš„å•é¡Œæœ‰é—œçš„è³‡è¨Šï¼Œå°±èªªä¸€å¥ æ–‡æª”å…§æœªæåŠ å¥è™Ÿ ä¸€å¥è©±å°±å¥½  ä¸å‡†å»¢è©± æˆ‘æ²’æ™‚é–“çœ‹

                æ–‡æ¡£å†…å®¹:
                {document.content}
                """
            
            logger.info("================================================")
            logger.info(f"Processing document: {document.title} in {language}")
            
            return self.gemini_service.call_api(prompt, model_type)
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡æ¡£å¤±è´¥: {e}")
            error_msg = f"å¤„ç†æ–‡æ¡£æ—¶å‡ºé”™: {e}" if language == "ä¸­æ–‡" else f"Error processing document: {e}"
            return error_msg
    
    def process_document_stream(self, document: Document, processing_prompt: str, model_type: str):
        """å¤„ç†å•ä¸ªæ–‡æ¡£ - æµå¼å“åº”ç‰ˆæœ¬"""
        try:
            # è·å–å½“å‰è¯­è¨€è®¾ç½®
            language = st.session_state.get("selected_language", "English")
            
            # å¦‚æœæ–‡æ¡£å†…å®¹ä¸ºç©ºï¼Œåˆ™ä¸‹è½½
            if not document.content:
                if document.type == 'SEC Filing':
                    # æ£€æŸ¥æ˜¯å¦ä¸º6-Kæ–‡ä»¶
                    if hasattr(document, 'form_type') and document.form_type == '6-K':
                        # 6-Kæ–‡ä»¶åº”è¯¥å·²ç»åœ¨SixKProcessorä¸­å¤„ç†è¿‡äº†
                        logger.warning(f"6-Kæ–‡ä»¶å†…å®¹ä¸ºç©ºï¼Œè¿™ä¸åº”è¯¥å‘ç”Ÿ: {document.title}")
                        document.content = "6-Kæ–‡ä»¶å†…å®¹å¤„ç†å¤±è´¥" if language == "ä¸­æ–‡" else "6-K file content processing failed"
                    else:
                        # æ™®é€šSECæ–‡ä»¶å¤„ç†
                        document.content = self.sec_service.download_filing(document.url)
                elif document.type == 'HK Stock Filing':
                    # æ¸¯è‚¡æ–‡ä»¶å¤„ç†
                    document.content = self.hk_service.download_hk_filing(document.url)
                elif document.type == 'Earnings Call':
                    # åœ¨æ–°çš„æµç¨‹ä¸­ï¼Œå†…å®¹å·²é¢„å…ˆè·å–
                    logger.warning(f"å¤„ç†æ–‡æ¡£æ—¶å‘ç°è´¢æŠ¥è®°å½•å†…å®¹ä¸ºç©º: {document.title}")
                    document.content = "å†…å®¹æœªæ‰¾åˆ°" if language == "ä¸­æ–‡" else "Content not found"
            
            # å‡†å¤‡prompt - æ ¹æ®è¯­è¨€é€‰æ‹©
            if language == "English":
                prompt = f"""
                You are a professional document analyst, specialized in extracting and analyzing information from financial documents.

                Document Title: {document.title}
                Document Date: {document.date}
                Document Type: {document.type}
                
                Processing Requirements: {processing_prompt}
                Also answer similar requirements, don't miss anything
                
                Requirements:
                - Carefully read the provided document content
                - Extract relevant information according to the user's specific requirements
                - Provide accurate, professional analysis
                - Ensure answers come from document content, don't imagine
                - I don't have time to read, ensure answers are direct and to the point, no need for polite conversation
                - Always answer in English
                - when markdown output, Escape all dollar signs $ for currency as ï¼„ to prevent Markdown from rendering them as math.
                
                Answer Requirements:
                - Start with ğŸ“ emoji, followed by what type of document this is and its purpose, 
                - second line Start with ğŸ’¡ on next new line row, directly state conclusions, answer conclusions related to my processing requirements, all in short sentences
                - Please provide structured analysis results, only answer key points, remember no nonsense.
                - First sentence should state key points without pleasantries. Don't say "According to the document content you provided..." such nonsense, directly state key points
                - Answer should start with conclusions, can use emojis to help users read, markdown format
                - If the document doesn't contain information related to my question, just say "Not mentioned in document" period, one sentence only, no nonsense, I don't have time to read

                Document Content:
                {document.content}
                """
            else:  # ä¸­æ–‡
                prompt = f"""
                ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ†æå¸ˆï¼Œä¸“é—¨è´Ÿè´£ä»è´¢åŠ¡æ–‡æ¡£ä¸­æå–å’Œåˆ†æä¿¡æ¯ã€‚

                æ–‡æ¡£æ ‡é¢˜: {document.title}
                æ–‡æ¡£æ—¥æœŸ: {document.date}
                æ–‡æ¡£ç±»å‹: {document.type}
                
                å¤„ç†è¦æ±‚: {processing_prompt}
                æœ‰èˆ‡ä»¥ä¸Šè¦æ±‚é¡ä¼¼çš„ä¹Ÿä¸€èµ·å›ç­”ï¼Œä¸è¦æ¼æ‰
                
                è¦æ±‚ï¼š
                - ä»”ç»†é˜…è¯»æä¾›çš„æ–‡æ¡£å†…å®¹
                - æ ¹æ®ç”¨æˆ·çš„å…·ä½“è¦æ±‚æå–ç›¸å…³ä¿¡æ¯
                - æä¾›å‡†ç¡®ã€ä¸“ä¸šçš„åˆ†æ
                - ç¢ºä¿å›ç­”éƒ½ä¾†è‡ªæ–‡æª”å…§å®¹ï¼Œä¸è¦æ†‘ç©ºæƒ³åƒ
                - æˆ‘æ²’æ™‚é–“çœ‹ ç¢ºä¿å›ç­”ç›´æ¥èªªé‡é» ä¸ç”¨åƒäººä¸€æ¨£é‚„è¦å®¢å¥—è©±
                - markdownè¼¸å‡ºï¼Œå°‡æ‰€æœ‰è¡¨ç¤ºé‡‘é¡çš„ $ æ”¹ç‚º ï¼„ï¼Œä»¥é¿å… Markdown è¢«èª¤åˆ¤ç‚ºæ•¸å­¸å…¬å¼ã€‚


                
                å›ç­”è¦æ±‚ï¼š
                - é–‹é ­ä»¥ğŸ“è¿™ä¸ªemojiå¼€é ­ï¼Œ ğŸ“å¾Œé¢æ¥é€™æ˜¯ä¸€ä»½ä»€éº¼æ–‡ä»¶ï¼Œæ–‡ä»¶ç›®çš„æ˜¯ä»€éº¼ï¼Œ
                - ç¬¬äºŒå¥ä¸‹ä¸€è¡Œï¼Œé–‹é ­ä»¥ ğŸ’¡ï¼Œè¨˜å¾—æ›è¡Œï¼Œç›´æ¥èªªçµè«–ï¼Œå›ç­”è·Ÿæˆ‘å¤„ç†è¦æ±‚æœ‰é—œçš„çµè«– éƒ½æ˜¯ç°¡çŸ­ä¸€å¥è©±
                - è¯·æä¾›ç»“æ„åŒ–çš„åˆ†æç»“æœï¼Œåªå›ç­”é‡é»å°±å¥½ï¼Œè¨˜å¾—ä¸å»¢è©±ã€‚
                - ç¬¬ä¸€å¥å°±èªªé‡é»ä¸ç”¨å®¢å¥—ã€‚ ä¸ç”¨èªª æ ¹æ®æ‚¨æä¾›çš„æ–‡æ¡£å†…å®¹... é€™ç¨®å»¢è©±ï¼Œç›´æ¥èªªé‡é»
                - å›ç­”è¦çµè«–å…ˆèªªï¼Œå¯ä»¥ä½¿ç”¨emojiå¹«åŠ©ä½¿ç”¨è€…é–±è®€ï¼Œmarkdownæ ¼å¼
                - å¦‚æœæ–‡æª”å…§æ²’æœ‰è·Ÿæˆ‘çš„å•é¡Œæœ‰é—œçš„è³‡è¨Šï¼Œå°±èªªä¸€å¥ æ–‡æª”å…§æœªæåŠ å¥è™Ÿ ä¸€å¥è©±å°±å¥½  ä¸å‡†å»¢è©± æˆ‘æ²’æ™‚é–“çœ‹

                æ–‡æ¡£å†…å®¹:
                {document.content}
                """
            
            logger.info("================================================")
            logger.info(f"Processing document (streaming): {document.title} in {language}")
            
            # è¿”å›æµå¼å“åº”ç”Ÿæˆå™¨
            return self.gemini_service.call_api_stream(prompt, model_type)
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡æ¡£å¤±è´¥: {e}")
            error_msg = f"å¤„ç†æ–‡æ¡£æ—¶å‡ºé”™: {e}" if language == "ä¸­æ–‡" else f"Error processing document: {e}"
            # å¯¹äºé”™è¯¯ï¼Œè¿”å›ä¸€ä¸ªç®€å•çš„ç”Ÿæˆå™¨
            def error_generator():
                yield error_msg
            return error_generator()

    def integrate_results(self, document_results: List[Dict], integration_prompt: str, user_question: str, ticker: str, model_type: str) -> str:
        """æ•´åˆåˆ†æç»“æœ"""
        try:
            # è·å–å½“å‰è¯­è¨€è®¾ç½®
            language = st.session_state.get("selected_language", "English")
            
            # æ„å»ºæ•´åˆæç¤ºè¯
            if language == "English":
                integration_input = f"""
                You are a professional financial analyst, specialized in integrating analysis results from multiple documents.

                User Question: {user_question}
                Stock Ticker: {ticker}
                
                Integration Requirements: {integration_prompt}
                
                Requirements:
                - If the content contains numbers for the same indicator at different time points, place a pivot table at the very beginning of the answer. Format: pivot table row names are different indicators, column names are the time when indicators were published, cells are the indicator numbers. Then explain below the pivot table after generation.
                - If the content contains business descriptions for the same indicator at different time points, place a pivot table at the very beginning of the answer. Format: pivot table row names are different indicators, column names are the time when indicators were published, cells are the indicator descriptions. Then explain below the pivot table after generation.
                - For example: row1 would be Indicator, 2025Q1, 2025Q2. row2 would be AI commercialization, Q2 expected to resume double-digit year-over-year growth, confident in achieving significant revenue growth for full year 2025
                - table output use markdown format, ensure markdown format is correct, no errors
                - Comprehensively analyze all provided document analysis results
                - Identify trends, patterns, and key changes
                - Provide deep insights and professional recommendations
                - Use tables, lists, and other formats to enhance readability
                - Highlight key information and critical findings
                - This is a comprehensive summary, don't repeat detailed content from individual documents
                - Focus on cross-document trends and correlations
                - Always answer in English
                - when markdown output, Escape all dollar signs $ for currency as ï¼„ to prevent Markdown from rendering them as math.
                
                Document Analysis Results:
                """
            else:  # ä¸­æ–‡
                integration_input = f"""
                ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆï¼Œä¸“é—¨è´Ÿè´£æ•´åˆå¤šä¸ªæ–‡æ¡£çš„åˆ†æç»“æœã€‚

                ç”¨æˆ·é—®é¢˜: {user_question}
                è‚¡ç¥¨ä»£ç : {ticker}
                
                æ•´åˆè¦æ±‚: {integration_prompt}
                
                è¦æ±‚ï¼š
                - å¦‚æœå…§æ–‡æœ‰ åŒæŒ‡æ¨™ä¸åŒæ™‚é–“é»çš„ æ•¸å­—ï¼Œå›ç­”çš„æœ€ä¸€é–‹å§‹ ä¸€å®šè¦æ”¾ä¸Šä¸€å€‹pivot tableï¼Œæ ¼å¼æ˜¯ pivot table row name æ˜¯ä¸åŒæŒ‡æ¨™ ï¼Œ column æŒ‡æ¨™å…¬å¸ƒçš„æ™‚é–“ï¼Œcell æ˜¯æŒ‡æ¨™çš„æ•¸å­—ã€‚ç„¶å¾Œpivot table ç”Ÿæˆå®Œ è¡¨æ ¼ä¸‹æ–¹è§£é‡‹ä¸€ä¸‹
                - å¦‚æœå…§æ–‡æœ‰ åŒæŒ‡æ¨™ä¸åŒæ™‚é–“é»çš„ æ¥­å‹™çš„æè¿°ï¼Œå›ç­”çš„æœ€ä¸€é–‹å§‹ ä¸€å®šè¦æ”¾ä¸Šä¸€å€‹pivot tableï¼Œæ ¼å¼æ˜¯ pivot table row name æ˜¯ä¸åŒæŒ‡æ¨™ ï¼Œ column æŒ‡æ¨™å…¬å¸ƒçš„æ™‚é–“ï¼Œcell æ˜¯æŒ‡æ¨™çš„æ•¸å­—ã€‚ç„¶å¾Œpivot table ç”Ÿæˆå®Œ è¡¨æ ¼ä¸‹æ–¹è§£é‡‹ä¸€ä¸‹
                - - èˆ‰ä¾‹é¡ä¼¼åƒæ˜¯  row1æœƒæ˜¯ æŒ‡æ¨™, 2025Q1, 2025Q2 ã€‚ row2æœƒæ˜¯ AIå•†ä¸šåŒ–, Q2é¢„è®¡å°†æ¢å¤ä¸¤ä½æ•°åŒæ¯”å¢é•¿, æœ‰ä¿¡å¿ƒåœ¨2025å…¨å¹´å¹´å®ç°æ˜¾è‘—æ”¶å…¥å¢é•¿
                - table éƒ½ç”¨markdownæ ¼å¼ï¼Œè¦ç¢ºä¿markdownæ ¼å¼æ­£ç¢ºï¼Œä¸è¦æœ‰éŒ¯èª¤
                - ç»¼åˆåˆ†ææ‰€æœ‰æä¾›çš„æ–‡æ¡£åˆ†æç»“æœ
                - è¯†åˆ«è¶‹åŠ¿ã€æ¨¡å¼å’Œå…³é”®å˜åŒ–
                - æä¾›æ·±å…¥çš„æ´å¯Ÿå’Œä¸“ä¸šå»ºè®®
                - ä½¿ç”¨è¡¨æ ¼ã€åˆ—è¡¨ç­‰æ ¼å¼å¢å¼ºå¯è¯»æ€§
                - çªå‡ºé‡ç‚¹ä¿¡æ¯å’Œå…³é”®å‘ç°
                - è¿™æ˜¯ä¸€ä¸ªç»¼åˆæ€»ç»“ï¼Œä¸è¦é‡å¤å•ä¸ªæ–‡æ¡£çš„è¯¦ç»†å†…å®¹
                - é‡ç‚¹å…³æ³¨è·¨æ–‡æ¡£çš„è¶‹åŠ¿å’Œå…³è”æ€§
                
                æ–‡æ¡£åˆ†æç»“æœ:
                """
            
            for result in document_results:
                integration_input += f"""
                
                === {result['title']} ({result['date']}) ===
                {result['analysis']}
                """
            
            completion_text = "Please provide a complete, professional comprehensive analysis report and summary." if language == "English" else "è¯·æä¾›å®Œæ•´ã€ä¸“ä¸šçš„ç»¼åˆåˆ†ææŠ¥å‘Šå’Œæ€»ç»“ã€‚"
            integration_input += f"\n\n{completion_text}"
            
            return self.gemini_service.call_api(integration_input, model_type)
            
        except Exception as e:
            logger.error(f"æ•´åˆç»“æœå¤±è´¥: {e}")
            error_msg = f"æ•´åˆç»“æœæ—¶å‡ºé”™: {e}" if st.session_state.get("selected_language", "English") == "ä¸­æ–‡" else f"Error integrating results: {e}"
            return error_msg
    
    def integrate_results_stream(self, document_results: List[Dict], integration_prompt: str, user_question: str, ticker: str, model_type: str):
        """æ•´åˆåˆ†æç»“æœ - æµå¼å“åº”ç‰ˆæœ¬"""
        try:
            # è·å–å½“å‰è¯­è¨€è®¾ç½®
            language = st.session_state.get("selected_language", "English")
            
            # æ„å»ºæ•´åˆæç¤ºè¯
            if language == "English":
                integration_input = f"""
                You are a professional financial analyst, specialized in integrating analysis results from multiple documents.

                User Question: {user_question}
                Stock Ticker: {ticker}
                
                Integration Requirements: {integration_prompt}
                
                Requirements:
                - If the content contains numbers for the same indicator at different time points, place a pivot table at the very beginning of the answer. Format: pivot table row names are different indicators, column names are the time when indicators were published, cells are the indicator numbers. Then explain below the pivot table after generation.
                - If the content contains business descriptions for the same indicator at different time points, place a pivot table at the very beginning of the answer. Format: pivot table row names are different indicators, column names are the time when indicators were published, cells are the indicator descriptions. Then explain below the pivot table after generation.
                - For example: row1 would be Indicator, 2025Q1, 2025Q2. row2 would be AI commercialization, Q2 expected to resume double-digit year-over-year growth, confident in achieving significant revenue growth for full year 2025
                - table output use markdown format, ensure markdown format is correct, no errors
                - Comprehensively analyze all provided document analysis results
                - Identify trends, patterns, and key changes
                - Provide deep insights and professional recommendations
                - Use tables, lists, and other formats to enhance readability
                - Highlight key information and critical findings
                - This is a comprehensive summary, don't repeat detailed content from individual documents
                - Focus on cross-document trends and correlations
                - Always answer in English
                - when markdown output, Escape all dollar signs $ for currency as ï¼„ to prevent Markdown from rendering them as math.
                
                Document Analysis Results:
                """
            else:  # ä¸­æ–‡
                integration_input = f"""
                ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆï¼Œä¸“é—¨è´Ÿè´£æ•´åˆå¤šä¸ªæ–‡æ¡£çš„åˆ†æç»“æœã€‚

                ç”¨æˆ·é—®é¢˜: {user_question}
                è‚¡ç¥¨ä»£ç : {ticker}
                
                æ•´åˆè¦æ±‚: {integration_prompt}
                
                è¦æ±‚ï¼š
                - å¦‚æœå…§æ–‡æœ‰ åŒæŒ‡æ¨™ä¸åŒæ™‚é–“é»çš„ æ•¸å­—ï¼Œå›ç­”çš„æœ€ä¸€é–‹å§‹ ä¸€å®šè¦æ”¾ä¸Šä¸€å€‹pivot tableï¼Œæ ¼å¼æ˜¯ pivot table row name æ˜¯ä¸åŒæŒ‡æ¨™ ï¼Œ column æŒ‡æ¨™å…¬å¸ƒçš„æ™‚é–“ï¼Œcell æ˜¯æŒ‡æ¨™çš„æ•¸å­—ã€‚ç„¶å¾Œpivot table ç”Ÿæˆå®Œ è¡¨æ ¼ä¸‹æ–¹è§£é‡‹ä¸€ä¸‹
                - å¦‚æœå…§æ–‡æœ‰ åŒæŒ‡æ¨™ä¸åŒæ™‚é–“é»çš„ æ¥­å‹™çš„æè¿°ï¼Œå›ç­”çš„æœ€ä¸€é–‹å§‹ ä¸€å®šè¦æ”¾ä¸Šä¸€å€‹pivot tableï¼Œæ ¼å¼æ˜¯ pivot table row name æ˜¯ä¸åŒæŒ‡æ¨™ ï¼Œ column æŒ‡æ¨™å…¬å¸ƒçš„æ™‚é–“ï¼Œcell æ˜¯æŒ‡æ¨™çš„æ•¸å­—ã€‚ç„¶å¾Œpivot table ç”Ÿæˆå®Œ è¡¨æ ¼ä¸‹æ–¹è§£é‡‹ä¸€ä¸‹
                - - èˆ‰ä¾‹é¡ä¼¼åƒæ˜¯  row1æœƒæ˜¯ æŒ‡æ¨™, 2025Q1, 2025Q2 ã€‚ row2æœƒæ˜¯ AIå•†ä¸šåŒ–, Q2é¢„è®¡å°†æ¢å¤ä¸¤ä½æ•°åŒæ¯”å¢é•¿, æœ‰ä¿¡å¿ƒåœ¨2025å…¨å¹´å¹´å®ç°æ˜¾è‘—æ”¶å…¥å¢é•¿
                - table éƒ½ç”¨markdownæ ¼å¼ï¼Œè¦ç¢ºä¿markdownæ ¼å¼æ­£ç¢ºï¼Œä¸è¦æœ‰éŒ¯èª¤
                - ç»¼åˆåˆ†ææ‰€æœ‰æä¾›çš„æ–‡æ¡£åˆ†æç»“æœ
                - è¯†åˆ«è¶‹åŠ¿ã€æ¨¡å¼å’Œå…³é”®å˜åŒ–
                - æä¾›æ·±å…¥çš„æ´å¯Ÿå’Œä¸“ä¸šå»ºè®®
                - ä½¿ç”¨è¡¨æ ¼ã€åˆ—è¡¨ç­‰æ ¼å¼å¢å¼ºå¯è¯»æ€§
                - çªå‡ºé‡ç‚¹ä¿¡æ¯å’Œå…³é”®å‘ç°
                - è¿™æ˜¯ä¸€ä¸ªç»¼åˆæ€»ç»“ï¼Œä¸è¦é‡å¤å•ä¸ªæ–‡æ¡£çš„è¯¦ç»†å†…å®¹
                - é‡ç‚¹å…³æ³¨è·¨æ–‡æ¡£çš„è¶‹åŠ¿å’Œå…³è”æ€§
                - markdownè¼¸å‡ºï¼Œå°‡æ‰€æœ‰è¡¨ç¤ºé‡‘é¡çš„ $ æ”¹ç‚º ï¼„ï¼Œä»¥é¿å… Markdown è¢«èª¤åˆ¤ç‚ºæ•¸å­¸å…¬å¼ã€‚
                
                æ–‡æ¡£åˆ†æç»“æœ:
                """
            
            for result in document_results:
                integration_input += f"""
                
                === {result['title']} ({result['date']}) ===
                {result['analysis']}
                """
            
            completion_text = "Please provide a complete, professional comprehensive analysis report and summary." if language == "English" else "è¯·æä¾›å®Œæ•´ã€ä¸“ä¸šçš„ç»¼åˆåˆ†ææŠ¥å‘Šå’Œæ€»ç»“ã€‚"
            integration_input += f"\n\n{completion_text}"
            
            # è¿”å›æµå¼å“åº”ç”Ÿæˆå™¨
            return self.gemini_service.call_api_stream(integration_input, model_type)
            
        except Exception as e:
            logger.error(f"æ•´åˆç»“æœå¤±è´¥: {e}")
            error_msg = f"æ•´åˆç»“æœæ—¶å‡ºé”™: {e}" if st.session_state.get("selected_language", "English") == "ä¸­æ–‡" else f"Error integrating results: {e}"
            # å¯¹äºé”™è¯¯ï¼Œè¿”å›ä¸€ä¸ªç®€å•çš„ç”Ÿæˆå™¨
            def error_generator():
                yield error_msg
            return error_generator()

# åˆå§‹åŒ–åº”ç”¨
@st.cache_resource
def initialize_app():
    """åˆå§‹åŒ–åº”ç”¨"""
    return SECEarningsAnalyzer()

# ä¸»é¡µé¢
def main():
    """ä¸»é¡µé¢å‡½æ•°"""
    # åœ¨åº”ç”¨åˆå§‹åŒ–ä¹‹å‰ï¼Œç¡®ä¿session stateå·²ç»å­˜åœ¨
    SessionManager.init_session_state()
    
    # åˆå§‹åŒ–session state for short scanner
    if "short_scanner_results" not in st.session_state:
        st.session_state.short_scanner_results = []
    # selected_detectors å·²è¢« selected_detector_classes æ›¿ä»£
    if "current_scan_results" not in st.session_state:
        st.session_state.current_scan_results = []
    
    # è™•ç†URLåƒæ•¸
    query_params = st.query_params
    if "p" in query_params:
        param_value = query_params["p"]
        if param_value.lower() == st.secrets["ACCESS_CODE"].lower():
            st.session_state.use_premium_api = True
            st.session_state.premium_access_code = param_value
            st.success("ğŸ‰ å·²å•Ÿç”¨ä»˜è²»APIæœå‹™ï¼")

    # åˆå§‹åŒ–åº”ç”¨
    analyzer = initialize_app()
    
    # åˆå§‹åŒ–åšç©ºä¿¡å·åˆ†æå™¨
    short_analyzer = ShortSignalAnalyzer(analyzer.gemini_service)
    
    # è·å–å½“å‰è¯­è¨€è®¾ç½®
    current_language = st.session_state.get("selected_language", "English")
    lang_config = LANGUAGE_CONFIG[current_language]
    
    # é¡µé¢æ ‡é¢˜
    st.title(lang_config["title"])

    # å·¦ä¾§è¾¹æ 
    with st.sidebar:
        st.header(lang_config["sidebar_header"])
        
        # Tickerè¾“å…¥
        ticker_input = st.text_input(
            lang_config["ticker_label"],
            value=st.session_state.analyzer_ticker,
            placeholder=lang_config["ticker_placeholder"]
        )
        
        # æ™ºèƒ½å¤„ç†tickeræ ¼å¼
        if ticker_input:
            if is_hk_stock(ticker_input):
                ticker = normalize_hk_ticker(ticker_input)
                st.info(lang_config["hk_stock_info"].format(ticker))
            else:
                ticker = ticker_input.upper()
                st.info(lang_config["us_stock_info"])
        else:
            ticker = ""
        
        # å¹´ä»½é€‰æ‹©
        years = st.number_input(
            lang_config["years_label"],
            min_value=1,
            max_value=10,
            value=st.session_state.analyzer_years,
            step=1
        )
        
        # æ•°æ®ç±»å‹é€‰æ‹© - æ ¹æ®è‚¡ç¥¨ç±»å‹æ˜¾ç¤ºä¸åŒé€‰é¡¹
        st.subheader(lang_config["data_type_header"])
        
        if is_hk_stock(ticker):
            # æ¸¯è‚¡é€‰é¡¹
            use_sec_reports = st.checkbox(lang_config["sec_reports_hk"], value=st.session_state.analyzer_use_sec_reports)
            use_sec_others = st.checkbox(lang_config["sec_others_hk"], value=st.session_state.analyzer_use_sec_others)
            
            use_earnings = st.checkbox(lang_config["earnings_label"], value=st.session_state.analyzer_use_earnings)
            st.caption(lang_config["earnings_caption"])
        else:
            # ç¾è‚¡é€‰é¡¹
            use_sec_reports = st.checkbox(lang_config["sec_reports_us"], value=st.session_state.analyzer_use_sec_reports)
            use_sec_others = st.checkbox(lang_config["sec_others_us"], value=st.session_state.analyzer_use_sec_others)
            
            use_earnings = st.checkbox(lang_config["earnings_label"], value=st.session_state.analyzer_use_earnings)
            st.caption(lang_config["earnings_caption"])
        
        # æ£€æµ‹æ¨¡å—é€‰æ‹©
        st.subheader(lang_config["detectors_header"])
        
        available_detectors = short_analyzer.get_available_detectors()
        detector_options = []
        detector_class_to_name = {}  # ç±»ååˆ°å½“å‰è¯­è¨€åç§°çš„æ˜ å°„
        detector_name_to_class = {}  # å½“å‰è¯­è¨€åç§°åˆ°ç±»åçš„æ˜ å°„
        
        for detector in available_detectors:
            class_name = detector.__class__.__name__
            current_name = detector.name
            detector_options.append(current_name)
            detector_class_to_name[class_name] = current_name
            detector_name_to_class[current_name] = class_name
        
        # ä½¿ç”¨ç±»åä½œä¸ºç¨³å®šçš„æ ‡è¯†ç¬¦æ¥å¤„ç†è¯­è¨€åˆ‡æ¢
        if "selected_detector_classes" not in st.session_state:
            # åˆå§‹åŒ–æ—¶é»˜è®¤é€‰æ‹©æ‰€æœ‰æ£€æµ‹å™¨
            st.session_state.selected_detector_classes = [detector.__class__.__name__ for detector in available_detectors]
        
        # æ ¹æ®é€‰ä¸­çš„ç±»åè·å–å½“å‰è¯­è¨€çš„åç§°
        default_selection = [detector_class_to_name[class_name] for class_name in st.session_state.selected_detector_classes if class_name in detector_class_to_name]
        
        help_text = "é€‰æ‹©è¦è¿è¡Œçš„æ£€æµ‹å™¨" if current_language == "ä¸­æ–‡" else "Select detectors to run"
        selected_detectors = st.multiselect(
            lang_config["detectors_label"],
            options=detector_options,
            default=default_selection,
            help=help_text
        )
        
        # æ›´æ–°é€‰ä¸­çš„æ£€æµ‹å™¨ç±»å
        st.session_state.selected_detector_classes = [detector_name_to_class[name] for name in selected_detectors]
        
        # æ˜¾ç¤ºæ£€æµ‹å™¨æè¿°
        if selected_detectors:
            selected_detectors_header = "**é€‰ä¸­çš„æ£€æµ‹å™¨ï¼š**" if current_language == "ä¸­æ–‡" else "**Selected Detectors:**"
            st.markdown(selected_detectors_header)
            for detector in available_detectors:
                if detector.__class__.__name__ in st.session_state.selected_detector_classes:
                    st.markdown(f"â€¢ **{detector.name}**")
                    st.markdown(f"  {detector.description}")
        
        # æ¨¡å‹é€‰æ‹©
        st.subheader(lang_config["model_header"])
        model_type = st.selectbox(
            lang_config["model_label"],
            list(config.MODELS.keys()),
            index=list(config.MODELS.keys()).index(st.session_state.analyzer_model),
            format_func=lambda x: config.MODELS[x]
        )
        
        # ä»˜è²»APIè¨­ç½®
        st.subheader(lang_config["api_header"])
        
        # è¼¸å…¥æ¡†
        access_code = st.text_input(
            lang_config["access_code_label"],
            value=st.session_state.get("premium_access_code", ""),
            placeholder=lang_config["access_code_placeholder"]
        )
        
        # é¡¯ç¤ºç•¶å‰ç‹€æ…‹
        if st.session_state.get("use_premium_api", False):
            st.success(lang_config["premium_enabled"])
        else:
            st.info(lang_config["free_api"])
        
        # æª¢æŸ¥è¼¸å…¥
        if access_code:
            if access_code.lower() == st.secrets["ACCESS_CODE"].lower():
                if not st.session_state.get("use_premium_api", False):
                    st.session_state.use_premium_api = True
                    st.session_state.premium_access_code = access_code
                    # å½“å¯ç”¨ä»˜è´¹APIæ—¶ï¼Œé»˜è®¤é€‰æ‹©2.5 proæ¨¡å‹
                    st.session_state.analyzer_model = "gemini-2.5-pro"
                    st.success(lang_config["premium_success"])
                    st.rerun()
            else:
                st.error(lang_config["access_code_error"])
        
        # è¯­è¨€é€‰æ‹©
        st.subheader(lang_config["language_header"])
        selected_language = st.selectbox(
            lang_config["language_label"],
            options=["English", "ä¸­æ–‡"],
            index=0 if st.session_state.get("selected_language", "English") == "English" else 1
        )
        
        # å¦‚æœè¯­è¨€æ”¹å˜ï¼Œæ›´æ–°session stateå¹¶é‡æ–°è¿è¡Œ
        if selected_language != st.session_state.get("selected_language", "English"):
            st.session_state.selected_language = selected_language
            st.rerun()
        
        # æ›´æ–°session state
        st.session_state.analyzer_ticker = ticker
        st.session_state.analyzer_years = years
        st.session_state.analyzer_use_sec_reports = use_sec_reports
        st.session_state.analyzer_use_sec_others = use_sec_others
        st.session_state.analyzer_use_earnings = use_earnings
        st.session_state.analyzer_model = model_type
        # selected_detectors å·²åœ¨ä¸Šé¢æ›´æ–°ä¸º selected_detector_classes
    
    # ä¸»å†…å®¹åŒºåŸŸ

    # æ˜¾ç¤ºå†å²æ‰«æç»“æœ
    if st.session_state.short_scanner_results:
        history_header = "ğŸ“Š å†å²æ‰«æç»“æœ" if current_language == "ä¸­æ–‡" else "ğŸ“Š Historical Scan Results"
        st.subheader(history_header)
        
        for i, result in enumerate(st.session_state.short_scanner_results):
            scan_result_label = f"æ‰«æç»“æœ {i+1}: {result['ticker']} ({result['timestamp']})" if current_language == "ä¸­æ–‡" else f"Scan Result {i+1}: {result['ticker']} ({result['timestamp']})"
            with st.expander(scan_result_label, expanded=True):
                st.markdown(result['report'])
                        
    # æ˜¾ç¤ºå½“å‰æ‰«æçš„ä¸­é—´ç»“æœ
    if st.session_state.current_scan_results:
        current_results_header = "ğŸ” å½“å‰æ‰«æç»“æœ" if current_language == "ä¸­æ–‡" else "ğŸ” Current Scan Results"
        st.subheader(current_results_header)
        
        total_signals = 0
        high_risk_signals = 0
        
        for result in st.session_state.current_scan_results:
            signals_text = "ä¸ªä¿¡å·" if current_language == "ä¸­æ–‡" else "signals"
            with st.expander(f"ğŸ“Š {result.detector_name} - {len(result.signals)} {signals_text}", expanded=True):
                if result.success:
                    success_text = f"âœ… æ‰§è¡ŒæˆåŠŸ - ç”¨æ—¶ {result.processing_time:.2f}ç§’" if current_language == "ä¸­æ–‡" else f"âœ… Execution successful - {result.processing_time:.2f}s"
                    st.success(success_text)
                    
                    if result.signals:
                        for signal in result.signals:
                            total_signals += 1
                            if signal.severity == "High":
                                high_risk_signals += 1
                                
                            # æ ¹æ®ä¸¥é‡ç¨‹åº¦é€‰æ‹©é¢œè‰²
                            if signal.severity == "High":
                                st.error(f"ğŸš¨ **{signal.title}**")
                            elif signal.severity == "Medium":
                                st.warning(f"âš ï¸ **{signal.title}**")
                            else:
                                st.info(f"ğŸ’¡ **{signal.title}**")
                            
                            if current_language == "ä¸­æ–‡":
                                # st.markdown(f"**ç½®ä¿¡åº¦**: {signal.confidence:.1%}")
                                st.markdown(f"**æè¿°**: {signal.description}")
                                st.markdown(f"**è¯æ®**: {signal.evidence}")
                                st.markdown(f"**å»ºè®®**: {signal.recommendation}")
                                
                                if signal.source_documents:
                                    st.markdown(f"**æ¥æºæ–‡æ¡£**: {', '.join(signal.source_documents)}")
                            else:
                                # st.markdown(f"**Confidence**: {signal.confidence:.1%}")
                                st.markdown(f"**Description**: {signal.description}")
                                st.markdown(f"**Evidence**: {signal.evidence}")
                                st.markdown(f"**Recommendation**: {signal.recommendation}")
                                
                                if signal.source_documents:
                                    st.markdown(f"**Source Documents**: {', '.join(signal.source_documents)}")
                            
                            st.markdown("---")
                    else:
                        no_signals_text = "æœªå‘ç°å¼‚å¸¸ä¿¡å·" if current_language == "ä¸­æ–‡" else "No anomalous signals detected"
                        st.info(no_signals_text)
                else:
                    error_text = f"âŒ æ‰§è¡Œå¤±è´¥: {result.error_message}" if current_language == "ä¸­æ–‡" else f"âŒ Execution failed: {result.error_message}"
                    st.error(error_text)
        
        # æ˜¾ç¤ºæ€»ç»“
        summary_header = "ğŸ“Š æ£€æµ‹æ€»ç»“" if current_language == "ä¸­æ–‡" else "ğŸ“Š Detection Summary"
        st.subheader(summary_header)
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            total_signals_label = "æ€»ä¿¡å·æ•°" if current_language == "ä¸­æ–‡" else "Total Signals"
            st.metric(total_signals_label, total_signals)
        with col2:
            high_risk_label = "é«˜é£é™©ä¿¡å·" if current_language == "ä¸­æ–‡" else "High Risk Signals"
            st.metric(high_risk_label, high_risk_signals)
        with col3:
            risk_level_label = "é£é™©ç­‰çº§" if current_language == "ä¸­æ–‡" else "Risk Level"
            if current_language == "ä¸­æ–‡":
                risk_level_value = "é«˜" if high_risk_signals > 0 else "ä¸­" if total_signals > 0 else "ä½"
            else:
                risk_level_value = "High" if high_risk_signals > 0 else "Medium" if total_signals > 0 else "Low"
            st.metric(risk_level_label, risk_level_value)
        with col4:
            clear_button_text = "ğŸ—‘ï¸ æ¸…ç†å½“å‰ç»“æœ" if current_language == "ä¸­æ–‡" else "ğŸ—‘ï¸ Clear Current Results"
            clear_help_text = "æ¸…ç†å½“å‰æ‰«æç»“æœ" if current_language == "ä¸­æ–‡" else "Clear current scan results"
            if st.button(clear_button_text, help=clear_help_text):
                st.session_state.current_scan_results = []
                st.rerun()
        
        st.markdown("---")
    
    # æ‰«ææ§åˆ¶åŒºåŸŸ
    col1, col2 = st.columns([3, 1])
    
    with col1:
        # æ˜¾ç¤ºå½“å‰é…ç½®
        if ticker:
            if current_language == "ä¸­æ–‡":
                st.info(f"ğŸ“Š **ç›®æ ‡è‚¡ç¥¨**: {ticker} | **æ•°æ®å¹´ä»½**: {years}å¹´ | **æ£€æµ‹å™¨**: {len(st.session_state.selected_detector_classes)}ä¸ª")
            else:
                years_text = "year" if years == 1 else "years"
                detectors_text = "detector" if len(st.session_state.selected_detector_classes) == 1 else "detectors"
                st.info(f"ğŸ“Š **Target Stock**: {ticker} | **Data Period**: {years} {years_text} | **Detectors**: {len(st.session_state.selected_detector_classes)} {detectors_text}")
        else:
            warning_text = "è¯·è¾“å…¥è‚¡ç¥¨ä»£ç " if current_language == "ä¸­æ–‡" else "Please enter stock ticker"
            st.warning(warning_text)
    
    with col2:
        # æ‰«ææŒ‰é’®
        scan_button = st.button(
            lang_config["scan_button"],
            disabled=not ticker or not st.session_state.selected_detector_classes,
            use_container_width=True
        )
    
    # å¤„ç†æ‰«æè¯·æ±‚
    if scan_button and ticker and st.session_state.selected_detector_classes:
        # å¯åŠ¨å¤„ç†æµç¨‹
        status = analyzer.session_manager.get_processing_status()
        status.is_processing = True
        status.processing_step = 1
        status.stop_requested = False
        analyzer.session_manager.update_processing_status(status)
        
        st.rerun()

    # åœ¨æ¯æ¬¡é‡æ–°è¿è¡Œè„šæœ¬æ—¶ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦å¤„ç†
    status = analyzer.session_manager.get_processing_status()
    if status.is_processing:
        # å¦‚æœæ­£åœ¨å¤„ç†ï¼Œæ˜¾ç¤ºstatus
        current_step = status.current_status_label or (lang_config.get("processing_status", "Processing..."))
        
        with st.expander(status.current_status_label, expanded=True):
            st.markdown(f"**{status.current_status_label}**")
            
            if status.total_documents > 0:
                progress_text = lang_config["progress_text"].format(status.completed_documents, status.total_documents)
                # st.progress(status.progress_percentage / 100, text=progress_text)
            
            # åœæ­¢æŒ‰é’®
            if st.button(lang_config["stop_button"], key="stop_processing"):
                status.stop_requested = True
                status.is_processing = False
                status.current_status_label = lang_config["stop_success"]
                analyzer.session_manager.update_processing_status(status)
                st.rerun()
            
            # æ˜¾ç¤ºæ–‡æ¡£åˆ—è¡¨å’Œå¤„ç†çŠ¶æ€
            if status.documents:
                st.markdown("---")
                for idx, doc in enumerate(status.documents):
                    if idx < status.completed_documents:
                        status_icon = "âœ…"
                    elif idx == status.completed_documents:
                        status_icon = "ğŸ”„"
                    else:
                        status_icon = "â³"
                    
                    # å„ªåŒ–æ–‡æ¡£æ ‡é¢˜æ˜¾ç¤ºï¼Œå¢åŠ é•·åº¦é™åˆ¶
                    doc_title = doc.title
                    if len(doc_title) > 80:
                        doc_title = doc_title[:77] + "..."
                    
                    st.markdown(f"{status_icon} {doc_title} ({doc.date})")
            
            # æ˜¾ç¤ºé”™è¯¯æ¶ˆæ¯
            if status.error_message:
                st.error(f"âŒ {status.error_message}")
        
        # è¿è¡Œåšç©ºä¿¡å·æ‰«ææµç¨‹
        process_short_signal_scan(
            analyzer, short_analyzer, ticker, years, 
            st.session_state.analyzer_use_sec_reports,
            st.session_state.analyzer_use_sec_others,
            use_earnings, st.session_state.selected_detector_classes, model_type
        )

def process_short_signal_scan(analyzer: SECEarningsAnalyzer, short_analyzer: ShortSignalAnalyzer, ticker: str, years: int, use_sec_reports: bool, use_sec_others: bool, use_earnings: bool, selected_detector_classes: List[str], model_type: str):
    """å¤„ç†åšç©ºä¿¡å·æ‰«æçš„å®Œæ•´æµç¨‹"""
    status = analyzer.session_manager.get_processing_status()
    language = st.session_state.get("selected_language", "English")
    
    # æ£€æŸ¥æ˜¯å¦å·²è¯·æ±‚åœæ­¢
    if status.stop_requested:
        return
    
    try:
        # æ­¥éª¤1ï¼šè·å–æ–‡æ¡£
        if status.processing_step == 1:
            # æ¸…ç†ä¹‹å‰çš„detection_results
            if 'detection_results' in st.session_state:
                del st.session_state['detection_results']
            # æ¸…ç†ä¹‹å‰çš„ä¸­é—´ç»“æœ
            st.session_state.current_scan_results = []
            # é‡ç½®æ£€æµ‹å™¨ç´¢å¼•
            st.session_state.current_detector_index = 0
                
            if language == "English":
                status.current_status_label = "ğŸ“‚ Retrieving documents for analysis..."
                status.add_status_message("ğŸ” Started document retrieval for short signal analysis")
            else:
                status.current_status_label = "ğŸ“‚ æ­£åœ¨è·å–åˆ†ææ–‡æ¡£..."
                status.add_status_message("ğŸ” å¼€å§‹ä¸ºåšç©ºä¿¡å·åˆ†æè·å–æ–‡æ¡£")
            
            analyzer.session_manager.update_processing_status(status)
            
            all_docs = []

            # å®šä¹‰è¡¨å•ç»„
            REPORTS_FORMS = ['10-K', '10-Q', '20-F', '6-K', '424B4']
            OTHER_FORMS = ['8-K', 'S-8', 'DEF 14A', 'F-3']
            
            selected_forms = []
            if use_sec_reports:
                selected_forms.extend(REPORTS_FORMS)
            if use_sec_others:
                selected_forms.extend(OTHER_FORMS)

            # è·å–æ–‡ä»¶ - æ ¹æ®è‚¡ç¥¨ä»£ç ç±»å‹é€‰æ‹©ä¸åŒçš„æœåŠ¡
            if selected_forms:
                if is_hk_stock(ticker):
                    # æ¸¯è‚¡æ–‡ä»¶
                    status.current_status_label = "ğŸ¢ æ­£åœ¨è¿æ¥æ¸¯è‚¡äº¤æ˜“æ‰€..." if language == "ä¸­æ–‡" else "ğŸ¢ Connecting to Hong Kong Stock Exchange..."
                    status.add_status_message("ğŸ¢ æ­£åœ¨è¿æ¥æ¸¯è‚¡äº¤æ˜“æ‰€...")
                    analyzer.session_manager.update_processing_status(status)
                    
                    # å°†è¡¨å•ç±»å‹è½¬æ¢ä¸ºæ¸¯è‚¡åˆ†ç±»
                    hk_forms = []
                    if any(form in REPORTS_FORMS for form in selected_forms):
                        hk_forms.append('quarterly_annual')
                    if any(form in OTHER_FORMS for form in selected_forms):
                        hk_forms.append('others')
                    
                    def hk_status_callback(msg):
                        status.add_status_message(msg)
                        analyzer.session_manager.update_processing_status(status)
                    
                    hk_filings = analyzer.hk_service.get_hk_filings(ticker, years, forms_to_include=hk_forms, status_callback=hk_status_callback)
                    all_docs.extend(hk_filings)
                    status.add_status_message(f"âœ… æˆåŠŸè·å– {len(hk_filings)} ä»½æ¸¯è‚¡æ–‡ä»¶")
                else:
                    # ç¾è‚¡SECæ–‡ä»¶
                    status.current_status_label = "ğŸ‡ºğŸ‡¸ æ­£åœ¨è¿æ¥SECæ•°æ®åº“..." if language == "ä¸­æ–‡" else "ğŸ‡ºğŸ‡¸ Connecting to SEC database..."
                    status.add_status_message("ğŸ‡ºğŸ‡¸ æ­£åœ¨è¿æ¥SECæ•°æ®åº“...")
                    analyzer.session_manager.update_processing_status(status)
                    
                    def sec_status_callback(msg):
                        status.add_status_message(msg)
                        analyzer.session_manager.update_processing_status(status)
                    
                    sec_filings = analyzer.sec_service.get_filings(ticker, years, forms_to_include=selected_forms, status_callback=sec_status_callback)
                    all_docs.extend(sec_filings)
                    status.add_status_message(f"âœ… æˆåŠŸè·å– {len(sec_filings)} ä»½SECæ–‡ä»¶")
            
            # è·å–è´¢æŠ¥è®°å½•
            if use_earnings:
                status.current_status_label = "ğŸ™ï¸ æ­£åœ¨è·å–è´¢æŠ¥ä¼šè®®è®°å½•..." if language == "ä¸­æ–‡" else "ğŸ™ï¸ Retrieving earnings call transcripts..."
                status.add_status_message("ğŸ™ï¸ æ­£åœ¨è·å–è´¢æŠ¥ä¼šè®®è®°å½•...")
                analyzer.session_manager.update_processing_status(status)
                
                all_earnings_urls = analyzer.earnings_service.get_available_quarters(ticker)
                
                # ä¿®æ­£å¹´ä»½è®¡ç®—é€»è¾‘
                current_year = datetime.now().year
                cutoff_date = datetime(current_year - years + 1, 1, 1).date()
                
                filtered_earnings_docs = []
                
                # æ‰¹é‡å¤„ç†è´¢æŠ¥è®°å½•
                batch_size = 3  # æ¯æ‰¹å¤„ç†3ä¸ª
                for batch_start in range(0, len(all_earnings_urls), batch_size):
                    if status.stop_requested:
                        break
                        
                    batch_end = min(batch_start + batch_size, len(all_earnings_urls))
                    batch_urls = all_earnings_urls[batch_start:batch_end]
                    
                    status.add_status_message(f"ğŸ“„ å¤„ç†è´¢æŠ¥æ‰¹æ¬¡ {batch_start//batch_size + 1}/{(len(all_earnings_urls) + batch_size - 1)//batch_size}")
                    analyzer.session_manager.update_processing_status(status)
                    
                    # æ‰¹é‡å¤„ç†å½“å‰æ‰¹æ¬¡
                    batch_results = analyzer.earnings_service.get_earnings_transcript_batch(batch_urls, max_workers=1)
                    
                    # å¤„ç†æ‰¹æ¬¡ç»“æœ
                    for url_path, transcript_info in zip(batch_urls, batch_results):
                        if status.stop_requested:
                            break
                            
                        if transcript_info and transcript_info.get('parsed_successfully'):
                            real_date = transcript_info.get('date')
                            if real_date and real_date >= cutoff_date:
                                doc = Document(
                                    type='Earnings Call',
                                    title=f"{transcript_info['ticker']} {transcript_info['year']} Q{transcript_info['quarter']} Earnings Call",
                                    date=real_date,
                                    url=url_path,
                                    content=transcript_info.get('content'),
                                    year=transcript_info.get('year'),
                                    quarter=transcript_info.get('quarter')
                                )
                                filtered_earnings_docs.append(doc)
                            else:
                                status.add_status_message(f"è´¢æŠ¥æ—¥æœŸ {real_date} æ—©äºæˆªæ­¢æ—¥æœŸï¼Œåœæ­¢è·å–")
                                break
                    
                    # å¦‚æœå‘ç°æ—¥æœŸè¿‡æ—©ï¼Œåœæ­¢å¤„ç†
                    if batch_results and any(
                        result and result.get('parsed_successfully') and 
                        result.get('date') and result.get('date') < cutoff_date 
                        for result in batch_results
                    ):
                        break
                
                all_docs.extend(filtered_earnings_docs)
                status.add_status_message(f"âœ… æˆåŠŸè·å– {len(filtered_earnings_docs)} ä»½è´¢æŠ¥è®°å½•")
            
            # æ’åºå¹¶å‡†å¤‡æ–‡æ¡£
            all_docs.sort(key=lambda x: x.date, reverse=True)
            status.documents = all_docs
            status.update_progress(0, len(all_docs), "æ–‡æ¡£è·å–å®Œæˆ")
            status.add_status_message(f"âœ… æ–‡æ¡£è·å–å®Œæˆï¼Œå…± {len(all_docs)} ä»½")
            status.processing_step = 2
            analyzer.session_manager.update_processing_status(status)
            st.rerun()

        # æ­¥éª¤2ï¼šä¸‹è½½æ–‡æ¡£å†…å®¹
        elif status.processing_step == 2:
            if status.stop_requested:
                return
                
            docs_to_process = status.documents
            
            status.current_status_label = "ğŸ“¥ æ­£åœ¨ä¸‹è½½æ–‡æ¡£å†…å®¹..." if language == "ä¸­æ–‡" else "ğŸ“¥ Downloading document contents..."
            status.add_status_message("ğŸ“¥ å¼€å§‹ä¸‹è½½æ–‡æ¡£å†…å®¹...")
            analyzer.session_manager.update_processing_status(status)
            
            # ä¸‹è½½æ‰€æœ‰æ–‡æ¡£å†…å®¹
            for idx, doc in enumerate(docs_to_process):
                if status.stop_requested:
                    break
                    
                if not doc.content:
                    status.add_status_message(f"ğŸ“¥ ä¸‹è½½æ–‡æ¡£ {idx+1}/{len(docs_to_process)}: {doc.title}")
                    status.update_progress(idx, len(docs_to_process), f"ä¸‹è½½æ–‡æ¡£ {idx+1}/{len(docs_to_process)}")
                    analyzer.session_manager.update_processing_status(status)
                    
                    if doc.type == 'SEC Filing':
                        doc.content = analyzer.sec_service.download_filing(doc.url)
                    elif doc.type == 'HK Stock Filing':
                        doc.content = analyzer.hk_service.download_hk_filing(doc.url)
                    # Earnings Call å†…å®¹å·²ç»é¢„å…ˆè·å–
            
            status.add_status_message("âœ… æ–‡æ¡£å†…å®¹ä¸‹è½½å®Œæˆ")
            status.processing_step = 3
            analyzer.session_manager.update_processing_status(status)
            st.rerun()

        # æ­¥éª¤3ï¼šè¿è¡Œæ£€æµ‹å™¨
        elif status.processing_step == 3:
            if status.stop_requested:
                return
                
            status.current_status_label = "ğŸ” æ­£åœ¨è¿è¡Œåšç©ºä¿¡å·æ£€æµ‹..." if language == "ä¸­æ–‡" else "ğŸ” Running short signal detection..."
            status.add_status_message("ğŸ” å¼€å§‹è¿è¡Œåšç©ºä¿¡å·æ£€æµ‹...")
            analyzer.session_manager.update_processing_status(status)
            
            # é€ä¸ªè¿è¡Œæ£€æµ‹å™¨ï¼Œæ¯ä¸ªå®Œæˆåç«‹å³æ˜¾ç¤ºç»“æœ
            if 'current_detector_index' not in st.session_state:
                st.session_state.current_detector_index = 0
            
            available_detectors = [d for d in short_analyzer.detectors if d.__class__.__name__ in selected_detector_classes]
            
            if st.session_state.current_detector_index < len(available_detectors):
                # è¿è¡Œå½“å‰æ£€æµ‹å™¨
                current_detector = available_detectors[st.session_state.current_detector_index]
                status.current_status_label = f"ğŸ” æ­£åœ¨è¿è¡Œ: {current_detector.name}..."
                status.add_status_message(f"ğŸ” è¿è¡Œæ£€æµ‹å™¨: {current_detector.name}")
                analyzer.session_manager.update_processing_status(status)
                
                try:
                    result = current_detector.detect(status.documents, model_type)
                    
                    # æ›´æ–°å½“å‰æ‰«æç»“æœ
                    if 'current_scan_results' not in st.session_state:
                        st.session_state.current_scan_results = []
                    st.session_state.current_scan_results.append(result)
                    
                    status.add_status_message(f"âœ… {current_detector.name} å®Œæˆï¼Œå‘ç° {len(result.signals)} ä¸ªä¿¡å·")
                    
                except Exception as e:
                    logger.error(f"æ£€æµ‹å™¨ {current_detector.name} æ‰§è¡Œå¤±è´¥: {e}")
                    error_result = DetectionResult(
                        detector_name=current_detector.name,
                        signals=[],
                        processing_time=0,
                        success=False,
                        error_message=str(e)
                    )
                    
                    if 'current_scan_results' not in st.session_state:
                        st.session_state.current_scan_results = []
                    st.session_state.current_scan_results.append(error_result)
                    
                    status.add_status_message(f"âŒ {current_detector.name} æ‰§è¡Œå¤±è´¥: {e}")
                
                # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªæ£€æµ‹å™¨
                st.session_state.current_detector_index += 1
                analyzer.session_manager.update_processing_status(status)
                st.rerun()
            else:
                # æ‰€æœ‰æ£€æµ‹å™¨å®Œæˆ
                detection_results = st.session_state.current_scan_results
                status.add_status_message("âœ… æ‰€æœ‰æ£€æµ‹å™¨æ‰§è¡Œå®Œæˆ")
                
                # å•ç‹¬ä¿å­˜detection_resultsåˆ°session_stateï¼Œå› ä¸ºå®ƒåŒ…å«å¤æ‚å¯¹è±¡
                st.session_state.detection_results = detection_results
                
                # è¿›å…¥ä¸‹ä¸€æ­¥
                status.processing_step = 4
                analyzer.session_manager.update_processing_status(status)
                st.rerun()

        # æ­¥éª¤4ï¼šç”Ÿæˆç»¼åˆæŠ¥å‘Š
        elif status.processing_step == 4:
            if status.stop_requested:
                return
                
            status.current_status_label = "ğŸ“ æ­£åœ¨ç”Ÿæˆç»¼åˆæŠ¥å‘Š..." if language == "ä¸­æ–‡" else "ğŸ“ Generating comprehensive report..."
            status.add_status_message("ğŸ“ å¼€å§‹ç”Ÿæˆç»¼åˆæŠ¥å‘Š...")
            analyzer.session_manager.update_processing_status(status)
            
            # ä»session_stateè·å–detection_results
            detection_results = st.session_state.get('detection_results', [])
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ£€æµ‹ç»“æœ
            if not detection_results:
                st.error("âŒ æ£€æµ‹ç»“æœä¸¢å¤±ï¼Œè¯·é‡æ–°è¿è¡Œæ‰«æ")
                logger.error("Detection results not found in session state")
                return
            
            # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            comprehensive_report = short_analyzer.generate_comprehensive_report(
                detection_results, 
                ticker, 
                model_type
            )
            
            # æ˜¾ç¤ºç»¼åˆæŠ¥å‘Š
            st.subheader("ğŸ“Š ç»¼åˆåšç©ºä¿¡å·æŠ¥å‘Š")
            st.markdown(comprehensive_report)
            
            # ä¿å­˜æ‰«æç»“æœåˆ°å†å²è®°å½•
            scan_result = {
                'ticker': ticker,
                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'report': comprehensive_report,
                'signals_count': sum(len(r.signals) for r in detection_results),
                'high_risk_count': sum(len([s for s in r.signals if s.severity == "High"]) for r in detection_results)
            }
            
            st.session_state.short_scanner_results.append(scan_result)
            
            # å®Œæˆå¤„ç†
            status.current_status_label = "âœ… æ‰«æå®Œæˆï¼" if language == "ä¸­æ–‡" else "âœ… Scan completed!"
            status.add_status_message("âœ… åšç©ºä¿¡å·æ‰«æå®Œæˆ")
            status.progress_percentage = 100.0
            analyzer.session_manager.update_processing_status(status)
            
            # çŸ­æš‚æ˜¾ç¤ºå®ŒæˆçŠ¶æ€
            time.sleep(0.1)
            
            # ä¸è¦ç«‹å³æ¸…ç†æ‰«æç»“æœï¼Œè®©ç”¨æˆ·å¯ä»¥æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯
            # st.session_state.current_scan_results = []  # æ³¨é‡Šæ‰è¿™è¡Œ
            
            # é‡ç½®çŠ¶æ€
            status = ProcessingStatus()
            analyzer.session_manager.update_processing_status(status)
            st.rerun()

    except Exception as e:
        logger.error(f"åšç©ºä¿¡å·æ‰«æå‡ºé”™: {e}", exc_info=True)
        error_msg = f"æ‰«æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}" if language == "ä¸­æ–‡" else f"Error during scan: {e}"
        st.error(error_msg)
        # é‡ç½®çŠ¶æ€
        status = ProcessingStatus()
        analyzer.session_manager.update_processing_status(status)
        st.rerun()


def process_user_question_new(analyzer: SECEarningsAnalyzer, ticker: str, years: int, use_sec_reports: bool, use_sec_others: bool, use_earnings: bool, model_type: str):
    """å¤„ç†ç”¨æˆ·é—®é¢˜çš„å®Œæ•´æµç¨‹ - æ–°ç‰ˆï¼Œå¸¦å®æ—¶çŠ¶æ€æ›´æ–°å’Œå¹¶è¡Œå¤„ç†"""
    status = analyzer.session_manager.get_processing_status()
    language = st.session_state.get("selected_language", "English")
    
    # æ£€æŸ¥æ˜¯å¦å·²è¯·æ±‚åœæ­¢
    if status.stop_requested:
        return
    
    try:
        # æ­¥éª¤1ï¼šåˆ†æé—®é¢˜
        if status.processing_step == 1:
            if language == "English":
                status.current_status_label = "ğŸ§  Analyzing your question..."
                status.add_status_message("Started analyzing user question")
                analyzer.session_manager.update_processing_status(status)
                
                status.current_status_label = "ğŸ” Parsing question content..."
                status.add_status_message("ğŸ” Parsing question content...")
                analyzer.session_manager.update_processing_status(status)
                
                status.current_status_label = "ğŸ¤– Calling AI model for analysis..."
                status.add_status_message("ğŸ¤– Calling AI model for analysis...")
                analyzer.session_manager.update_processing_status(status)
            else:
                status.current_status_label = "ğŸ§  æ­£åœ¨åˆ†ææ‚¨çš„é—®é¢˜..."
                status.add_status_message("é–‹å§‹åˆ†æç”¨æˆ¶å•é¡Œ")
                analyzer.session_manager.update_processing_status(status)
                
                status.current_status_label = "ğŸ” è§£æå•é¡Œå…§å®¹..."
                status.add_status_message("ğŸ” è§£æå•é¡Œå…§å®¹...")
                analyzer.session_manager.update_processing_status(status)
                
                status.current_status_label = "ğŸ¤– èª¿ç”¨AIæ¨¡å‹åˆ†æ..."
                status.add_status_message("ğŸ¤– èª¿ç”¨AIæ¨¡å‹åˆ†æ...")
                analyzer.session_manager.update_processing_status(status)
            
            # åˆ›å»ºAIåˆ†æçŠ¶æ€æ˜¾ç¤º
            ai_analysis_placeholder = st.empty()
            with ai_analysis_placeholder.status("ğŸ¤– AIæ­£åœ¨åˆ†ææ‚¨çš„é—®é¢˜...", expanded=True) as ai_analysis_status:
                if language == "English":
                    ai_analysis_status.write("ğŸ” Parsing question intent...")
                    ai_analysis_status.write(f"ğŸ“ Question: {status.user_question}")
                    ai_analysis_status.write(f"ğŸ“Š Stock: {ticker}")
                    ai_analysis_status.write("ğŸ§  Calling AI model to generate analysis prompts...")
                    ai_analysis_status.write("â³ Waiting for AI response...")
                else:
                    ai_analysis_status.write("ğŸ” æ­£åœ¨è§£æé—®é¢˜æ„å›¾...")
                    ai_analysis_status.write(f"ğŸ“ é—®é¢˜: {status.user_question}")
                    ai_analysis_status.write(f"ğŸ“Š è‚¡ç¥¨: {ticker}")
                    ai_analysis_status.write("ğŸ§  æ­£åœ¨è°ƒç”¨AIæ¨¡å‹ç”Ÿæˆåˆ†ææç¤ºè¯...")
                    ai_analysis_status.write("â³ ç­‰å¾…AIå“åº”ä¸­...")
                
                # æ‰§è¡Œå®é™…çš„AIåˆ†æ
                processing_prompt, integration_prompt = analyzer.analyze_question(status.user_question, ticker, model_type)
                
                ai_analysis_status.write("âœ… AIåˆ†æå®Œæˆï¼")
                ai_analysis_status.update(label="âœ… é—®é¢˜åˆ†æå®Œæˆ", state="complete")
            
            # æ¸…é™¤AIåˆ†æçŠ¶æ€æ˜¾ç¤º
            ai_analysis_placeholder.empty()
            
            status.processing_prompt = processing_prompt
            status.integration_prompt = integration_prompt
            
            success_msg = "âœ… User question analysis completed" if language == "English" else "âœ… ç”¨æˆ¶å•é¡Œåˆ†æå®Œæˆ"
            status.add_status_message(success_msg)
            status.processing_step = 2
            analyzer.session_manager.update_processing_status(status)
            time.sleep(0.1) # çŸ­æš‚åœç•™ï¼Œè®©ç”¨æˆ·çœ‹åˆ°æ¶ˆæ¯
            st.rerun()
        
        # æ­¥éª¤2ï¼šè·å–å’Œç­›é€‰æ–‡æ¡£
        elif status.processing_step == 2:
            if status.stop_requested:
                return
                
            if language == "English":
                status.current_status_label = "ğŸ“‚ Retrieving and filtering documents..."
                status.add_status_message("ğŸ” Started document retrieval")
                analyzer.session_manager.update_processing_status(status)
                
                all_docs = []

                # å®šä¹‰è¡¨å•ç»„
                REPORTS_FORMS = ['10-K', '10-Q', '20-F', '6-K', '424B4']
                OTHER_FORMS = ['8-K', 'S-8', 'DEF 14A', 'F-3']
                
                status.add_status_message("ğŸ“‹ Preparing document type filtering...")
                analyzer.session_manager.update_processing_status(status)
                
                selected_forms = []
                if use_sec_reports:
                    selected_forms.extend(REPORTS_FORMS)
                if use_sec_others:
                    selected_forms.extend(OTHER_FORMS)

                # è·å–æ–‡ä»¶ - æ ¹æ®è‚¡ç¥¨ä»£ç ç±»å‹é€‰æ‹©ä¸åŒçš„æœåŠ¡
                if selected_forms:
                    if is_hk_stock(ticker):
                        # æ¸¯è‚¡æ–‡ä»¶
                        status.current_status_label = "ğŸ¢ Connecting to Hong Kong Stock Exchange..."
                        status.add_status_message("ğŸ¢ Connecting to Hong Kong Stock Exchange...")
                        analyzer.session_manager.update_processing_status(status)
                        
                        status.current_status_label = "ğŸ“„ Retrieving Hong Kong stock filings list..."
                        status.add_status_message("ğŸ“„ Retrieving Hong Kong stock filings list...")
                        analyzer.session_manager.update_processing_status(status)
                        
                        # å°†è¡¨å•ç±»å‹è½¬æ¢ä¸ºæ¸¯è‚¡åˆ†ç±»
                        hk_forms = []
                        if any(form in REPORTS_FORMS for form in selected_forms):
                            hk_forms.append('quarterly_annual')
                        if any(form in OTHER_FORMS for form in selected_forms):
                            hk_forms.append('others')
                        
                        def hk_status_callback(msg):
                            status.add_status_message(msg)
                            analyzer.session_manager.update_processing_status(status)
                        
                        hk_filings = analyzer.hk_service.get_hk_filings(ticker, years, forms_to_include=hk_forms, status_callback=hk_status_callback)
                        all_docs.extend(hk_filings)
                        status.add_status_message(f"âœ… Successfully retrieved {len(hk_filings)} Hong Kong stock filings")
                    else:
                        # ç¾è‚¡SECæ–‡ä»¶
                        status.current_status_label = "ğŸ‡ºğŸ‡¸ Connecting to SEC database..."
                        status.add_status_message("ğŸ‡ºğŸ‡¸ Connecting to SEC database...")
                        analyzer.session_manager.update_processing_status(status)
                        
                        status.current_status_label = "ğŸ“„ Retrieving SEC filings list..."
                        status.add_status_message("ğŸ“„ Retrieving SEC filings list...")
                        analyzer.session_manager.update_processing_status(status)
                        
                        def sec_status_callback(msg):
                            status.add_status_message(msg)
                            analyzer.session_manager.update_processing_status(status)
                        
                        sec_filings = analyzer.sec_service.get_filings(ticker, years, forms_to_include=selected_forms, status_callback=sec_status_callback)
                        all_docs.extend(sec_filings)
                        status.add_status_message(f"âœ… Successfully retrieved {len(sec_filings)} SEC filings")
                
                # è·å–è´¢æŠ¥è®°å½• - æ”¯æŒç¾è‚¡å’Œæ¸¯è‚¡
                if use_earnings:
                    status.current_status_label = "ğŸ™ï¸ Connecting to earnings call transcript service..."
                    status.add_status_message("ğŸ™ï¸ Connecting to earnings call transcript service...")
                    analyzer.session_manager.update_processing_status(status)
                    
                    status.current_status_label = "ğŸ“‹ Retrieving available earnings call list..."
                    status.add_status_message("ğŸ“‹ Retrieving available earnings call list...")
                    analyzer.session_manager.update_processing_status(status)
                    
                    all_earnings_urls = analyzer.earnings_service.get_available_quarters(ticker)
                    
                    # ä¿®æ­£å¹´ä»½è®¡ç®—é€»è¾‘ï¼šä¸SECä¿æŒä¸€è‡´
                    current_year = datetime.now().year
                    cutoff_date = datetime(current_year - years + 1, 1, 1).date()  # å¾€å‰æ¨yearså¹´
                    status.add_status_message(f"â° Started retrieving earnings calls and filtering by cutoff date ({cutoff_date})...")
                    analyzer.session_manager.update_processing_status(status)

                    filtered_earnings_docs = []
                    
                    # ä½¿ç”¨å¹¶è¡Œå¤„ç†æ¥æå‡é€Ÿåº¦
                    status.add_status_message("ğŸ“„ Starting parallel processing of earnings calls...")
                    analyzer.session_manager.update_processing_status(status)
                    
                    # åˆ›å»ºearningsè·å–çŠ¶æ€æ˜¾ç¤º
                    earnings_status_placeholder = st.empty()
                    with earnings_status_placeholder.status("ğŸ™ï¸ Retrieving earnings call transcripts...", expanded=True) as earnings_status:
                        earnings_status.write(f"ğŸ“‹ Found {len(all_earnings_urls)} available earnings calls")
                        earnings_status.write(f"ğŸ“… Filtering by cutoff date: {cutoff_date}")
                        earnings_status.write("ğŸ”„ Starting batch processing...")
                        
                        # åˆ†æ‰¹å¤„ç†ä»¥é¿å…è¿‡å¤šå¹¶å‘è¯·æ±‚
                        batch_size = 1  # æ¯æ‰¹å¤„ç†1ä¸ª
                        for batch_start in range(0, len(all_earnings_urls), batch_size):
                            if status.stop_requested:
                                break
                                
                            batch_end = min(batch_start + batch_size, len(all_earnings_urls))
                            batch_urls = all_earnings_urls[batch_start:batch_end]
                            
                            status.add_status_message(f"ğŸ“„ Processing batch {batch_start//batch_size + 1}/{(len(all_earnings_urls) + batch_size - 1)//batch_size} ({batch_start + 1}-{batch_end}/{len(all_earnings_urls)})")
                            analyzer.session_manager.update_processing_status(status)
                            
                            # æ˜¾ç¤ºå½“å‰æ‰¹æ¬¡æ­£åœ¨å¤„ç†çš„earnings
                            earnings_status.write(f"ğŸ“¦ Processing batch {batch_start//batch_size + 1}/{(len(all_earnings_urls) + batch_size - 1)//batch_size}")
                            
                            # æ˜¾ç¤ºå½“å‰æ‰¹æ¬¡çš„å…·ä½“é¡¹ç›®
                            for url_path in batch_urls:
                                parsed_info = analyzer.earnings_service.parse_transcript_url(url_path)
                                if parsed_info:
                                    _ticker, year, quarter = parsed_info
                                    earnings_status.write(f"â³ å¼€å§‹è·å–: {_ticker} {year} Q{quarter}")
                            
                            # é¡ºåºå¤„ç†å½“å‰æ‰¹æ¬¡
                            batch_results = analyzer.earnings_service.get_earnings_transcript_batch(batch_urls, max_workers=1)
                            
                            # å¤„ç†æ‰¹æ¬¡ç»“æœ
                            for i, (url_path, transcript_info) in enumerate(zip(batch_urls, batch_results)):
                                if status.stop_requested:
                                    break
                                    
                                parsed_info = analyzer.earnings_service.parse_transcript_url(url_path)
                                if parsed_info:
                                    _ticker, year, quarter = parsed_info
                                    
                                    if transcript_info and transcript_info.get('parsed_successfully'):
                                        real_date = transcript_info.get('date')
                                        if real_date:
                                            if real_date >= cutoff_date:
                                                doc = Document(
                                                    type='Earnings Call',
                                                    title=f"{transcript_info['ticker']} {transcript_info['year']} Q{transcript_info['quarter']} Earnings Call",
                                                    date=real_date, url=url_path, content=transcript_info.get('content'),
                                                    year=transcript_info.get('year'), quarter=transcript_info.get('quarter')
                                                )
                                                filtered_earnings_docs.append(doc)
                                                earnings_status.write(f"âœ… æˆåŠŸè·å–: {_ticker} {year} Q{quarter} ({real_date})")
                                            else:
                                                earnings_status.write(f"â¹ï¸ æ—¥æœŸè¿‡æ—©ï¼Œåœæ­¢è·å–: {_ticker} {year} Q{quarter} ({real_date})")
                                                status.add_status_message(f"Earnings call date {real_date} is earlier than cutoff date, stopping retrieval")
                                                time.sleep(0.1)
                                                break
                                    else:
                                        earnings_status.write(f"âš ï¸ è·å–å¤±è´¥: {_ticker} {year} Q{quarter}")
                                        logger.warning(f"Failed to retrieve or parse earnings call, skipping: {_ticker} {year} Q{quarter}")
                            
                            # å¦‚æœå‘ç°æ—¥æœŸè¿‡æ—©ï¼Œåœæ­¢å¤„ç†
                            if batch_results and any(
                                result and result.get('parsed_successfully') and 
                                result.get('date') and result.get('date') < cutoff_date 
                                for result in batch_results
                            ):
                                break
                        
                        earnings_status.write(f"âœ… å®Œæˆï¼å…±è·å– {len(filtered_earnings_docs)} ä¸ªæœ‰æ•ˆçš„è´¢æŠ¥è®°å½•")
                        earnings_status.update(label="âœ… Earnings call retrieval completed", state="complete")
                    
                    # æ¸…é™¤earningsçŠ¶æ€æ˜¾ç¤º
                    earnings_status_placeholder.empty()
                    
                    all_docs.extend(filtered_earnings_docs)
                    status.add_status_message(f"âœ… Successfully filtered {len(filtered_earnings_docs)} relevant earnings calls")
                    analyzer.session_manager.update_processing_status(status)

                status.add_status_message("ğŸ“Š Organizing document list...")
                analyzer.session_manager.update_processing_status(status)
                
                all_docs.sort(key=lambda x: x.date, reverse=True)
                status.documents = all_docs
                status.update_progress(0, len(all_docs), "Document list ready")
                status.add_status_message(f"âœ… Document list ready, total {len(all_docs)} documents")
                status.processing_step = 3
                analyzer.session_manager.update_processing_status(status)
            else:
                # ä¸­æ–‡ç‰ˆæœ¬çš„æ¶ˆæ¯
                status.current_status_label = "ğŸ“‚ æ­£åœ¨æ£€ç´¢å’Œç­›é€‰æ–‡æ¡£..."
                status.add_status_message("ğŸ” é–‹å§‹æª¢ç´¢æ–‡æª”")
                analyzer.session_manager.update_processing_status(status)
                
                all_docs = []

                # å®šä¹‰è¡¨å•ç»„
                REPORTS_FORMS = ['10-K', '10-Q', '20-F', '6-K', '424B4']
                OTHER_FORMS = ['8-K', 'S-8', 'DEF 14A', 'F-3']
                
                status.add_status_message("ğŸ“‹ æº–å‚™æ–‡æª”é¡å‹ç¯©é¸...")
                analyzer.session_manager.update_processing_status(status)
                
                selected_forms = []
                if use_sec_reports:
                    selected_forms.extend(REPORTS_FORMS)
                if use_sec_others:
                    selected_forms.extend(OTHER_FORMS)

                # è·å–æ–‡ä»¶ - æ ¹æ®è‚¡ç¥¨ä»£ç ç±»å‹é€‰æ‹©ä¸åŒçš„æœåŠ¡
                if selected_forms:
                    if is_hk_stock(ticker):
                        # æ¸¯è‚¡æ–‡ä»¶
                        status.current_status_label = "ğŸ¢ æ­£åœ¨é€£æ¥æ¸¯è‚¡äº¤æ˜“æ‰€..."
                        status.add_status_message("ğŸ¢ æ­£åœ¨é€£æ¥æ¸¯è‚¡äº¤æ˜“æ‰€...")
                        analyzer.session_manager.update_processing_status(status)
                        
                        status.current_status_label = "ğŸ“„ æ­£åœ¨è·å–æ¸¯è‚¡æ–‡ä»¶åˆ—è¡¨..."
                        status.add_status_message("ğŸ“„ æ­£åœ¨è·å–æ¸¯è‚¡æ–‡ä»¶åˆ—è¡¨...")
                        analyzer.session_manager.update_processing_status(status)
                        
                        # å°†è¡¨å•ç±»å‹è½¬æ¢ä¸ºæ¸¯è‚¡åˆ†ç±»
                        hk_forms = []
                        if any(form in REPORTS_FORMS for form in selected_forms):
                            hk_forms.append('quarterly_annual')
                        if any(form in OTHER_FORMS for form in selected_forms):
                            hk_forms.append('others')
                        
                        def hk_status_callback(msg):
                            status.add_status_message(msg)
                            analyzer.session_manager.update_processing_status(status)
                        
                        hk_filings = analyzer.hk_service.get_hk_filings(ticker, years, forms_to_include=hk_forms, status_callback=hk_status_callback)
                        all_docs.extend(hk_filings)
                        status.add_status_message(f"âœ… æˆåŠŸè·å– {len(hk_filings)} ä»½æ¸¯è‚¡æ–‡ä»¶")
                    else:
                        # ç¾è‚¡SECæ–‡ä»¶
                        status.current_status_label = "ğŸ‡ºğŸ‡¸ æ­£åœ¨é€£æ¥SECæ•¸æ“šåº«..."
                        status.add_status_message("ğŸ‡ºğŸ‡¸ æ­£åœ¨é€£æ¥SECæ•¸æ“šåº«...")
                        analyzer.session_manager.update_processing_status(status)
                        
                        status.current_status_label = "ğŸ“„ æ­£åœ¨è·å–SECæ–‡ä»¶åˆ—è¡¨..."
                        status.add_status_message("ğŸ“„ æ­£åœ¨è·å–SECæ–‡ä»¶åˆ—è¡¨...")
                        analyzer.session_manager.update_processing_status(status)
                        
                        def sec_status_callback(msg):
                            status.add_status_message(msg)
                            analyzer.session_manager.update_processing_status(status)
                        
                        sec_filings = analyzer.sec_service.get_filings(ticker, years, forms_to_include=selected_forms, status_callback=sec_status_callback)
                        all_docs.extend(sec_filings)
                        status.add_status_message(f"âœ… æˆåŠŸè·å– {len(sec_filings)} ä»½SECæ–‡ä»¶")
                
                # è·å–è´¢æŠ¥è®°å½• - æ”¯æŒç¾è‚¡å’Œæ¸¯è‚¡
                if use_earnings:
                    status.current_status_label = "ğŸ™ï¸ æ­£åœ¨é€£æ¥è²¡å ±æœƒè­°è¨˜éŒ„æœå‹™..."
                    status.add_status_message("ğŸ™ï¸ æ­£åœ¨é€£æ¥è²¡å ±æœƒè­°è¨˜éŒ„æœå‹™...")
                    analyzer.session_manager.update_processing_status(status)
                    
                    status.current_status_label = "ğŸ“‹ æ­£åœ¨è·å–å¯ç”¨è´¢æŠ¥åˆ—è¡¨..."
                    status.add_status_message("ğŸ“‹ æ­£åœ¨è·å–å¯ç”¨è´¢æŠ¥åˆ—è¡¨...")
                    analyzer.session_manager.update_processing_status(status)
                    
                    all_earnings_urls = analyzer.earnings_service.get_available_quarters(ticker)
                    
                    # ä¿®æ­£å¹´ä»½è®¡ç®—é€»è¾‘ï¼šä¸SECä¿æŒä¸€è‡´
                    current_year = datetime.now().year
                    cutoff_date = datetime(current_year - years + 1, 1, 1).date()  # å¾€å‰æ¨yearså¹´
                    status.add_status_message(f"â° é–‹å§‹é€ä¸€è·å–è´¢æŠ¥ä¸¦æŒ‰æˆªæ­¢æ—¥æœŸ ({cutoff_date}) ç¯©é¸...")
                    analyzer.session_manager.update_processing_status(status)

                    filtered_earnings_docs = []
                    
                    # ä½¿ç”¨å¹¶è¡Œå¤„ç†æ¥æå‡é€Ÿåº¦
                    status.add_status_message("ğŸ“„ å¼€å§‹å¹¶è¡Œå¤„ç†è´¢æŠ¥è®°å½•...")
                    analyzer.session_manager.update_processing_status(status)
                    
                    # åˆ›å»ºearningsè·å–çŠ¶æ€æ˜¾ç¤º
                    earnings_status_placeholder = st.empty()
                    with earnings_status_placeholder.status("ğŸ™ï¸ æ­£åœ¨è·å–è´¢æŠ¥ä¼šè®®è®°å½•...", expanded=True) as earnings_status:
                        earnings_status.write(f"ğŸ“‹ æ‰¾åˆ° {len(all_earnings_urls)} ä¸ªå¯ç”¨çš„è´¢æŠ¥è®°å½•")
                        earnings_status.write(f"ğŸ“… æŒ‰æˆªæ­¢æ—¥æœŸç­›é€‰: {cutoff_date}")
                        earnings_status.write("ğŸ”„ å¼€å§‹æ‰¹é‡å¤„ç†...")
                        
                        # åˆ†æ‰¹å¤„ç†ä»¥é¿å…è¿‡å¤šå¹¶å‘è¯·æ±‚
                        batch_size = 6  # æ¯æ‰¹å¤„ç†6ä¸ª
                        for batch_start in range(0, len(all_earnings_urls), batch_size):
                            if status.stop_requested:
                                break
                                
                            batch_end = min(batch_start + batch_size, len(all_earnings_urls))
                            batch_urls = all_earnings_urls[batch_start:batch_end]
                            
                            status.add_status_message(f"ğŸ“„ å¤„ç†æ‰¹æ¬¡ {batch_start//batch_size + 1}/{(len(all_earnings_urls) + batch_size - 1)//batch_size} ({batch_start + 1}-{batch_end}/{len(all_earnings_urls)})")
                            analyzer.session_manager.update_processing_status(status)
                            
                            # æ˜¾ç¤ºå½“å‰æ‰¹æ¬¡æ­£åœ¨å¤„ç†çš„earnings
                            earnings_status.write(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_start//batch_size + 1}/{(len(all_earnings_urls) + batch_size - 1)//batch_size}")
                            
                            # æ˜¾ç¤ºå½“å‰æ‰¹æ¬¡çš„å…·ä½“é¡¹ç›®
                            for url_path in batch_urls:
                                parsed_info = analyzer.earnings_service.parse_transcript_url(url_path)
                                if parsed_info:
                                    _ticker, year, quarter = parsed_info
                                    earnings_status.write(f"â³ å¼€å§‹è·å–: {_ticker} {year} Q{quarter}")
                            
                            # é¡ºåºå¤„ç†å½“å‰æ‰¹æ¬¡
                            batch_results = analyzer.earnings_service.get_earnings_transcript_batch(batch_urls, max_workers=1)
                            
                            # å¤„ç†æ‰¹æ¬¡ç»“æœ
                            for i, (url_path, transcript_info) in enumerate(zip(batch_urls, batch_results)):
                                if status.stop_requested:
                                    break
                                    
                                parsed_info = analyzer.earnings_service.parse_transcript_url(url_path)
                                if parsed_info:
                                    _ticker, year, quarter = parsed_info
                                    
                                    if transcript_info and transcript_info.get('parsed_successfully'):
                                        real_date = transcript_info.get('date')
                                        if real_date:
                                            if real_date >= cutoff_date:
                                                doc = Document(
                                                    type='Earnings Call',
                                                    title=f"{transcript_info['ticker']} {transcript_info['year']} Q{transcript_info['quarter']} Earnings Call",
                                                    date=real_date, url=url_path, content=transcript_info.get('content'),
                                                    year=transcript_info.get('year'), quarter=transcript_info.get('quarter')
                                                )
                                                filtered_earnings_docs.append(doc)
                                                earnings_status.write(f"âœ… æˆåŠŸè·å–: {_ticker} {year} Q{quarter} ({real_date})")
                                            else:
                                                earnings_status.write(f"â¹ï¸ æ—¥æœŸè¿‡æ—©ï¼Œåœæ­¢è·å–: {_ticker} {year} Q{quarter} ({real_date})")
                                                status.add_status_message(f"è´¢æŠ¥æ—¥æœŸ {real_date} æ—©äºæˆªæ­¢æ—¥æœŸï¼Œåœæ­¢è·å–")
                                                time.sleep(0.1)
                                                break
                                    else:
                                        earnings_status.write(f"âš ï¸ è·å–å¤±è´¥: {_ticker} {year} Q{quarter}")
                                        logger.warning(f"è·å–æˆ–è§£æè´¢æŠ¥å¤±è´¥ï¼Œè·³è¿‡: {_ticker} {year} Q{quarter}")
                            
                            # å¦‚æœå‘ç°æ—¥æœŸè¿‡æ—©ï¼Œåœæ­¢å¤„ç†
                            if batch_results and any(
                                result and result.get('parsed_successfully') and 
                                result.get('date') and result.get('date') < cutoff_date 
                                for result in batch_results
                            ):
                                break
                        
                        earnings_status.write(f"âœ… å®Œæˆï¼å…±è·å– {len(filtered_earnings_docs)} ä¸ªæœ‰æ•ˆçš„è´¢æŠ¥è®°å½•")
                        earnings_status.update(label="âœ… è´¢æŠ¥è®°å½•è·å–å®Œæˆ", state="complete")
                    
                    # æ¸…é™¤earningsçŠ¶æ€æ˜¾ç¤º
                    earnings_status_placeholder.empty()
                    
                    all_docs.extend(filtered_earnings_docs)
                    status.add_status_message(f"âœ… æˆåŠŸç­›é€‰å‡º {len(filtered_earnings_docs)} ä»½ç›¸å…³è´¢æŠ¥")
                    analyzer.session_manager.update_processing_status(status)

                status.add_status_message("ğŸ“Š æ­£åœ¨æ•´ç†æ–‡æª”åˆ—è¡¨...")
                analyzer.session_manager.update_processing_status(status)
                
                all_docs.sort(key=lambda x: x.date, reverse=True)
                status.documents = all_docs
                status.update_progress(0, len(all_docs), "æ–‡æ¡£åˆ—è¡¨å‡†å¤‡å°±ç»ª")
                status.add_status_message(f"âœ… æ–‡æ¡£åˆ—è¡¨å‡†å¤‡å°±ç»ªï¼Œå…± {len(all_docs)} ä»½")
                status.processing_step = 3
                analyzer.session_manager.update_processing_status(status)

            st.rerun()

        # æ­¥éª¤3ï¼šæŒ‰æ—¥æœŸé¡ºåºå¤„ç†æ–‡æ¡£
        elif status.processing_step == 3:
            if status.stop_requested:
                return
                
            docs_to_process = status.documents
            
            # åˆå§‹åŒ–å¤„ç†çŠ¶æ€
            if status.completed_documents == 0:
                status.document_results = []
            
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ–‡æ¡£éœ€è¦å¤„ç†
            if status.completed_documents < len(docs_to_process):
                current_doc = docs_to_process[status.completed_documents]
                
                # æ›´æ–°çŠ¶æ€
                analyzing_msg = f"æ­£åœ¨åˆ†æ: {current_doc.title}" if language == "ä¸­æ–‡" else f"Analyzing: {current_doc.title}"
                status.add_status_message(analyzing_msg)
                
                progress_label = f"ğŸ“– åˆ†ææ–‡æ¡£ä¸­... {status.completed_documents + 1}/{len(docs_to_process)}" if language == "ä¸­æ–‡" else f"ğŸ“– Analyzing document... {status.completed_documents + 1}/{len(docs_to_process)}"
                status.update_progress(status.completed_documents, len(docs_to_process), progress_label)
                analyzer.session_manager.update_processing_status(status)
                
                try:
                    # ç‰¹æ®Šå¤„ç†6-Kæ–‡ä»¶
                    if hasattr(current_doc, 'form_type') and current_doc.form_type == '6-K':
                        sixk_msg = f"æ£€æµ‹åˆ°6-Kæ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†é™„ä»¶" if language == "ä¸­æ–‡" else f"Detected 6-K file, starting to process attachments"
                        status.add_status_message(sixk_msg)
                        
                        # åˆå§‹åŒ–6-Kå¤„ç†å™¨
                        analyzer.sec_service._init_sixk_processor(analyzer.document_manager.temp_dir)
                        
                        # ä»URLä¸­æå–tickerå’Œcik
                        ticker = st.session_state.analyzer_ticker
                        
                        # è·å–CIK
                        ticker_map = analyzer.sec_service.get_cik_map()
                        cik = ticker_map.get(ticker.upper(), '')
                        
                        downloading_msg = f"æ­£åœ¨ä¸‹è½½å’Œå¤„ç†6-Ké™„ä»¶..." if language == "ä¸­æ–‡" else f"Downloading and processing 6-K attachments..."
                        status.add_status_message(downloading_msg)
                        analyzer.session_manager.update_processing_status(status)
                        
                        # å¤„ç†6-Kæ–‡ä»¶
                        processed_docs = analyzer.sec_service.sixk_processor.process_6k_filing(
                            ticker, cik, current_doc.url, current_doc
                        )
                        
                        completed_msg = f"6-Kå¤„ç†å®Œæˆï¼Œç”Ÿæˆäº† {len(processed_docs)} ä¸ªåˆ†ææ–‡æ¡£" if language == "ä¸­æ–‡" else f"6-K processing completed, generated {len(processed_docs)} analysis documents"
                        status.add_status_message(completed_msg)
                        
                        # æ£€æŸ¥æ˜¯å¦éœ€è¦å¯¹6-Kæ–‡ä»¶è¿›è¡Œåˆ†ç±»è¿‡æ»¤
                        should_filter_6k = (st.session_state.analyzer_use_sec_reports and 
                                           not st.session_state.analyzer_use_sec_others)
                        
                        # å¤„ç†æ‰€æœ‰6-Kç›¸å…³æ–‡æ¡£
                        for i, doc in enumerate(processed_docs):
                            if status.stop_requested:
                                break
                                
                            analyzing_6k_msg = f"æ­£åœ¨åˆ†æç¬¬ {i+1}/{len(processed_docs)} ä¸ª6-Kæ–‡æ¡£: {doc.title}" if language == "ä¸­æ–‡" else f"Analyzing {i+1}/{len(processed_docs)} 6-K document: {doc.title}"
                            status.add_status_message(analyzing_6k_msg)
                            analyzer.session_manager.update_processing_status(status)
                            
                            # å¦‚æœéœ€è¦è¿‡æ»¤6-Kæ–‡ä»¶ï¼Œå…ˆç”¨ä¾¿å®œæ¨¡å‹è¿›è¡Œåˆ†ç±»
                            if should_filter_6k:
                                classifying_msg = f"æ­£åœ¨åˆ†ç±»6-Kæ–‡æ¡£..." if language == "ä¸­æ–‡" else f"Classifying 6-K document..."
                                status.add_status_message(classifying_msg)
                                
                                # ç¡®ä¿æ–‡æ¡£æœ‰å†…å®¹
                                if not doc.content:
                                    if doc.type == 'SEC Filing':
                                        doc.content = analyzer.sec_service.download_filing(doc.url)
                                
                                # ä½¿ç”¨ä¾¿å®œæ¨¡å‹è¿›è¡Œåˆ†ç±»
                                is_quarterly_annual_ipo = analyzer.gemini_service.classify_6k_document(doc.content)
                                
                                if not is_quarterly_annual_ipo:
                                    # å¦‚æœä¸æ˜¯å­£æŠ¥/å¹´æŠ¥/IPOï¼Œè·³è¿‡è¿™ä¸ªæ–‡æ¡£
                                    skip_msg = f"è·³è¿‡éå­£æŠ¥/å¹´æŠ¥/IPOçš„6-Kæ–‡æ¡£: {doc.title}" if language == "ä¸­æ–‡" else f"Skipping non-quarterly/annual/IPO 6-K document: {doc.title}"
                                    status.add_status_message(skip_msg)
                                    continue
                                else:
                                    # å¦‚æœæ˜¯å­£æŠ¥/å¹´æŠ¥/IPOï¼Œç»§ç»­å¤„ç†
                                    continue_msg = f"æ£€æµ‹åˆ°å­£æŠ¥/å¹´æŠ¥/IPOæ–‡æ¡£ï¼Œç»§ç»­åˆ†æ: {doc.title}" if language == "ä¸­æ–‡" else f"Detected quarterly/annual/IPO document, continuing analysis: {doc.title}"
                                    status.add_status_message(continue_msg)
                            
                            # åˆ›å»ºAIåˆ†æçŠ¶æ€æ˜¾ç¤º
                            ai_status_placeholder = st.empty()
                            with ai_status_placeholder.status(f"ğŸ¤– AIæ­£åœ¨åˆ†æ6-Kæ–‡æ¡£ {i+1}/{len(processed_docs)}...", expanded=True) as ai_status:
                                ai_status.write(f"ğŸ“„ æ­£åœ¨åˆ†æ: {doc.title}")
                                ai_status.write("ğŸ“ æ­£åœ¨æ„å»ºåˆ†ææç¤ºè¯...")
                                ai_status.write("ğŸ§  æ­£åœ¨è°ƒç”¨AIæ¨¡å‹è¿›è¡Œæ·±åº¦åˆ†æ...")
                                ai_status.write("â³ å¼€å§‹æµå¼å“åº”...")
                                
                                # æ‰§è¡Œå®é™…çš„AIåˆ†æ - ä½¿ç”¨æµå¼å“åº”
                                stream_generator = analyzer.process_document_stream(doc, status.processing_prompt, model_type)
                                
                                ai_status.write("âœ… AIåˆ†æå¼€å§‹ï¼")
                                ai_status.update(label=f"âœ… 6-Kæ–‡æ¡£ {i+1}/{len(processed_docs)} åˆ†æå¼€å§‹", state="complete")
                            
                            # æ¸…é™¤AIçŠ¶æ€æ˜¾ç¤º
                            ai_status_placeholder.empty()
                            
                            # æ˜¾ç¤ºæ–‡æ¡£æ ‡é¢˜
                            st.markdown(f"### ğŸ“… {doc.date}")
                            st.markdown(f"### {doc.title}")
                            
                            # ä½¿ç”¨æµå¼å“åº”æ˜¾ç¤ºç»“æœ
                            analysis_result = st.write_stream(stream_generator)
                            
                            # æ ¹æ®æ–‡æ¡£ç±»å‹è®¾ç½®å¤´åƒ
                            avatar = "ğŸ“„"
                            
                            # ä¿å­˜æ–‡æ¡£å†…å®¹åˆ°ä¸´æ—¶æ–‡ä»¶
                            temp_file_path = analyzer.document_manager.save_document_content(doc)
                            
                            # å°†åˆ†æç»“æœæ·»åŠ åˆ°èŠå¤©å†å²ä¸­ï¼Œè¿™æ ·rerunæ—¶ä¸ä¼šä¸¢å¤±
                            message_content = f"### ğŸ“… {doc.date}\n### {doc.title}\n\n{analysis_result}"
                            st.session_state.analyzer_messages.append({
                                "role": "assistant",
                                "content": message_content,
                                "avatar": avatar,
                                "temp_file_path": temp_file_path,
                                "document_title": doc.title
                            })
                            
                            # ä¿å­˜ç»“æœ
                            status.document_results.append({
                                "title": doc.title,
                                "date": doc.date.isoformat(),
                                "analysis": analysis_result
                            })
                            
                            completed_6k_msg = f"å®Œæˆç¬¬ {i+1} ä¸ª6-Kæ–‡æ¡£åˆ†æ" if language == "ä¸­æ–‡" else f"Completed {i+1} 6-K document analysis"
                            status.add_status_message(completed_6k_msg)
                    else:
                        # æ™®é€šæ–‡æ¡£å¤„ç†
                        # åˆ›å»ºAIåˆ†æçŠ¶æ€æ˜¾ç¤º
                        ai_status_placeholder = st.empty()
                        with ai_status_placeholder.status("ğŸ¤– AIæ­£åœ¨åˆ†ææ–‡æ¡£å†…å®¹...", expanded=True) as ai_status:
                            # æ˜¾ç¤ºè¯¦ç»†çš„AIåˆ†ææ­¥éª¤
                            ai_status.write("ğŸ“„ æ­£åœ¨å‡†å¤‡æ–‡æ¡£å†…å®¹...")
                            
                            # æ£€æŸ¥æ–‡æ¡£å†…å®¹æ˜¯å¦éœ€è¦ä¸‹è½½
                            if not current_doc.content:
                                ai_status.write("ğŸ“¥ æ­£åœ¨ä¸‹è½½æ–‡æ¡£å†…å®¹...")
                                if current_doc.type == 'SEC Filing':
                                    if hasattr(current_doc, 'form_type') and current_doc.form_type == '6-K':
                                        ai_status.write("âš ï¸ 6-Kæ–‡ä»¶å†…å®¹å¤„ç†å¤±è´¥")
                                    else:
                                        ai_status.write("ğŸ”— æ­£åœ¨ä»SEC EDGARä¸‹è½½æ–‡æ¡£...")
                                elif current_doc.type == 'HK Stock Filing':
                                    ai_status.write("ğŸ”— æ­£åœ¨ä»æ¸¯äº¤æ‰€ä¸‹è½½æ–‡æ¡£...")
                                elif current_doc.type == 'Earnings Call':
                                    ai_status.write("ğŸ”— æ­£åœ¨è·å–è´¢æŠ¥ä¼šè®®è®°å½•...")
                            
                            ai_status.write("ğŸ“ æ­£åœ¨æ„å»ºåˆ†ææç¤ºè¯...")
                            ai_status.write("ğŸ§  æ­£åœ¨è°ƒç”¨AIæ¨¡å‹è¿›è¡Œæ·±åº¦åˆ†æ...")
                            ai_status.write("â³ å¼€å§‹æµå¼å“åº”...")
                            
                            # æ‰§è¡Œå®é™…çš„AIåˆ†æ - ä½¿ç”¨æµå¼å“åº”
                            stream_generator = analyzer.process_document_stream(current_doc, status.processing_prompt, model_type)
                            
                            ai_status.write("âœ… AIåˆ†æå¼€å§‹ï¼")
                            ai_status.update(label="âœ… AIåˆ†æå¼€å§‹", state="complete")
                        
                        # æ¸…é™¤AIçŠ¶æ€æ˜¾ç¤º
                        ai_status_placeholder.empty()
                        
                        # æ˜¾ç¤ºæ–‡æ¡£æ ‡é¢˜
                        st.markdown(f"### ğŸ“… {current_doc.date}")
                        st.markdown(f"### {current_doc.title}")
                        
                        # ä½¿ç”¨æµå¼å“åº”æ˜¾ç¤ºç»“æœ
                        analysis_result = st.write_stream(stream_generator)
                        
                        # æ ¹æ®æ–‡æ¡£ç±»å‹è®¾ç½®å¤´åƒ
                        if current_doc.type == 'SEC Filing':
                            avatar = "ğŸ“„"
                        elif current_doc.type == 'HK Stock Filing':
                            avatar = "ğŸ¢"
                        elif current_doc.type == 'Earnings Call':
                            avatar = "ğŸ™ï¸"
                        else:
                            avatar = "ğŸ“„"
                        
                        # ä¿å­˜æ–‡æ¡£å†…å®¹åˆ°ä¸´æ—¶æ–‡ä»¶
                        temp_file_path = analyzer.document_manager.save_document_content(current_doc)
                        
                        # å°†åˆ†æç»“æœæ·»åŠ åˆ°èŠå¤©å†å²ä¸­ï¼Œè¿™æ ·rerunæ—¶ä¸ä¼šä¸¢å¤±
                        message_content = f"### ğŸ“… {current_doc.date}\n### {current_doc.title}\n\n{analysis_result}"
                        st.session_state.analyzer_messages.append({
                            "role": "assistant",
                            "content": message_content,
                            "avatar": avatar,
                            "temp_file_path": temp_file_path,
                            "document_title": current_doc.title
                        })
                        
                        # ä¿å­˜ç»“æœ
                        status.document_results.append({
                            "title": current_doc.title,
                            "date": current_doc.date.isoformat(),
                            "analysis": analysis_result
                        })
                    
                    status.completed_documents += 1
                    
                    # æ›´æ–°çŠ¶æ€
                    analyzer.session_manager.update_processing_status(status)
                    
                    # å¦‚æœè¿˜æœ‰æ›´å¤šæ–‡æ¡£éœ€è¦å¤„ç†ï¼Œç»§ç»­ä¸‹ä¸€ä¸ª
                    if status.completed_documents < len(docs_to_process) and not status.stop_requested:
                        st.rerun()
                    else:
                        # æ‰€æœ‰æ–‡æ¡£å¤„ç†å®Œæˆï¼Œè¿›å…¥ä¸‹ä¸€æ­¥
                        status.processing_step = 4
                        analyzer.session_manager.update_processing_status(status)
                        st.rerun()
                    
                except Exception as exc:
                    failed_msg = f"åˆ†æå¤±è´¥: {current_doc.title} - {exc}" if language == "ä¸­æ–‡" else f"Analysis failed: {current_doc.title} - {exc}"
                    status.add_status_message(failed_msg)
                    logger.error(f"æ–‡æ¡£åˆ†æå¤±è´¥: {current_doc.title} - {exc}")
                    
                    # ä¹Ÿå°†é”™è¯¯ä¿¡æ¯æ·»åŠ åˆ°èŠå¤©å†å²ä¸­
                    error_prefix = f"**âš ï¸ {current_doc.title} åˆ†æå¤±è´¥:**" if language == "ä¸­æ–‡" else f"**âš ï¸ {current_doc.title} Analysis Failed:**"
                    error_message = f"{error_prefix}\n\n{exc}"
                    st.session_state.analyzer_messages.append({
                        "role": "assistant", 
                        "content": error_message,
                        "avatar": "âš ï¸"
                    })
                    
                    # è·³è¿‡å¤±è´¥çš„æ–‡æ¡£ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª
                    status.completed_documents += 1
                    analyzer.session_manager.update_processing_status(status)
                    st.rerun()
            else:
                # æ‰€æœ‰æ–‡æ¡£å¤„ç†å®Œæˆ
                all_completed_msg = "âœ… æ‰€æœ‰æ–‡æ¡£åˆ†æå®Œæˆ" if language == "ä¸­æ–‡" else "âœ… All document analysis completed"
                status.current_status_label = all_completed_msg
                status.processing_step = 4
                analyzer.session_manager.update_processing_status(status)
                st.rerun()
        
        # æ­¥éª¤4ï¼šæ•´åˆç»“æœ
        elif status.processing_step == 4:
            if status.stop_requested:
                return
                
            generating_msg = "ğŸ“Š æ­£åœ¨ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š..." if language == "ä¸­æ–‡" else "ğŸ“Š Generating final report..."
            status.current_status_label = generating_msg
            
            integrating_msg = "æ•´åˆæ‰€æœ‰åˆ†æç»“æœ..." if language == "ä¸­æ–‡" else "Integrating all analysis results..."
            status.add_status_message(integrating_msg)
            analyzer.session_manager.update_processing_status(status)
            
            # è¿‡æ»¤æ‰å¤±è´¥çš„ç»“æœ
            successful_results = [res for res in status.document_results if res is not None]
            
            # æ˜¾ç¤ºç»¼åˆæŠ¥å‘Šæ ‡é¢˜
            st.markdown("### ğŸ“Š Summary")
            
            # ä½¿ç”¨æµå¼å“åº”æ˜¾ç¤ºæœ€ç»ˆæŠ¥å‘Š
            final_report_stream = analyzer.integrate_results_stream(
                successful_results, status.integration_prompt, status.user_question, ticker, model_type
            )
            final_report = st.write_stream(final_report_stream)
            
            # å°†ç»¼åˆæŠ¥å‘Šæ·»åŠ åˆ°èŠå¤©å†å²ä¸­
            summary_content = f"### ğŸ“Š Summary\n\n{final_report}"
            st.session_state.analyzer_messages.append({
                "role": "assistant",
                "content": summary_content,
                "avatar": "ğŸ“Š"
            })
            
            report_completed_msg = "ç»¼åˆæŠ¥å‘Šç”Ÿæˆå®Œæ¯•ï¼" if language == "ä¸­æ–‡" else "Comprehensive report generated!"
            status.add_status_message(report_completed_msg)
            
            processing_completed_msg = "âœ… å¤„ç†å®Œæˆï¼" if language == "ä¸­æ–‡" else "âœ… Processing completed!"
            status.current_status_label = processing_completed_msg
            status.progress_percentage = 100.0
            analyzer.session_manager.update_processing_status(status)
            
            # çŸ­æš‚æ˜¾ç¤ºå®ŒæˆçŠ¶æ€
            time.sleep(0.1)
            
            # é‡ç½®çŠ¶æ€
            status = ProcessingStatus()
            analyzer.session_manager.update_processing_status(status)

            st.rerun()

    except Exception as e:
        logger.error(f"å¤„ç†æµç¨‹å‡ºé”™: {e}", exc_info=True)
        error_msg = f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°ä¸¥é‡é”™è¯¯: {e}" if language == "ä¸­æ–‡" else f"A serious error occurred during processing: {e}"
        st.error(error_msg)
        # é‡ç½®çŠ¶æ€
        status = ProcessingStatus()
        analyzer.session_manager.update_processing_status(status)
        st.rerun()


def process_user_question(analyzer: SECEarningsAnalyzer, ticker: str, years: int, use_sec: bool, use_earnings: bool, model_type: str):
    """å¤„ç†ç”¨æˆ·é—®é¢˜çš„å®Œæ•´æµç¨‹"""
    # DEPRECATED: use process_user_question_new instead
    pass

if __name__ == "__main__":
    main() 