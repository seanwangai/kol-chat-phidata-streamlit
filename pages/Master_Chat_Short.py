import streamlit as st
import pandas as pd
from datetime import datetime, timedelta, timezone
import calendar
from collections import Counter
from itertools import cycle
import json
import time
import os
import re
import zipfile
from dateutil.relativedelta import relativedelta
from exa_py import Exa
from google import genai
from pathlib import Path
import concurrent.futures
import random
import backoff
import httpx

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="Risk Scanner",
    page_icon="ğŸ”",
    layout="wide"
)

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if "selected_experts" not in st.session_state:
    st.session_state.selected_experts = []
if "expert_agents" not in st.session_state:
    st.session_state.expert_agents = {}
if "should_run_analysis" not in st.session_state:
    st.session_state.should_run_analysis = False
if "expert_names" not in st.session_state:
    st.session_state.expert_names = []
if "analysis_results" not in st.session_state:
    st.session_state.analysis_results = {}
if "company_name" not in st.session_state:
    st.session_state.company_name = ""
if "show_search_results" not in st.session_state:
    st.session_state.show_search_results = False
if "selected_model" not in st.session_state:
    st.session_state.selected_model = "gemini-2.5-flash"

# è®¾ç½®é»˜è®¤é€‰ä¸­çš„ä¸“å®¶
DEFAULT_EXPERTS = [
    # "Bonitas Research",
    "Grizzly Research",
    # "Publication - The Most Dangerous Trade",
    # "Hindenburg Research",
    # "Viceroy Research",
    "Muddy Waters",
    # "Publication - Confidence Game",
    # "J Capital"
    "Publication - Financial Shenanigans",
]

# åˆå§‹åŒ–APIå®¢æˆ·ç«¯
@st.cache_resource
def get_exa_client():
    """è·å–Exa APIå®¢æˆ·ç«¯å¹¶è½®æ¢APIå¯†é’¥"""
    if "exa_api_key_cycle" not in st.session_state:
        st.session_state.exa_api_key_cycle = cycle(st.secrets["EXA_API_KEYS"])
    return Exa(api_key=next(st.session_state.exa_api_key_cycle))

@st.cache_resource
def get_gemini_client():
    """è·å–Gemini APIå®¢æˆ·ç«¯å¹¶è½®æ¢APIå¯†é’¥"""
    if "google_api_key_cycle" not in st.session_state:
        st.session_state.google_api_key_cycle = cycle(st.secrets["GOOGLE_API_KEYS"])
    return genai.Client(api_key=next(st.session_state.google_api_key_cycle))

# æ ¼å¼åŒ–æ—¥æœŸä¸ºAPIæ‰€éœ€çš„ISO 8601æ ¼å¼
def format_date_for_api(date):
    """è½¬æ¢ä¸ºUTCæ—¶é—´æ ¼å¼"""
    return date.strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + 'Z'

# ç”Ÿæˆä»2020å¹´è‡³ä»Šçš„å­£åº¦æ—¶é—´èŒƒå›´
def generate_quarter_ranges():
    """ç”Ÿæˆä»2020å¹´è‡³ä»Šçš„å­£åº¦æ—¶é—´èŒƒå›´"""
    quarters = []
    
    # èµ·å§‹æ—¥æœŸï¼š2020å¹´1æœˆ1æ—¥
    start_date = datetime(2024, 1, 1)
    # ç»“æŸæ—¥æœŸï¼šå½“å‰æ—¥æœŸ
    end_date = datetime.now()
    
    current_date = start_date
    while current_date < end_date:
        # è®¡ç®—å½“å‰å­£åº¦çš„ç»“æŸæ—¥æœŸ
        if current_date.month in [1, 2, 3]:
            quarter_end = datetime(current_date.year, 3, 31, 23, 59, 59, 999999)
        elif current_date.month in [4, 5, 6]:
            quarter_end = datetime(current_date.year, 6, 30, 23, 59, 59, 999999)
        elif current_date.month in [7, 8, 9]:
            quarter_end = datetime(current_date.year, 9, 30, 23, 59, 59, 999999)
        else:
            quarter_end = datetime(current_date.year, 12, 31, 23, 59, 59, 999999)
        
        # å¦‚æœå­£åº¦ç»“æŸæ—¥æœŸè¶…è¿‡äº†å½“å‰æ—¥æœŸï¼Œä½¿ç”¨å½“å‰æ—¥æœŸä½œä¸ºç»“æŸ
        if quarter_end > end_date:
            quarter_end = end_date
            
        quarters.append({
            'start': current_date,
            'end': quarter_end,
            'name': f"{current_date.year}Q{(current_date.month-1)//3+1}"
        })
        
        # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªå­£åº¦çš„ç¬¬ä¸€å¤©
        current_date = (quarter_end + timedelta(days=1))
        
    return quarters

# ä½¿ç”¨Exaæœç´¢å…³é”®è¯å¹¶ç›´æ¥è·å–å†…å®¹
def run_exa_search(keyword, start_date, end_date, max_results=100):
    """ä½¿ç”¨Exa APIæœç´¢ç‰¹å®šæ—¶é—´èŒƒå›´å†…çš„å…³é”®è¯å¹¶è·å–å†…å®¹"""
    exa = get_exa_client()
    
    try:
        formatted_start = format_date_for_api(start_date)
        formatted_end = format_date_for_api(end_date)
        
        # ç§»é™¤spinnerï¼Œå› ä¸ºä¼šåœ¨å¹¶å‘æœç´¢ä¸­é›†ä¸­æ˜¾ç¤º
        print(f"æœç´¢: {keyword} - {formatted_start} è‡³ {formatted_end}")
        
        result = exa.search_and_contents(
            keyword,
            type="keyword",
            num_results=max_results,
            start_published_date=formatted_start,
            end_published_date=formatted_end,
            # include_domains=["xueqiu.com"],
            text={
                "max_characters": 10000  # è·å–æ›´å¤šå†…å®¹
            }
        )
        
        # å¤„ç†ç»“æœå¹¶æå–å†…å®¹
        search_results = []
        contents = []
        
        # ç›´æ¥è®¿é—®resultså±æ€§
        if hasattr(result, 'results'):
            search_results = result.results
        # å°è¯•é€šè¿‡dataå±æ€§è®¿é—®
        elif hasattr(result, 'data') and hasattr(result.data, 'results'):
            search_results = result.data.results
        # å°è¯•è½¬æ¢ä¸ºå­—å…¸
        elif hasattr(result, 'to_dict'):
            result_dict = result.to_dict()
            if 'data' in result_dict and 'results' in result_dict['data']:
                search_results = result_dict['data']['results']
        # å¤„ç†å­—å…¸ç±»å‹
        elif isinstance(result, dict):
            if 'data' in result and 'results' in result['data']:
                search_results = result['data']['results']
            elif 'results' in result:
                search_results = result['results']
        
        # ä»ç»“æœä¸­æå–å†…å®¹
        for item in search_results:
            content = None
            url = None
            
            # å°è¯•è·å–å†…å®¹
            if hasattr(item, 'text') and item.text:
                content = item.text
            elif isinstance(item, dict) and 'text' in item and item['text']:
                content = item['text']
            
            # å°è¯•è·å–URL
            if hasattr(item, 'url'):
                url = item.url
            elif isinstance(item, dict) and 'url' in item:
                url = item['url']
            
            # æ·»åŠ åˆ°ç»“æœä¸­
            if content and url:
                contents.append({
                    'url': url,
                    'content': content
                })
        
        return {
            'keyword': keyword,
            'quarter': f"{start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}",
            'results': search_results,
            'contents': contents
        }
            
    except Exception as e:
        st.error(f"æœç´¢ {keyword} æ—¶å‡ºé”™: {str(e)}")
        import traceback
        st.code(traceback.format_exc(), language="python")
        return {
            'keyword': keyword,
            'quarter': f"{start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}",
            'results': [],
            'contents': []
        }

# è·å–ä¸“å®¶ç›®å½•åˆ—è¡¨
def get_expert_names():
    """è·å–å¯ç”¨çš„ä¸“å®¶åç§°åˆ—è¡¨"""
    expert_dir = Path("data_short_expert")
    
    if not expert_dir.exists():
        st.warning("ä¸“å®¶æ•°æ®ç›®å½•ä¸å­˜åœ¨ï¼Œå°è¯•ä¸‹è½½...")
        if not initialize_dropbox():
            st.error("æ— æ³•åˆå§‹åŒ–ä¸“å®¶æ•°æ®")
            return []
    
    # æŸ¥æ‰¾æ‰€æœ‰å­ç›®å½•
    experts = []
    for folder in expert_dir.iterdir():
        if folder.is_dir():
            experts.append(folder.name)
    
    return experts

# ä»Dropboxä¸‹è½½ä¸“å®¶æ•°æ®
def initialize_dropbox():
    """åˆå§‹åŒ–Dropboxå¹¶ä¸‹è½½ä¸“å®¶æ•°æ®"""
    dropbox_url_key = "DROPBOX_DATA_URL_SHORT_EXPERT"
    target_dir = Path("data_short_expert")
    
    if dropbox_url_key in st.secrets:
        try:
            # åˆ›å»ºç›®æ ‡ç›®å½•
            target_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¿®æ”¹URLä¸ºç›´æ¥ä¸‹è½½é“¾æ¥
            url = st.secrets[dropbox_url_key]
            url = url.split('&dl=')[0] + '&dl=1'
            
            # ç¡®ä¿ä¸´æ—¶æ–‡ä»¶è·¯å¾„å­˜åœ¨
            temp_zip = target_dir / "temp_download.zip"
            
            try:
                # ä¸‹è½½æ–‡ä»¶
                import requests
                from requests.adapters import HTTPAdapter
                from urllib3.util.retry import Retry
                
                # è®¾ç½®é‡è¯•ç­–ç•¥
                session = requests.Session()
                retries = Retry(
                    total=5,  # æœ€å¤šé‡è¯•5æ¬¡
                    backoff_factor=1,  # é‡è¯•é—´éš”æ—¶é—´
                    status_forcelist=[429, 500, 502, 503, 504],  # æœåŠ¡å™¨é”™è¯¯æ—¶é‡è¯•
                )
                session.mount('https://', HTTPAdapter(max_retries=retries))
                
                with st.spinner("æ­£åœ¨ä¸‹è½½ä¸“å®¶æ•°æ®..."):
                    # st.info(f"å¼€å§‹ä¸‹è½½: {url}")
                    
                    # åˆ†å—ä¸‹è½½ä»¥ä¾¿æ˜¾ç¤ºè¿›åº¦
                    response = session.get(url, stream=True)
                    response.raise_for_status()
                    
                    # è·å–æ–‡ä»¶å¤§å°(å­—èŠ‚)
                    total_size = int(response.headers.get('content-length', 0))
                    # st.info(f"æ–‡ä»¶å¤§å°: {total_size/1024/1024:.2f} MB")
                    
                    # åˆ›å»ºè¿›åº¦æ¡
                    progress_bar = st.progress(0)
                    download_status = st.empty()
                    
                    # å†™å…¥ä¸´æ—¶æ–‡ä»¶
                    downloaded_size = 0
                    chunk_size = 1024 * 1024  # 1MB å—å¤§å°
                    
                    try:
                        with open(temp_zip, 'wb') as f:
                            for chunk in response.iter_content(chunk_size=chunk_size):
                                if chunk:
                                    f.write(chunk)
                                    downloaded_size += len(chunk)
                                    
                                    # æ›´æ–°è¿›åº¦
                                    if total_size > 0:
                                        progress = min(downloaded_size / total_size, 1.0)
                                        progress_bar.progress(progress)
                                        download_status.write(f"ä¸‹è½½è¿›åº¦: {downloaded_size/1024/1024:.2f} MB / {total_size/1024/1024:.2f} MB")
                    except Exception as e:
                        st.error(f"ä¸‹è½½è¿‡ç¨‹ä¸­æ–­: {str(e)}")
                        if temp_zip.exists():
                            temp_zip.unlink()  # ç§»é™¤ä¸å®Œæ•´æ–‡ä»¶
                        raise
                    
                    download_status.empty()
                    progress_bar.empty()
                    
                    st.success(f"ä¸‹è½½å®Œæˆ: {temp_zip.stat().st_size/1024/1024:.2f} MB")
                
                # éªŒè¯æ–‡ä»¶
                if not temp_zip.exists():
                    raise FileNotFoundError(f"ä¸‹è½½çš„æ–‡ä»¶æœªæ‰¾åˆ°: {temp_zip}")
                
                if not zipfile.is_zipfile(temp_zip):
                    raise ValueError(f"ä¸‹è½½çš„æ–‡ä»¶ä¸æ˜¯æœ‰æ•ˆçš„ZIPæ–‡ä»¶: {temp_zip}")
                
                # æ¸…ç©ºç›®æ ‡ç›®å½•
                for item in target_dir.iterdir():
                    if item != temp_zip:
                        if item.is_file():
                            item.unlink()
                        elif item.is_dir():
                            import shutil
                            shutil.rmtree(item)
                
                # è§£å‹æ–‡ä»¶
                with st.spinner("æ­£åœ¨è§£å‹ä¸“å®¶æ•°æ®..."):
                    extract_status = st.empty()
                    extract_status.info("å¼€å§‹è§£å‹æ–‡ä»¶...")
                    
                    try:
                        with zipfile.ZipFile(temp_zip, 'r') as zip_ref:
                            # è·å–å‹ç¼©åŒ…ä¸­çš„æ–‡ä»¶åˆ—è¡¨
                            file_list = zip_ref.namelist()
                            total_files = len(file_list)
                            
                            # åˆ›å»ºè¿›åº¦æ¡
                            extract_progress = st.progress(0)
                            
                            # é€ä¸ªè§£å‹æ–‡ä»¶å¹¶æ›´æ–°è¿›åº¦
                            for i, file in enumerate(file_list):
                                extract_status.write(f"è§£å‹æ–‡ä»¶ ({i+1}/{total_files}): {file}")
                                zip_ref.extract(file, target_dir)
                                extract_progress.progress((i+1)/total_files)
                                
                            extract_status.success("è§£å‹å®Œæˆ")
                            extract_progress.empty()
                    except Exception as e:
                        extract_status.error(f"è§£å‹å¤±è´¥: {str(e)}")
                        raise
                
                # åˆ é™¤ä¸´æ—¶ZIPæ–‡ä»¶
                temp_zip.unlink()
                
                # æ›´æ–°ä¸“å®¶åˆ—è¡¨
                expert_count = len([f for f in target_dir.iterdir() if f.is_dir()])
                st.success(f"å‘ç° {expert_count} ä¸ªä¸“å®¶ç›®å½•")
                st.session_state.expert_names = get_expert_names()
                
                return True
                
            except Exception as e:
                st.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
                import traceback
                st.code(traceback.format_exc(), language="python")
                return False
                
        except Exception as e:
            st.error(f"åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            return False
            
    st.warning(f"æœªæ‰¾åˆ° {dropbox_url_key} é…ç½®ä¿¡æ¯")
    return False

# è·å–ä¸“å®¶çš„å¤´åƒ
def get_expert_avatar(expert_name):
    """æ ¹æ®ä¸“å®¶åç§°è·å–å¤´åƒ"""
    # ä¸ºæ¯ä¸ªä¸“å®¶é…ç½®ä¸åŒçš„å¤´åƒ
    avatars = {
        "Bonitas Research": "ğŸ”",
        "Grizzly Research": "ğŸ»",
        "Hindenburg Research": "ğŸ”¥",
        "Muddy Waters": "ğŸ¦",
        "J Capital": "ğŸ’¸",
        "Viceroy Research": "ğŸ•µï¸â€â™‚ï¸",
        "Citron": "ğŸ‹",
        "Spruce": "ğŸŒ²",
        "Chanos": "ğŸ›ï¸",
        "Publication - Confidence Game": "ğŸ“š",
        "Publication - The Art of Short Selling": "ğŸ“–",
        "Publication - The Most Dangerous Trade": "ğŸ“Š",
        "Publication - The Smartest Guys in the Room": "ğŸ§©",
        "Publication - Financial Shenanigans": "ğŸ­",
        "Publication - Others": "ğŸ“"
    }
    
    # å¦‚æœæ‰¾ä¸åˆ°é¢„å®šä¹‰çš„å¤´åƒï¼Œéšæœºåˆ†é…ä¸€ä¸ª
    if expert_name not in avatars:
        import random
        random_emojis = ["âš¡", "ğŸ”¬", "ğŸ“ˆ", "ğŸ”®", "ğŸ§®", "ğŸ’¼", "ğŸ—‚ï¸", "ğŸ“Š", "ğŸ’°", "ğŸ’¹"]
        # ä½¿ç”¨ä¸“å®¶åç§°çš„å“ˆå¸Œå€¼æ¥ç¡®ä¿åŒä¸€ä¸“å®¶æ¯æ¬¡è·å¾—ç›¸åŒçš„emoji
        random.seed(hash(expert_name))
        return random.choice(random_emojis)
        
    return avatars.get(expert_name)

# æ·»åŠ tokenè®¡æ•°è¾…åŠ©å‡½æ•°
def estimate_tokens(text):
    """ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡ï¼ˆç²—ç•¥ä¼°è®¡ï¼Œä½¿ç”¨ç®€å•å…¬å¼ï¼‰"""
    # ä¸€ä¸ªç®€å•çš„ä¼°è®¡æ–¹æ³•ï¼šæŒ‰ç©ºæ ¼åˆ†è¯åçš„è¯æ•° * 1.3
    words = text.split()
    return int(len(words) * 1.3)

# ä¸ºå•ä¸ªä¸“å®¶åˆ›å»ºLLM
def create_expert_llm(expert_name):
    """ä¸ºå•ä¸ªä¸“å®¶åˆ›å»ºLLM"""
    # æ£€æŸ¥æ˜¯å¦å·²ç»åˆ›å»ºè¿‡æ¨¡å‹
    if expert_name in st.session_state.expert_agents:
        return st.session_state.expert_agents[expert_name]
    
    expert_dir = Path("data_short_expert") / expert_name
    expert_knowledge = []
    documents_count = 0
    
    # è¯»å–ä¸“å®¶æ–‡æ¡£å†…å®¹
    for file in expert_dir.glob("**/*"):
        if file.is_file():
            if file.suffix.lower() in ['.txt', '.md']:
                try:
                    with open(file, 'r', encoding='utf-8') as f:
                        expert_knowledge.append(f.read())
                        documents_count += 1
                except:
                    pass
            elif file.suffix.lower() == '.pdf':
                try:
                    import fitz  # PyMuPDF
                    doc = fitz.open(file)
                    pdf_text = ""
                    for page in doc:
                        pdf_text += page.get_text()
                    expert_knowledge.append(pdf_text)
                    documents_count += 1
                except:
                    pass
            elif file.suffix.lower() == '.docx':
                try:
                    import docx
                    doc = docx.Document(file)
                    docx_text = ""
                    for para in doc.paragraphs:
                        docx_text += para.text + "\n"
                    expert_knowledge.append(docx_text)
                    documents_count += 1
                except Exception as e:
                    print(f"æ— æ³•è¯»å–docxæ–‡ä»¶ {file}: {str(e)}")
                    pass
            elif file.suffix.lower() == '.epub':
                try:
                    import ebooklib
                    from ebooklib import epub
                    from bs4 import BeautifulSoup
                    
                    book = epub.read_epub(file)
                    epub_text = ""
                    
                    for item in book.get_items():
                        if item.get_type() == ebooklib.ITEM_DOCUMENT:
                            soup = BeautifulSoup(item.get_content(), 'html.parser')
                            epub_text += soup.get_text() + "\n"
                    
                    expert_knowledge.append(epub_text)
                    documents_count += 1
                except:
                    pass
    
    # åˆ›å»ºä¸“å®¶LLM
    if expert_knowledge:
        # çŸ­ä¸€é»
        all_expert_knowledge  = ' '.join(expert_knowledge)
        original_length = len(all_expert_knowledge)
        
        
        expert_prompt = f"""ä½ æ˜¯çŸ¥ååšç©ºæ©Ÿæ§‹åˆ†æå¸« ä»¥ä¸‹æ˜¯ä½ è¿‡å»shortè¿‡çš„æ‰€æœ‰case
        {all_expert_knowledge} 

========
ä»¥ä¸Šæ˜¯ä½ è¿‡å»shortè¿‡çš„æ‰€æœ‰caseï¼Œæˆ‘æ¥ä¸‹ä¾†æœƒè¼¸å…¥æˆ‘åœ¨ç ”ç©¶çš„æ–°çš„å…¬å¸çš„è³‡è¨Šï¼Œå¹«æˆ‘åˆ¤æ–· 

1. æ ¹æ“š ä½ çš„çŸ¥è­˜ å’Œä½ éå»ç¶“é©— ï¼Œåˆ†æé€™å®¶å…¬å¸ æœ‰é¢¨éšª çš„å¯èƒ½æ€§å¤šé«˜(é«˜ ä¸­ ä½)ï¼Ÿ
2. æ ¹æ“šä½ çš„çŸ¥è­˜å’Œéå»ç¶“é©—ï¼Œæœ‰æ²’æœ‰çœ‹åˆ°é€™å®¶å…¬å¸æœ‰å­˜åœ¨ è·Ÿä½ ä¹‹å‰çœ‹çš„caseç±»ä¼¼çš„ æœ‰é€ å‡çš„ æˆ–æ˜¯æœ‰å¤§å•é¡Œ "å±é™©ä¿¡å·" ï¼Œè©³ç´°èªªæ˜ï¼Œå…ˆèªªé€™å®¶å…¬å¸ä¹Ÿæœ‰çš„éƒ¨åˆ†ï¼Œå†èªªé€™å®¶å…¬å¸è·Ÿ ä¹‹å‰çœ‹shortéçš„case shortä¸»å› ç›¸ä¼¼è™• è©³è¿°ä¹‹å‰çš„caseæ€æ¨£é€ å‡ è¦éå¸¸è©³ç´°èªªæ˜è·Ÿéå»ç›¸ä¼¼çš„é»
3. æœ€å¾Œæ ¹æ“šéå»çœ‹é€ å‡å…¬å¸çš„ç¶“é©—ï¼Œç„¶å¾Œçµåˆé€™å®¶å…¬å¸çš„ç›®å‰å¾—åˆ°çš„è³‡è¨Š  è·Ÿæˆ‘èªªå¯ä»¥å†æ·±å…¥æœä»€éº¼æ–¹å‘ç ”ç©¶ åšä»€éº¼èª¿ç ” ç‚ºä»€éº¼è¦ä½œé€™å€‹èª¿ç ” 

ç”¨æ¨™æº–çš„markdownæ ¼å¼å›ç­”ï¼Œè¦æœ‰çµæ§‹ï¼Œå¯ä»¥åŠ å…¥emoji è¦æ˜¯å°ˆæ¥­çš„emoji å¹«åŠ©é–±è®€ç”¨çš„ï¼Œæ³¨æ„é€™æ˜¯éå¸¸å°ˆæ¥­çš„å ±å‘Šï¼Œè¬›è§£çš„è¶Šè©³ç´°è¶Šå¥½ï¼Œçµ¦å°ˆæ¥­æŠ•è³‡äººçœ‹çš„
æ³¨æ„ï¼Œä½ æ˜¯éå¸¸å°ˆæ¥­çš„è²¡å‹™è‚¡å¸‚åˆ†æå¸«ï¼Œæ‰€ä»¥å›ç­”éƒ½è¦æœ‰é‚è¼¯

ä¸ç”¨èªªï¼šå¥½çš„ï¼Œä»¥ä¸‹æ˜¯æ‚¨æä¾›çš„.... ï¼Œä¹Ÿä¸ç”¨å…è´£è²æ˜ï¼Œé€™ç¨®å»¢è©±ï¼Œç›´æ¥é€²å…¥æ­£é¡Œé–‹å§‹å›ç­”ï¼Œæ‰€ä»¥ä¸è¦èªª "å¥½çš„" ä¹Ÿä¸è¦èªª "å¸Œæœ›è¿™äº›åˆ†æå¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ï¼


"""
        
        # æ‰“å°ç³»ç»Ÿæç¤ºä¿¡æ¯ç»Ÿè®¡
        prompt_char_count = len(expert_prompt)
        prompt_token_estimate = estimate_tokens(expert_prompt)
        
        print(f"\n============== {expert_name} ä¸“å®¶ç»Ÿè®¡ ==============")
        print(f"æ–‡æ¡£æ•°é‡: {documents_count} ä¸ª")
        print(f"åŸå§‹ä¸“å®¶çŸ¥è¯†åº“: {original_length:,} å­—ç¬¦")
        print(f"ç³»ç»Ÿæç¤ºæ€»å­—æ•°: {prompt_char_count:,} å­—ç¬¦")
        print(f"ç³»ç»Ÿæç¤ºä¼°è®¡Tokenæ•°: çº¦ {prompt_token_estimate:,} tokens")
        print("==============================================\n")
        
        try:
            # åˆ›å»ºä¸“å®¶å¯¹è±¡ï¼Œåªå­˜å‚¨æç¤ºå’Œæ¨¡å‹åç§°ï¼Œä¸å†åˆ›å»ºå®¢æˆ·ç«¯
            expert = {
                "system_prompt": expert_prompt,
                "model": st.session_state.selected_model,  # ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„æ¨¡å‹
                "stats": {
                    "documents_count": documents_count,
                    "original_length": original_length,
                    "char_count": prompt_char_count,
                    "token_estimate": prompt_token_estimate
                }
            }
            
            # ä¿å­˜ä¸“å®¶ä¿¡æ¯åˆ°session state
            st.session_state.expert_agents[expert_name] = expert
            return expert
        except Exception as e:
            st.error(f"åˆ›å»º{expert_name}æ¨¡å‹æ—¶å‡ºé”™: {str(e)}")
            import traceback
            st.code(traceback.format_exc(), language="python")
            return None
    
    return None

# ä½¿ç”¨ä¸“å®¶åˆ†æå†…å®¹
def analyze_with_experts(company_content):
    """ä½¿ç”¨ä¸“å®¶LLMåˆ†æå…¬å¸å†…å®¹"""
    results = {}
    
    # æ‰“å°å°†è¦åˆ†æçš„å†…å®¹çš„ç»Ÿè®¡ä¿¡æ¯
    input_char_count = len(company_content)
    input_token_estimate = estimate_tokens(company_content)
    print("\n============== è¾“å…¥å†…å®¹ç»Ÿè®¡ ==============")
    print(f"è¾“å…¥å†…å®¹æ€»å­—æ•°: {input_char_count:,} å­—ç¬¦")
    print(f"è¾“å…¥å†…å®¹ä¼°è®¡Tokenæ•°: çº¦ {input_token_estimate:,} tokens")
    print("==============================================\n")
    
    # åˆ›å»ºä¸“å®¶å›ç­”åŒºåŸŸ
    st.subheader(f"ä¸“å®¶åˆ†æ ({len(st.session_state.selected_experts)} åä¸“å®¶)")
    
    # é¢„å…ˆä¸ºæ¯ä¸ªä¸“å®¶åˆ›å»ºä¸€ä¸ªå®¹å™¨å’ŒçŠ¶æ€å ä½ç¬¦
    expert_containers = {}
    status_placeholders = {}
    
    # æ·»åŠ CSSæ ·å¼ä»¥ç¾åŒ–ä¸“å®¶å›ç­”åŒºåŸŸ
    st.markdown("""
    <style>
    .expert-box {
        border: 2px solid #4CAF50;
        border-radius: 10px;
        padding: 20px;
        margin-bottom: 25px;
        background-color: #f8f9fa;
        box-shadow: 0 4px 8px rgba(0,0,0,0.1);
    }
    .expert-header {
        font-size: 1.3em;
        font-weight: bold;
        margin-bottom: 15px;
        padding-bottom: 10px;
        border-bottom: 2px solid #4CAF50;
        color: #2E7D32;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # é¦–å…ˆåˆ›å»ºæ‰€æœ‰ä¸“å®¶çš„å®¹å™¨
    for i, expert_name in enumerate(st.session_state.selected_experts):
        avatar = get_expert_avatar(expert_name)
        
        # åˆ›å»ºä¸“å®¶å®¹å™¨ï¼Œä½¿ç”¨expander
        expert_expander = st.expander(f"{avatar} {expert_name}", expanded=True)
        expert_containers[expert_name] = expert_expander
        status_placeholders[expert_name] = expert_expander.empty()
        status_placeholders[expert_name].info(f"{avatar} {expert_name} æ­£åœ¨åˆ†æ...")
    
    # ç„¶åæ‰§è¡Œåˆ†æ
    for expert_name in st.session_state.selected_experts:
        try:
            # è·å–ä¸“å®¶å¤´åƒ
            avatar = get_expert_avatar(expert_name)
            
            # è·å–ä¹‹å‰åˆ›å»ºçš„çŠ¶æ€å ä½ç¬¦
            status_placeholder = status_placeholders[expert_name]
            
            # åˆ›å»ºæˆ–è·å–ä¸“å®¶
            expert = create_expert_llm(expert_name)
            
            if expert:
                # ç”Ÿæˆå›ç­”
                try:
                    # å‡†å¤‡è¾“å…¥
                    from google.genai import types
                    
                    # åˆå¹¶ç³»ç»Ÿæç¤ºå’Œç”¨æˆ·æ¶ˆæ¯åˆ°ä¸€ä¸ªuserè§’è‰²æ¶ˆæ¯ä¸­
                    combined_prompt = f"è¯·åŸºäºä»¥ä¸‹çš„å…¬å¸ä¿¡æ¯ï¼Œåˆ†ææ­¤å…¬å¸çš„æ½œåœ¨é£é™©å’Œé—®é¢˜:\n\n{company_content}"
                    
                    # è®°å½•åˆå¹¶åçš„æç¤ºç»Ÿè®¡
                    combined_prompt_chars = len(combined_prompt)
                    combined_prompt_tokens = estimate_tokens(combined_prompt)
                    print(f"\n============== {expert_name} åˆå¹¶æç¤ºç»Ÿè®¡ ==============")
                    print(f"åˆå¹¶æç¤ºæ€»å­—æ•°: {combined_prompt_chars:,} å­—ç¬¦")
                    print(f"åˆå¹¶æç¤ºä¼°è®¡Tokenæ•°: çº¦ {combined_prompt_tokens:,} tokens")
                    print("==============================================\n")
                    contents = [
                        types.Content(
                            role="user",
                            parts=[types.Part.from_text(text=combined_prompt)],
                        ),
                    ]
                    
                    # é…ç½®ç”Ÿæˆå‚æ•°
                    generate_content_config = types.GenerateContentConfig(
                        temperature=0.2,
                        top_p=0.95,
                        top_k=40,
                        max_output_tokens=8192,
                        response_mime_type="text/plain",
                        system_instruction=[
                            types.Part.from_text(text=expert['system_prompt']),
                        ],
                    )
                    
                    # å®šä¹‰å¸¦é€€é¿ç­–ç•¥çš„é‡è¯•å‡½æ•°
                    @backoff.on_exception(
                        backoff.expo, 
                        (Exception, httpx.ConnectError, httpx.ReadTimeout), 
                        max_tries=5,
                        factor=2,
                        jitter=backoff.full_jitter
                    )
                    def generate_with_retry():
                        # ä¸ºæ¯æ¬¡è¯·æ±‚è·å–ä¸€ä¸ªæ–°çš„APIå¯†é’¥
                        if "google_api_key_cycle" not in st.session_state:
                            st.session_state.google_api_key_cycle = cycle(st.secrets["GOOGLE_API_KEYS"])
                        api_key = next(st.session_state.google_api_key_cycle)
                        print(f"ä½¿ç”¨APIå¯†é’¥: {api_key[:5]}...{api_key[-4:]}")
                        
                        # éšæœºæš‚åœä¸€å°æ®µæ—¶é—´ï¼Œé¿å…å¹¶å‘è¯·æ±‚è¿‡å¤š
                        time.sleep(random.uniform(0.5, 2.0))
                        
                        # ä½¿ç”¨æ–°çš„APIå¯†é’¥åˆ›å»ºå®¢æˆ·ç«¯
                        client = genai.Client(api_key=api_key)
                        
                        try:
                            # ç§»é™¤timeoutå‚æ•°
                            response = client.models.generate_content(
                                model=expert["model"],
                                contents=contents,
                                config=generate_content_config,
                            )
                            return response
                        except (httpx.ConnectError, httpx.ReadTimeout) as e:
                            print(f"è¿æ¥é”™è¯¯: {str(e)}, å‡†å¤‡é‡è¯•...")
                            raise
                        except Exception as e:
                            print(f"ç”Ÿæˆå†…å®¹é”™è¯¯: {str(e)}, å‡†å¤‡é‡è¯•...")
                            raise
                    
                    # ä¿®æ”¹åçš„è¶…æ—¶å¤„ç†æ–¹æ³•ï¼Œä¸ä½¿ç”¨å•ç‹¬çš„çº¿ç¨‹æ¥æ›´æ–°UI
                    def execute_with_timeout(func, timeout_seconds=60):
                        # åˆ›å»ºFutureå¯¹è±¡
                        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                            future = executor.submit(func)
                            
                            # åœ¨ä¸»çº¿ç¨‹ä¸­ç­‰å¾…ç»“æœï¼ŒåŒæ—¶æ›´æ–°å€’è®¡æ—¶
                            start_time = time.time()
                            
                            # æ¯0.5ç§’æ£€æŸ¥ä¸€æ¬¡ç»“æœå¹¶æ›´æ–°å€’è®¡æ—¶
                            try:
                                while not future.done() and time.time() - start_time < timeout_seconds:
                                    elapsed = time.time() - start_time
                                    remaining = timeout_seconds - elapsed
                                    
                                    # åœ¨ä¸»çº¿ç¨‹ä¸­æ›´æ–°UI - åˆå¹¶åˆ†æçŠ¶æ€å’Œå€’è®¡æ—¶
                                    status_placeholder.info(f"{avatar} {expert_name} æ­£åœ¨åˆ†æ... â±ï¸ {remaining:.0f}ç§’")
                                    
                                    # çŸ­æš‚ç­‰å¾…ï¼Œé¿å…è¿‡é«˜çš„CPUä½¿ç”¨ç‡
                                    time.sleep(0.5)
                                    
                                    # å°è¯•éé˜»å¡åœ°è·å–ç»“æœ
                                    try:
                                        # éé˜»å¡æ£€æŸ¥
                                        result = future.result(timeout=0.01)
                                        status_placeholder.empty()
                                        return result
                                    except concurrent.futures.TimeoutError:
                                        # ç»§ç»­ç­‰å¾…
                                        pass
                                
                                # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                                if future.done():
                                    status_placeholder.empty()
                                    return future.result()
                                else:
                                    # è¶…æ—¶
                                    status_placeholder.warning(f"{avatar} {expert_name} å“åº”è¶…æ—¶ï¼Œå‡†å¤‡é‡è¯•...")
                                    # å°è¯•å–æ¶ˆä»»åŠ¡
                                    future.cancel()
                                    raise TimeoutError(f"æ“ä½œè¶…æ—¶ï¼ˆè¶…è¿‡{timeout_seconds}ç§’ï¼‰")
                            
                            except Exception as e:
                                status_placeholder.warning(f"{avatar} {expert_name} å‘ç”Ÿé”™è¯¯: {str(e)}")
                                # å°è¯•å–æ¶ˆä»»åŠ¡
                                future.cancel()
                                raise
                    
                    # å°è¯•æ‰§è¡ŒAPIè°ƒç”¨ï¼Œæœ€å¤šé‡è¯•5æ¬¡
                    max_retries = 5
                    retry_count = 0
                    response = None
                    
                    # åœ¨ä¸»æµç¨‹ä¸­é‡è¯•
                    while response is None and retry_count < max_retries:
                        try:
                            # ä½¿ç”¨è¶…æ—¶æ§åˆ¶æ‰§è¡ŒAPIè°ƒç”¨
                            response = execute_with_timeout(generate_with_retry, 60)
                        except (TimeoutError, Exception) as e:
                            retry_count += 1
                            print(f"å°è¯• {retry_count}/{max_retries} å¤±è´¥: {str(e)}")
                            
                            if retry_count < max_retries:
                                status_placeholder.warning(f"{avatar} {expert_name} é‡è¯•ä¸­... ({retry_count}/{max_retries})")
                                time.sleep(1)  # çŸ­æš‚æš‚åœåé‡è¯•
                            else:
                                status_placeholder.error(f"{avatar} {expert_name} æœ€å¤§é‡è¯•æ¬¡æ•°å·²è¾¾åˆ°")
                                raise Exception(f"åœ¨ {max_retries} æ¬¡å°è¯•åä»ç„¶å¤±è´¥: {str(e)}")
                    
                    if response:
                        # è·å–ç»“æœ
                        result_text = response.text
                        
                        # è®°å½•è¾“å‡ºç»“æœç»Ÿè®¡
                        output_chars = len(result_text)
                        output_tokens = estimate_tokens(result_text)
                        print(f"\n============== {expert_name} è¾“å‡ºç»“æœç»Ÿè®¡ ==============")
                        print(f"è¾“å‡ºç»“æœæ€»å­—æ•°: {output_chars:,} å­—ç¬¦")
                        print(f"è¾“å‡ºç»“æœä¼°è®¡Tokenæ•°: çº¦ {output_tokens:,} tokens")
                        print("==============================================\n")
                        
                        results[expert_name] = {
                            "content": result_text,
                            "avatar": avatar,
                            "stats": {
                                "input_chars": combined_prompt_chars,
                                "input_tokens": combined_prompt_tokens,
                                "output_chars": output_chars,
                                "output_tokens": output_tokens
                            }
                        }
                        
                        # ç«‹å³æ˜¾ç¤ºç»“æœ
                        status_placeholder.empty()
                        with expert_containers[expert_name]:
                            # ç›´æ¥ä½¿ç”¨markdownæ¸²æŸ“ä¸“å®¶å›ç­”å†…å®¹ï¼Œä¿æŒmarkdownæ ¼å¼
                            st.markdown(result_text)
                    else:
                        raise Exception("æ— æ³•ä»APIè·å–å“åº”")
                    
                except Exception as e:
                    error_msg = f"å¤šæ¬¡é‡è¯•åä»ç„¶ç”Ÿæˆåˆ†æå¤±è´¥: {str(e)}"
                    print(error_msg)  # åªåœ¨æ§åˆ¶å°æ˜¾ç¤ºé”™è¯¯
                    import traceback
                    print(traceback.format_exc())  # åªåœ¨æ§åˆ¶å°æ˜¾ç¤ºè¯¦ç»†é”™è¯¯
                    results[expert_name] = {
                        "content": error_msg,
                        "avatar": avatar
                    }
                    # æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
                    with expert_containers[expert_name]:
                        status_placeholder.error(f"{avatar} {expert_name} åˆ†æå¤±è´¥ (å¤šæ¬¡é‡è¯•å)")
                    
            else:
                results[expert_name] = {
                    "content": "æ— æ³•åŠ è½½ä¸“å®¶æ¨¡å‹",
                    "avatar": avatar
                }
                # æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
                with expert_containers[expert_name]:
                    status_placeholder.error(f"{avatar} {expert_name} æ¨¡å‹åŠ è½½å¤±è´¥")
                
        except Exception as e:
            results[expert_name] = {
                "content": f"åˆ†æå¤±è´¥: {str(e)}",
                "avatar": get_expert_avatar(expert_name)
            }
            # åªåœ¨æ§åˆ¶å°æ˜¾ç¤ºè¯¦ç»†é”™è¯¯
            import traceback
            print(traceback.format_exc())
            # æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
            with expert_containers[expert_name]:
                status_placeholder.error(f"{avatar} {expert_name} åˆ†æå¤±è´¥")
    
    # æ±‡æ€»æ‰€æœ‰ä¸“å®¶çš„åˆ†æç»“æœç»Ÿè®¡ - åªåœ¨ç»ˆç«¯æ˜¾ç¤º
    total_output_chars = 0
    total_output_tokens = 0
    total_input_tokens = 0
    
    for expert_name, result in results.items():
        if "stats" in result:
            total_output_chars += result["stats"]["output_chars"]
            total_output_tokens += result["stats"]["output_tokens"]
            total_input_tokens += result["stats"]["input_tokens"]
    
    print("\n============== æ‰€æœ‰ä¸“å®¶åˆ†æç»“æœæ±‡æ€» ==============")
    print(f"ä¸“å®¶æ•°é‡: {len(results)}")
    print(f"æ€»è¾“å‡ºå­—ç¬¦æ•°: {total_output_chars:,} å­—ç¬¦")
    print(f"æ€»è¾“å‡ºä¼°è®¡Tokenæ•°: çº¦ {total_output_tokens:,} tokens")
    print(f"æ€»è¾“å…¥ä¼°è®¡Tokenæ•°: çº¦ {total_input_tokens:,} tokens")
    print(f"æ€»Tokenæ¶ˆè€—: çº¦ {total_input_tokens + total_output_tokens:,} tokens")
    print("==============================================\n")
    
    return results

# æ‰§è¡Œå…¬å¸åˆ†æ
def run_analysis(company_name):
    # ç”Ÿæˆå…³é”®è¯æœç´¢
    keywords = [
        f"{company_name} ä¸šç»©",
        f"{company_name} é£é™©",
        f"{company_name} é€ å‡"
    ]
    
    # åˆ›å»ºå­£åº¦æ—¥æœŸèŒƒå›´
    quarter_ranges = generate_quarter_ranges()
    
    # å­˜å‚¨æ‰€æœ‰æœç´¢ç»“æœå’Œå†…å®¹
    all_results = []
    all_contents = []
    
    # è¿›åº¦æ¡
    progress_bar = st.progress(0)
    search_status = st.empty()
    
    # å‡†å¤‡æ‰€æœ‰æœç´¢ä»»åŠ¡
    search_tasks = []
    for keyword in keywords:
        for quarter in quarter_ranges:
            search_tasks.append((keyword, quarter['start'], quarter['end']))
    
    # æ˜¾ç¤ºæœç´¢çŠ¶æ€
    search_status.write("æ­£åœ¨å¹¶è¡Œæ‰§è¡Œæœç´¢è¯·æ±‚...")
    
    # å¹¶è¡Œæ‰§è¡Œæœç´¢
    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_search = {
            executor.submit(run_exa_search, task[0], task[1], task[2], 20): task 
            for task in search_tasks
        }
        
        # å¤„ç†ç»“æœ
        completed_searches = 0
        total_searches = len(search_tasks)
        
        for future in concurrent.futures.as_completed(future_to_search):
            completed_searches += 1
            task = future_to_search[future]
            keyword, start_date, end_date = task
            
            try:
                search_data = future.result()
                if search_data['results']:
                    all_results.extend(search_data['results'])
                
                if search_data['contents']:
                    all_contents.extend(search_data['contents'])
                
                # æ›´æ–°è¿›åº¦æ¡å’ŒçŠ¶æ€ä¿¡æ¯
                progress = completed_searches / total_searches
                progress_bar.progress(progress)
                search_status.write(f"å®Œæˆ: {completed_searches}/{total_searches} - {search_data['keyword']} ({search_data['quarter']})")
            
            except Exception as e:
                st.error(f"å¤„ç†æœç´¢ç»“æœæ—¶å‡ºé”™: {str(e)}")
    
    search_status.empty()
    progress_bar.empty()
    
    # æå–æ‰€æœ‰URLå’Œå†…å®¹
    url_to_content = {}
    content_hashes = set()
    unique_contents = []
    duplicates_count = 0
    
    # å¯¹æœç´¢ç»“æœè¿›è¡Œå»é‡
    for item in all_contents:
        url = item['url']
        content = item['content']
        
        # ä½¿ç”¨å†…å®¹çš„å“ˆå¸Œå€¼æ¥å»é‡
        content_hash = hash(content)
        
        if content_hash not in content_hashes:
            # è¿™æ˜¯ä¸€ä¸ªæ–°çš„ä¸é‡å¤å†…å®¹
            content_hashes.add(content_hash)
            unique_contents.append(item)
            url_to_content[url] = content
        else:
            # è¿™æ˜¯é‡å¤å†…å®¹
            duplicates_count += 1
    
    # æ›´æ–°åˆ°å»é‡åçš„å†…å®¹åˆ—è¡¨
    all_contents = unique_contents
    
    # æå–æ‰€æœ‰ä¸é‡å¤çš„URL
    all_urls = list(url_to_content.keys())
    
    # è®¡ç®—æœç´¢ç»“æœæ€»å­—æ•°
    total_search_chars = 0
    for item in all_contents:
        total_search_chars += len(item['content'])
    
    

    # æ‰“å°æœç´¢ç»“æœç»Ÿè®¡ä¿¡æ¯ - åªåœ¨ç»ˆç«¯æ˜¾ç¤º
    print("\n============== æœç´¢ç»“æœç»Ÿè®¡ ==============")
    print(f"æœç´¢ç»“æœæ•°: {len(all_results)} æ¡")
    print(f"ç‹¬ç‰¹URLæ•°: {len(all_urls)} ä¸ª")
    print(f"å»é™¤é‡å¤å†…å®¹: {duplicates_count} ä¸ª")
    print(f"æœç´¢ç»“æœæ€»å­—æ•°: {total_search_chars:,} å­—ç¬¦")
    print(f"æœç´¢ç»“æœä¼°è®¡Tokenæ•°: çº¦ {estimate_tokens(str(total_search_chars))} tokens")
    print("==============================================\n")
    
    # å¤„ç†å†…å®¹
    if all_contents:
        st.markdown("æˆåŠŸè·å–å†…å®¹: {} ä¸ªç½‘é¡µ".format(len(all_contents)))
        
        # åˆå¹¶æ‰€æœ‰å†…å®¹
        combined_text = []
        for item in all_contents:
            combined_text.append(f"URL: {item['url']}\n\n{item['content']}\n\n---\n\n")
        
        combined_content = "".join(combined_text)
        combined_content_chars = len(combined_content)
        combined_content_tokens = estimate_tokens(combined_content)
        
        # åªåœ¨ç»ˆç«¯æ˜¾ç¤ºåˆå¹¶å†…å®¹çš„ç»Ÿè®¡ä¿¡æ¯
        print("\n============== åˆå¹¶å†…å®¹ç»Ÿè®¡ ==============")
        print(f"åˆå¹¶åå†…å®¹æ€»å­—æ•°: {combined_content_chars:,} å­—ç¬¦")
        print(f"åˆå¹¶åå†…å®¹ä¼°è®¡Tokenæ•°: çº¦ {combined_content_tokens:,} tokens")
        print("==============================================\n")
        
        # å¦‚æœå¼€å¯äº†æ˜¾ç¤ºæœç´¢ç»“æœé€‰é¡¹ï¼Œåˆ™æ˜¾ç¤ºæœç´¢ç»“æœ
        if st.session_state.show_search_results:
            search_results_container = st.expander("æŸ¥çœ‹æœç´¢ç»“æœ", expanded=True)
            with search_results_container:
                for i, item in enumerate(all_contents):
                    st.markdown(f"### ç»“æœ {i+1}: {item['url']}")
                    st.text_area(f"å†…å®¹ {i+1}", item['content'], height=200, key=f"content_{i}")
                    st.markdown("---")
        
        # å¼€å§‹ä¸“å®¶åˆ†æ
        if combined_content:
            # ä½¿ç”¨ä¸“å®¶åˆ†æ
            expert_results = analyze_with_experts(combined_content)
            
            # ä¿å­˜ç»“æœåˆ°session state
            st.session_state.analysis_results = expert_results
        else:
            st.warning("æœªè·å–åˆ°ç½‘é¡µå†…å®¹ï¼Œæ— æ³•è¿›è¡Œä¸“å®¶åˆ†æ")
    else:
        st.warning("æœªæ‰¾åˆ°ç›¸å…³ç½‘é¡µ")
    
    # åˆ†æå®Œæˆ
    st.session_state.should_run_analysis = False

# æ£€æŸ¥æ•°æ®ç›®å½•å’Œåˆå§‹åŒ–ä¸“å®¶åˆ—è¡¨
if 'expert_names_loaded' not in st.session_state:
    expert_dir = Path("data_short_expert")
    
    if expert_dir.exists() and any(expert_dir.iterdir()):
        st.session_state.expert_names = get_expert_names()
        # é»˜è®¤é€‰ä¸­æŒ‡å®šçš„ä¸“å®¶
        st.session_state.selected_experts = [expert for expert in DEFAULT_EXPERTS if expert in st.session_state.expert_names]
        st.session_state.expert_names_loaded = True
    else:
        with st.spinner("æ­£åœ¨åˆå§‹åŒ–ä¸“å®¶æ•°æ®..."):
            if initialize_dropbox():
                st.session_state.expert_names = get_expert_names()
                # é»˜è®¤é€‰ä¸­æŒ‡å®šçš„ä¸“å®¶
                st.session_state.selected_experts = [expert for expert in DEFAULT_EXPERTS if expert in st.session_state.expert_names]
                st.session_state.expert_names_loaded = True
            else:
                st.error("æ— æ³•åˆå§‹åŒ–ä¸“å®¶æ•°æ®ã€‚è¯·ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨æˆ–é…ç½®æ­£ç¡®ã€‚")

# ä¾§è¾¹æ é…ç½®
with st.sidebar:
    st.title("ğŸ” Risk Scanner è®¾ç½®")
    
    # ç³»ç»Ÿé…ç½®é€‰é¡¹
    st.subheader("âš™ï¸ æ¨¡å‹è®¾ç½®")
    # æ·»åŠ æ¨¡å‹é€‰æ‹©é€‰é¡¹
    selected_model = st.radio(
        "é€‰æ‹©åˆ†ææ¨¡å‹",
        options=["gemini-2.5-flash", "gemini-2.5-pro"],
        index=0 if st.session_state.selected_model == "gemini-2.5-flash" else 1,
        help="é€‰æ‹©ç”¨äºåˆ†æçš„AIæ¨¡å‹",
        key="model_selector"
    )
    
    # æ›´æ–°ä¼šè¯çŠ¶æ€ä¸­çš„æ¨¡å‹é€‰æ‹©
    if selected_model != st.session_state.selected_model:
        st.session_state.selected_model = selected_model
        # å½“æ¨¡å‹æ”¹å˜æ—¶ï¼Œæ¸…ç©ºä¸“å®¶ä»£ç†ç¼“å­˜
        st.session_state.expert_agents = {}
        st.success(f"å·²åˆ‡æ¢è‡³ {selected_model} æ¨¡å‹")
    
    # æ˜¾ç¤ºä¸“å®¶åˆ—è¡¨
    st.header("ğŸ’¡ ä¸“å®¶åˆ—è¡¨")
    
    # å…¨é€‰/å–æ¶ˆå…¨é€‰æŒ‰é’®
    if st.button("å…¨é€‰" if len(st.session_state.selected_experts) < len(st.session_state.expert_names) else "å–æ¶ˆå…¨é€‰"):
        if len(st.session_state.selected_experts) < len(st.session_state.expert_names):
            st.session_state.selected_experts = st.session_state.expert_names.copy()
        else:
            st.session_state.selected_experts = []
    
    # å¯¹ä¸“å®¶åå•è¿›è¡Œæ’åºï¼Œå°†"Publication"å¼€å¤´çš„æ”¾åœ¨æœ€å
    sorted_experts = sorted(st.session_state.expert_names, 
                          key=lambda x: (1 if x.startswith("Publication") else 0, x))
    
    # ä¸“å®¶é€‰æ‹©
    for expert_name in sorted_experts:
        avatar = get_expert_avatar(expert_name)
        col1, col2 = st.columns([0.7, 3])
        
        with col1:
            is_selected = expert_name in st.session_state.selected_experts
            if st.checkbox(
                label=f"é€‰æ‹©{expert_name}",
                value=is_selected,
                key=f"check_{expert_name}",
                label_visibility="collapsed"
            ):
                if expert_name not in st.session_state.selected_experts:
                    st.session_state.selected_experts.append(expert_name)
            else:
                if expert_name in st.session_state.selected_experts:
                    st.session_state.selected_experts.remove(expert_name)
        
        with col2:
            st.markdown(f"{avatar} {expert_name}")
    
    # æ·»åŠ åˆ†éš”çº¿
    st.markdown("---")
    
    # æ›´æ–°æŒ‰é’®
    if st.button("ğŸ”„ æ›´æ–°ä¸“å®¶åˆ—è¡¨", type="primary"):
        with st.spinner("æ­£åœ¨æ›´æ–°ä¸“å®¶èµ„æ–™..."):
            if initialize_dropbox():
                # æ¸…é™¤ç°æœ‰ä¸“å®¶æ¨¡å‹
                st.session_state.expert_agents = {}
                # é»˜è®¤é€‰ä¸­æŒ‡å®šçš„ä¸“å®¶
                st.session_state.selected_experts = [expert for expert in DEFAULT_EXPERTS if expert in st.session_state.expert_names]
                st.success("ä¸“å®¶èµ„æ–™æ›´æ–°æˆåŠŸï¼")
                st.rerun()
            else:
                st.error("æ›´æ–°å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–é…ç½®ã€‚")

# ä¸»ç•Œé¢å†…å®¹ï¼ˆä¸ä½¿ç”¨åˆ—åˆ†å‰²ï¼Œç›´æ¥åœ¨ä¸»åŒºåŸŸæ”¾ç½®å†…å®¹ï¼‰
st.title("ğŸ” Risk Scanner")
st.write("è¾“å…¥å…¬å¸åç§°ï¼Œæ‰«ææ½œåœ¨æ¬ºè¯ˆé£é™©")

# ä½¿ç”¨è¡¨å•ç¡®ä¿åªæœ‰æŒ‰Enteræ‰æäº¤
with st.form(key="search_form"):
    company_name = st.text_input("å…¬å¸åç§°", placeholder="ä¾‹å¦‚ï¼šé˜¿é‡Œå·´å·´ (è¾“å…¥åæŒ‰Enteræ‰§è¡Œåˆ†æ)")
    # æ˜¾ç¤ºæäº¤æŒ‰é’®
    submit_button = st.form_submit_button("å¼€å§‹åˆ†æ", type="primary")

# åªæœ‰å½“è¡¨å•æäº¤æ—¶æ‰æ‰§è¡Œåˆ†æ
if submit_button and company_name:
    # æ›´æ–°çŠ¶æ€
    st.session_state.company_name = company_name
    st.session_state.should_run_analysis = True

# å½“éœ€è¦æ‰§è¡Œåˆ†ææ—¶
if st.session_state.should_run_analysis and company_name:
    # æ£€æŸ¥æ˜¯å¦é€‰æ‹©äº†ä¸“å®¶
    if not st.session_state.selected_experts:
        st.warning("è¯·åœ¨ä¾§è¾¹æ é€‰æ‹©è‡³å°‘ä¸€ä½ä¸“å®¶è¿›è¡Œåˆ†æ")
        st.session_state.should_run_analysis = False
    else:
        # æ‰§è¡Œåˆ†æ
        run_analysis(company_name)

# å¦‚æœæœ‰ä¹‹å‰çš„åˆ†æç»“æœï¼Œæ˜¾ç¤ºå®ƒä»¬
elif st.session_state.analysis_results and not st.session_state.should_run_analysis:
    # æ˜¾ç¤ºä¸“å®¶åˆ†æç»“æœ
    st.subheader(f"ä¸“å®¶åˆ†æ ({len(st.session_state.analysis_results)} åä¸“å®¶)")
    
    # æ·»åŠ CSSæ ·å¼ä»¥ç¾åŒ–ä¸“å®¶å›ç­”åŒºåŸŸ
    st.markdown("""
    <style>
    .expert-box {
        border: 2px solid #4CAF50;
        border-radius: 10px;
        padding: 20px;
        margin-bottom: 25px;
        background-color: #f8f9fa;
        box-shadow: 0 4px 8px rgba(0,0,0,0.1);
    }
    .expert-header {
        font-size: 1.3em;
        font-weight: bold;
        margin-bottom: 15px;
        padding-bottom: 10px;
        border-bottom: 2px solid #4CAF50;
        color: #2E7D32;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # æ˜¾ç¤ºæ¯ä¸ªä¸“å®¶çš„ç»“æœï¼Œå¹¶ä¸ºæ¯ä¸ªä¸“å®¶æ·»åŠ å•ç‹¬çš„å®¹å™¨
    for i, (expert_name, result) in enumerate(st.session_state.analysis_results.items()):
        # åˆ›å»ºä¸“å®¶å®¹å™¨ï¼Œä½¿ç”¨expander
        with st.expander(f"{result['avatar']} {expert_name}", expanded=True):
            # ç›´æ¥ä½¿ç”¨markdownæ¸²æŸ“å†…å®¹
            st.markdown(result['content']) 